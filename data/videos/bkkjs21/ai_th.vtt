WEBVTT

00:00:02.700 --> 00:00:08.233
สวัสดีทุกท่านที่เข้ามาในงาน bkk.js นะครับ

00:00:08.300 --> 00:00:11.633
ยินดีต้อนรับเข้าสู่เซสชั่นแรกนะครับ

00:00:11.700 --> 00:00:16.133
กับชื่อตัวของ Developing JavaScript for open source

00:00:16.200 --> 00:00:22.633
Generative AI Developers in Nowadays แนะนำตัวอย่างเป็นทางการนะครับ ผม Boat Charunthon นะครับ

00:00:22.700 --> 00:00:28.833
ใครที่อยากจะมาคอนเน็คก็สามารถที่จะสแกน QR Code เข้ามาข้างใน

00:00:28.900 --> 00:00:35.833
เว็บไซต์ได้นะครับ ก็ตอนนี้ก็ทำงานเป็น tech contribution ให้กับ

00:00:35.900 --> 00:00:42.233
ทางโครงการ Microsoft Learn Student Ambassador นะครับ แล้วก็เป็น Student Tech Lead ของประเทศไทยนะครับผม

00:00:42.300 --> 00:00:48.633
แล้วก็เป็นตัวของ Student Trainer แล้วก็เป็น Microsoft Office Specialist ด้วยนะครับ

00:00:49.400 --> 00:00:54.733
โอเคครับ ก็ปกติแล้วเนี่ย คือในโลกของซอฟต์แวร์เนี่ย

00:00:54.800 --> 00:01:00.033
ปกติมันจะแบ่งอยู่ 2 ประเภทใช่ไหมครับ ก็จะมีแบบ

00:01:00.100 --> 00:01:04.032
Proprietary แล้วก็ตัวของ open source นะครับ

00:01:04.099 --> 00:01:09.833
แต่ตัว proprietary มันก็คือคล้ายๆ กับ commercial ซอฟต์แอร์

00:01:09.900 --> 00:01:15.032
อาจจะมีการเสียค่าไลเซ็นส์ หรือว่าอาจจะแบบว่าใช้บริการที่

00:01:15.100 --> 00:01:20.433
มันอาจจะไม่ได้ฟรีนะครับ ก็มีฟรีเทียร์บ้าง แล้วก็มีแบบ

00:01:20.500 --> 00:01:24.633
premium ซึ่งมันก็ถือว่าเสียตังค์ใช่ไหมครับ แล้วก็ในเรื่องของ

00:01:24.700 --> 00:01:31.233
open source มันก็เป็นซอฟต์แวร์ที่ฟรี แล้วก็สามารถที่พัฒนา

00:01:31.300 --> 00:01:34.733
อะไรต่อไปได้ ขอซาวเสียงหน่อยครับ

00:01:34.800 --> 00:01:41.133
ว่ามีใครชอบใช้พวกซอฟต์แวร์ open source บ้างไหมครับ ยกมือหน่อย

00:01:42.700 --> 00:01:47.532
อ๋อ มีประมาณนึงเนอะ นะครับ ก็ เอ่อ หลักๆ เนี่ย

00:01:47.600 --> 00:01:51.433
ตัว tech stack เนี่ย หลักๆ มันก็จะมีประมาณนี้ใช่มั้ยครับ แล้วก็

00:01:52.100 --> 00:01:56.233
เอ่อ ถ้าเป็นฝั่ง AI แล้วก็ Machine Learning เนี่ย มันก็จะมีแบบพวก

00:01:56.300 --> 00:02:01.233
ตัวของ Hugging Face หรือว่าจะเป็นตัวของ Ollama หรือว่าจะเป็นพวก

00:02:01.300 --> 00:02:04.933
TensorFlow หรือ Git อะไรประมาณเนี้ยครับ

00:02:05.000 --> 00:02:11.632
มันจะมีประมาณนี้ครับ แต่ว่าใน experiment ของวันนี้ที่เราเอามาใช้ในเรื่องของ open source

00:02:11.700 --> 00:02:17.132
AI model ที่เราจะทำเนี่ย มันจะอยู่ในพารามิเตอร์ที่มันต่ำ หรือ

00:02:17.200 --> 00:02:20.833
อีกอย่างนึงที่เขาจะเรียกเนี่ย เขาจะเรียกว่า Small Language Model นะครับ

00:02:22.100 --> 00:02:28.933
โอเคครับ ก็วันนี้หลักๆ เนี่ย เอ่อ เราก็จะมา

00:02:29.000 --> 00:02:39.333
ทำความรู้จักกับ 3 ตัวนี้นะครับ ก็จะเป็นทางฝั่งของ transformers.js ที่มาจากตัวของ Hugging Face หรือว่าจะเป็นเรื่องของ Ollama

00:02:39.400 --> 00:02:51.733
.js นะครับ ที่จะเป็นตัวใหม่ที่เป็นเทรนด์ใหม่ในการ run โมเดลบนเครื่องอะไรประมาณนี้นะครับ แล้วก็อีกอันนึงที่อยากจะแนะนำก็คือ

00:02:51.800 --> 00:02:55.433
เป็นเรื่องของ GitHub Models แล้วก็ GitHub Repo นะครับ

00:02:57.200 --> 00:03:02.433
มาดูคำจำกัดความในเรื่องของ Small Language Model กันหน่อยนะครับ ว่า

00:03:02.500 --> 00:03:06.533
ตกลงเนี่ยมันคืออะไรกันแน่นะครับ ตัว Small Language Model เนี่ย

00:03:06.600 --> 00:03:14.632
มันจะเป็นในเรื่องของการ downsize ตัว Large Language Model ให้มันสามารถที่จะเอามาใช้ใน บนเครื่อง local ของเราได้ครับ

00:03:15.500 --> 00:03:23.033
ปัญหา หรือ pain points ที่เกิดขึ้น ณ ปัจจุบันนะครับ ก็คือว่า พอเรา เอ่อ โหมกัน

00:03:23.100 --> 00:03:26.632
ใช้ตัว Large Language Model ใน Data Center

00:03:26.700 --> 00:03:31.132
มันจะเกิดในเรื่องของ power consumption หรือว่าจะเป็นเรื่องของการใช้พลังงานที่

00:03:31.200 --> 00:03:35.733
เอามาประมวลผลของตัว Large Language Model มากขึ้น

00:03:35.800 --> 00:03:38.632
ทำให้เกิดภาวะโลกร้อนนั่นเองครับ แต่

00:03:38.700 --> 00:03:45.033
ถ้าเกิดว่าในกรณีที่เกิด outage หรือว่าเกิดการล่มของพวก

00:03:45.100 --> 00:03:49.233
ChatGPT หรือว่า Gemini เราจะสามารถเห็นได้หลายครั้งครับว่า

00:03:49.300 --> 00:03:53.333
ตัวระบบเองมันมีโอกาสล่มนะครับ

00:03:54.500 --> 00:03:59.833
ทางที่ดีก็คือก็จะมีพวก AI researcher หรือว่า AI engineer

00:03:59.900 --> 00:04:02.433
ช่วยคิดพัฒนา Small Language Model

00:04:03.300 --> 00:04:09.833
มาให้เราใช้บน local device ไม่ว่าจะเป็นบนมือถือของเราเองหรือว่าจะเป็น

00:04:09.900 --> 00:04:15.533
ตัวของ device อย่างคอมเราเองแบบสามารถที่จะใช้แบบ offline ได้นั่นเองครับ

00:04:15.600 --> 00:04:20.832
หรือว่าเราเองสามารถจะเก็บไว้ใน Docker ก็ได้เช่นกันนะครับผม

00:04:21.500 --> 00:04:27.832
แต่ถ้าเราไม่ได้เห็นภาพว่าจาก Large Language Model มาอยู่ Small Language Model ได้

00:04:27.900 --> 00:04:32.832
จริงๆ มันก็จะมีแบบพวก minimizer เนี่ยในการ

00:04:32.900 --> 00:04:36.933
ย่อส่วนของตัว Large Language Model ให้เป็น Small Language Model นะครับ

00:04:37.000 --> 00:04:42.332
จริงๆ มันจะมีอีกคำนึงเดี๋ยวผมจะให้ดูในสไลด์ถัดๆ ไปว่า

00:04:43.000 --> 00:04:48.832
มันใช้คำว่าอะไรนั่นเองนะครับ แต่ผมเองเนี่ยจะไม่ได้ลงรายละเอียดว่า

00:04:48.900 --> 00:04:55.433
วันนี้ตัว AI มันทำหน้าที่ยังไง แต่ว่าเราจะมาคุยใน scope ของ JavaScript นะครับ

00:04:55.500 --> 00:05:01.933
อันแรกที่ผมอยากจะคุยกันก็คือตัว GitHub Models นะครับ

00:05:02.000 --> 00:05:07.733
คำถาม คำแรกของผม พอเวลาผมเจอตัว GitHub Models เนี่ย ผมเองก็สงสัยว่า

00:05:07.800 --> 00:05:11.832
เอ๊ย มันมาแทนที่ตัว Hugging Face ที่มันเป็น open source หรือเปล่า

00:05:13.100 --> 00:05:16.933
ซึ่งตัว GitHub Models เนี่ยจริงๆ มันอยู่

00:05:17.000 --> 00:05:23.733
ภายใต้ ecosystem ของ GitHub นั่นเองครับ แต่ว่าเราเองเนี่ย ไม่จำเป็นที่จะต้องไป drag

00:05:23.800 --> 00:05:30.733
environment เพิ่มเติมจากตัวของ Hugging Face แล้วก็จริง ๆ ตัวโมเดลมันยังออกมา

00:05:30.800 --> 00:05:33.332
ไม่เยอะเท่าไหร่ครับ เพราะว่ามันเพิ่งเปิดตัว

00:05:34.100 --> 00:05:40.033
หลัก ๆ เนี่ย ตัวโมเดลเนี่ย มันก็จะมีในเรื่องของพวก Azure OpenAI หรือว่าจะเป็นทาง Microsoft Phi 3

00:05:40.100 --> 00:05:44.533
แล้วก็จะมีพวก Cohere หรือว่าเป็นตัว Mistral AI นั่นเองครับ

00:05:45.900 --> 00:05:51.133
มาดู syntax หลัก ๆ กันนะครับ ว่าใน GitHub Models นี้มันจะเป็นยังไงบ้างนะครับ

00:05:51.200 --> 00:05:55.633
หลัก ๆ ตัวของ GitHub Models มันมีอย่างเช่น JavaScript

00:05:55.700 --> 00:06:01.933
C# แล้วก็ตัวของ Python ใช่มั้ยครับ แต่วันนี้เดี๋ยวผมจะมาคุยในเรื่องของ JavaScript นั่นเองครับ

00:06:02.000 --> 00:06:07.832
ตอนแรกเราจะ import กันก่อนนะครับ import มาจากตัว Azure Rest AI Inference

00:06:07.900 --> 00:06:12.533
มีการดึงตัว key credential ใน batch นะครับ

00:06:12.600 --> 00:06:18.633
จากนั้นมีการ generate ตัว token มาก่อนนะครับ จากใน GitHub แล้วก็เอามาใส่

00:06:18.700 --> 00:06:23.233
จากนั้นเราก็ค่อยมาใส่ตัวโมเดลที่เราจะใช้

00:06:25.300 --> 00:06:32.133
โอเค แล้วก็ทางส่วนตอนนี้ปุ๊บ เราก็จะเห็นว่า เอ่อ ในหลักการทำงานของ client และ response เนี่ย

00:06:32.200 --> 00:06:36.633
มันก็จะมีการตั้งของ system แล้วก็ทางฝั่งของ user นั่นเองครับ

00:06:36.700 --> 00:06:45.533
แล้วก็ตัวของ response เนี่ย เราสามารถ reset ตัว temperature แล้วก็ตัวของ max token ในหลาย ๆ กรณีได้เช่นกันนะครับผม

00:06:49.000 --> 00:06:54.133
ถ้าเกิดว่ามันมีปัญหาขึ้นมาเนี่ย มันก็จะต้องมี

00:06:54.200 --> 00:07:00.033
ตัว console error ที่มาโชว์ว่า เอ้อ มันเจอปัญหาในการดึง AI อะไรประมาณนี้นั่นเองครับ

00:07:00.700 --> 00:07:06.033
แล้วก็มาเรื่องที่ 2 นะครับ คือตัวของ transformers.js ใน Hugging Face นะครับ

00:07:06.900 --> 00:07:09.933
ตอนแรกเนี่ย คือ ปกติเนี่ย เราอาจจะไม่ได้เห็น

00:07:10.000 --> 00:07:14.733
ว่า use case ของ transformer.js มันใช้งานยังไง

00:07:14.800 --> 00:07:20.533
บางทีเราอาจจะเห็นระบบหลังบ้านเนี่ย คือตามหลายบริษัทเนี่ย

00:07:20.600 --> 00:07:24.332
อาจจะปิดไม่ให้เราได้เห็นตัว source code อะไรประมาณเนี้ยครับ แต่ว่า

00:07:24.400 --> 00:07:31.332
ผมพอไปเห็นแบบ use case บางส่วนจาก อย่างเช่นตัวเว็บ LLM chat นะครับ

00:07:31.400 --> 00:07:36.933
ก็คือมันก็จะทำการโหลดโมเดลโดยการใช้ cache นั่นเองครับ แล้วก็เอามาโหลดในเครื่องนะครับ

00:07:37.000 --> 00:07:48.832
แล้วก็อีกอันนึงก็คือตัวที่พัฒนามาจากตัวเว็บ LLM ก็จะใช้ engine ที่เรียกว่า WebGPU นั่นเองครับ

00:07:48.900 --> 00:07:53.832
หลักๆ ตัวโครงสร้างมันก็จะมาจากตัว transformer.js นะครับ

00:07:55.300 --> 00:07:59.933
แต่ว่าระหว่างทางที่ผมทำ experiment ตัวเนี้ยครับ

00:08:00.000 --> 00:08:06.633
เกิดสะดุดขึ้นมาครับ ก็กลายเป็นว่าคือเครื่องผมอะ มันมีปัญหาอยู่แล้วครับ เพราะว่า

00:08:06.700 --> 00:08:11.733
เครื่องผมสเปกค่อนข้างที่จะอ่า กากอยู่พอสมควรนะครับ เพราะว่า

00:08:12.700 --> 00:08:19.233
คือจริงๆ แล้วเนี่ย ตัวผมเป็น GeForce RTX 3080 โดยประมาณนะครับ

00:08:19.300 --> 00:08:23.933
มันก็จะทำให้เกิดอ่า การ์ดจอมีปัญหาอะไรประมาณเนี้ยครับ เพราะว่า

00:08:24.000 --> 00:08:30.633
การ generate ความร้อนเนี่ย ภายในเครื่องเนี่ย มันทำให้ตัวหน้าจอมีปัญหาไปด้วยนะครับ

00:08:30.700 --> 00:08:34.732
เรามาดู syntax บาง syntax กันนะครับ ก็

00:08:34.799 --> 00:08:40.232
มันจะมีการ ในการดึงตัว WebGPU เข้ามาใช้นั่นเองครับ แล้วก็ให้มัน navigate

00:08:41.500 --> 00:08:41.933
นะครับ

00:08:43.400 --> 00:08:49.233
จากนั้นมันก็จะมีระบบตัวแอป แล้วก็มาทำเป็นพวก status loading message

00:08:49.300 --> 00:08:56.433
เรื่อยๆ นะครับผม จากนั้นก็มันจะมีในการทำ อ่า แบ่ง role ว่า

00:08:56.500 --> 00:08:59.333
เป็น user หรือจะเป็นตัวของ system นะครับ

00:09:00.100 --> 00:09:06.633
แต่ว่าตัวแอปเนี่ย อาจจะไม่ได้มีอะไรครับ มันก็จะไปหนักอยู่ที่ worker.js หรือว่า

00:09:06.700 --> 00:09:09.033
ตัวของ AI นะครับ ก็

00:09:09.100 --> 00:09:14.333
ตัวแรกมันก็จะ import ตัวของ AutoTokenizer ก็คือระบบการคิดคำ

00:09:14.400 --> 00:09:21.333
แล้วก็มีเรื่องของ text streamer ที่เอาไว้พริ้นต์ตัว text ออกมาที่เป็น output ออกมานะครับ

00:09:21.400 --> 00:09:26.933
แล้วก็แน่นอนครับว่า เวลาเราดึงตัวโมเดลเนี่ย เราจะดึงตัวของ

00:09:27.000 --> 00:09:28.733
Transformer นั่นเองนะครับ

00:09:32.900 --> 00:09:37.333
พอมาตัวคลาสอีกคลาสหนึ่งนะครับ ก็จะเป็นตัว text generation pipeline

00:09:37.400 --> 00:09:43.733
ก็คือมันจะดึงทั้งตัว model ID แล้วก็ดึงทั้งตัวของ

00:09:44.400 --> 00:09:46.433
โมเดลที่เราจะเลือกใช้นะครับ ก็

00:09:46.500 --> 00:09:52.533
เราจะ copy จากรหัสที่มีอยู่ใน Hugging Face แล้วก็เอามาลงใน model ID นั่นเองครับ

00:09:53.500 --> 00:09:57.633
แล้วก็ พอใครเห็นตัว syntax ตรงเนี้ย

00:09:57.700 --> 00:10:01.333
เผื่ออาจจะมีใครสงสัยว่า ไอ้ Q4 เนี่ยมันคืออะไรนะครับ

00:10:01.400 --> 00:10:04.933
ตัว Q4 เนี่ย มันคือ quantization 4 bit

00:10:05.000 --> 00:10:09.633
หรือว่า ตัวย่อขนาดโดยใช้ 4 bit นั่นเองนะครับ

00:10:09.700 --> 00:10:15.633
ซึ่ง quantization อย่างที่เมื่อกี้ที่ผมใช้คำว่า minimizer กับ quantization เนี่ย

00:10:15.700 --> 00:10:22.333
มันก็จะมีความหมายคล้ายๆ กัน โดยที่ convert bit จาก LLM ที่มีขนาดใหญ่ ให้มีขนาดที่

00:10:22.400 --> 00:10:28.533
เล็กลง แล้วก็เอามาอยู่ให้กับตัวเครื่องได้นั่นเองนะครับ

00:10:31.600 --> 00:10:37.233
มาถึงตัวสุดท้ายนะครับ ก็จะเป็นตัวของ Ollama.js

00:10:37.300 --> 00:10:42.833
ก็จะเป็นตัว Ollama ที่มัน run local อยู่ในแพลตฟอร์มของเรานั่นเองครับ

00:10:44.800 --> 00:10:47.833
โอเค ตัวนี้มันเป็นตัวที่ค่อนข้างที่น่าสนใจ

00:10:47.900 --> 00:10:55.233
แล้วก็จะเป็นเทรนด์ใหม่ในอนาคตด้วยนะครับ ว่าในกรณีที่ผมเคยเล่าไปแล้วว่า

00:10:55.300 --> 00:11:01.733
มันอาจจะเกิดกรณีในเรื่องของ outage ของ LLM อย่างพวก ChatGPT หรือว่าจะเป็นพวก Gemini

00:11:01.800 --> 00:11:09.033
ที่มัน maintenance แบบว่า เขาเรียกว่า พอมันล่มปุ๊บ

00:11:09.100 --> 00:11:13.433
แล้วก็มีการ maintenance เราก็ไม่รู้ว่ามันจะซ่อมเสร็จเมื่อไหร่ใช่ไหมครับ แล้วเราเองอะ

00:11:13.500 --> 00:11:17.433
เราก็ต้องพึ่ง AI ในการทำงานอะไรสักอย่าง

00:11:18.400 --> 00:11:25.233
งานของเราหรือว่า productivity ของเราเนี่ย ในหลักการทำงานของ Ollama เองเนี่ยครับ มันก็จะ

00:11:26.000 --> 00:11:27.333
มีทั้งการ

00:11:27.400 --> 00:11:33.833
load โมเดลจากตัว Docker ของทาง Ollama เอง แล้วก็เอามาใส่ในเครื่องของเราใน local machine นะครับ

00:11:33.900 --> 00:11:37.333
แล้วเวลาการ run เนี่ย มันจะ run ใน background service นะครับ

00:11:37.400 --> 00:11:42.333
เราจะใช้คำสั่ง Ollama run model name ซึ่ง client ตรงเนี้ย

00:11:42.400 --> 00:11:45.333
มันคือทางเว็บไซต์ของ Ollama

00:11:46.400 --> 00:11:53.333
เพราะเราอยากจะได้โมเดลสักโมเดลหนึ่ง เราก็มาขอจากตัวของ client

00:11:53.400 --> 00:11:59.133
แล้วจากนั้นเนี่ย ให้ตัว client เนี่ย มาส่งทาง background service นั่นเองครับ

00:11:59.200 --> 00:12:03.633
พอจะ create หรือจะ push เนี่ย เราก็สามารถทำ integrate

00:12:03.700 --> 00:12:07.433
ระหว่างตัวเครื่อง แล้วก็จากทางตัวของ Ollama นั่นเองนะครับ

00:12:10.200 --> 00:12:15.133
แล้วก็ที่สำคัญก็คือ พอเราอยากจะทำเป็นตัวของ

00:12:15.200 --> 00:12:18.933
browsing หรือว่าจะเป็นตัวของการดึง API เนี่ย

00:12:19.000 --> 00:12:24.533
เราก็จะใช้ตัว localhost ตัวนี้ด้านล่างเป็นตัวหลักนั่นเองนะครับผม

00:12:25.700 --> 00:12:32.333
มาดู ecosystem ในการทำ Ollama ใน use case อื่นๆ

00:12:32.400 --> 00:12:38.633
บางทีในกรณีที่มันไม่ได้ทำเฉพาะในเรื่องของ text generation อย่างเดียว

00:12:38.700 --> 00:12:48.033
ทำในเรื่องของ code generation ได้เช่นกันนะครับ โดยที่มันจะใช้เนี่ย บางคนอาจจะเคยได้ยินว่ามันจะมี extension นึงของ VS Code

00:12:48.100 --> 00:12:54.933
ที่ชื่อว่า Continue ที่เอามาช่วยในเรื่องของ code generation ซึ่งมันก็จะดึงจาก

00:12:55.000 --> 00:12:58.633
ตัวแพลตฟอร์ม Ollama ของเครื่องของเรานั่นเองครับ แต่

00:12:58.700 --> 00:13:02.733
ก่อนที่มันจะ run ได้เนี่ย เราจะต้องมีโมเดลก่อน ซึ่ง

00:13:02.800 --> 00:13:06.833
ตัวโมเดล เราสามารถที่จะเลือก ว่าจะเป็นตัวของ

00:13:06.900 --> 00:13:12.933
Gemma 2 ก็ได้ หรือว่าจะเป็น Phi 3.5 ก็ได้เช่นกันครับ

00:13:13.000 --> 00:13:17.633
อย่างในตัวอย่างนี้ เราก็จะเห็นว่าเขาใช้ในตัวของ Granite

00:13:17.700 --> 00:13:22.633
20 พันล้านพารามิเตอร์ หรือว่าจะเป็นแบบ 8b ก็แล้วแต่ครับ

00:13:22.700 --> 00:13:27.133
แต่ก่อนที่มันจะมาเป็นตัวโมเดล ที่มาอยู่ในเครื่องเนี่ย

00:13:27.200 --> 00:13:30.933
มันก็จะมีในการดึงตัวของ

00:13:31.000 --> 00:13:37.633
โมเดลจาก Hugging Face ก่อน แล้วก็เอามาแปลงเป็น GGUF โดยการ quantize มันให้เหลือ 4 bit

00:13:37.700 --> 00:13:40.933
จากนั้นเนี่ย เราก็เอามาใช้แบบ local ได้นั่นเองนะครับ

00:13:42.000 --> 00:13:47.133
โอเคครับ syntax ง่ายๆ เลยครับ ก็คือตอนแรกเราจะ import ตัวของ

00:13:47.200 --> 00:13:54.133
OpenAI ก่อน ซึ่งเป็นเหมือนกับระบบ main นะครับ ที่ อ่า ลอกเลียนแบบจากตัวของ OpenAI นั่นเอง

00:13:54.200 --> 00:13:58.233
จากนั้นเราก็จะมี base URL แล้วก็ใส่ API key

00:13:58.300 --> 00:14:05.033
สำหรับ Ollama เองเนี่ย เราจะไม่ได้พึ่งตัว API key เลยครับ เราแค่ base ตัว URL ได้อย่างเดียว

00:14:05.100 --> 00:14:10.433
แล้วก็ อ่า chunk ที่เราจะมาใช้เนี่ย ก็คือเราเลือกโมเดลที่เรามีอยู่ในเครื่อง

00:14:10.500 --> 00:14:13.733
แล้วก็จากนั้นก็จะใช้ตัว message แล้วก็

00:14:14.300 --> 00:14:21.033
ทำเป็นแบ่ง role ว่าจะเป็น user หรือจะเป็น system ก็แล้วแต่นั่นเองครับ

00:14:21.100 --> 00:14:22.733
เรามาดูเดโมกันนะครับ

00:14:36.000 --> 00:14:36.533
โอเคครับ

00:15:32.800 --> 00:15:39.733
OK ครับ ก็เริ่มต้น ผมจะเขียนตัว generate1.js ขึ้นมานะครับ ว่า

00:15:39.800 --> 00:15:43.133
หลักๆ เนี่ย เราจะ import ตัวอันนี้ก่อน

00:15:46.100 --> 00:15:52.133
ผมจะ import ตัวของ Ollama ก่อนนะครับ จากนั้นเนี่ยก็จะ

00:15:52.200 --> 00:15:57.733
control ตัว generate ว่า model ที่เราจะใช้ก็คือตัว model Phi 3.5 นะครับ

00:15:57.800 --> 00:16:04.733
จากนั้นก็ prompt เนี่ย ผมก็ใส่ prompt ว่า What is 2 + 5? 2 + 5 ได้เท่าไหร่ อะไรประมาณนั้นนะครับ

00:16:04.800 --> 00:16:09.233
แล้วก็มีการใส่ console.log คราวนี้นะครับ ก็

00:16:09.300 --> 00:16:14.433
ก่อนที่จะมาถึงตรงนี้ เราอาจจะต้องไปดาวน์โหลดตัว model ที่ชื่อว่า Ollama กันก่อนนะครับ

00:16:15.600 --> 00:16:21.933
ตอนแรกที่เราจะดาวน์โหลดเนี่ย เราเข้าไปที่ตัวเว็บไซต์ olama.com นะครับ

00:16:22.000 --> 00:16:28.633
ซึ่งเรา search Ollama ปุ๊บ จากนั้นเราก็สามารถกดดาวน์โหลดตรงนี้ได้นั่นเอง

00:16:29.700 --> 00:16:35.133
เราสามารถ ที่จะมาดูตัว model ว่าเราอยากจะใช้ model ตัวไหน

00:16:35.200 --> 00:16:46.033
เราสามารถเข้ามาที่ตัว model แล้วก็จะมีเป็น Llama 3.1, Gemma 2 หรือว่าอะไรก็แล้วแต่ และตอนนี้ผมลงตัว Ollama ไปในเครื่องเรียบร้อยแล้วนะครับ

00:16:46.700 --> 00:16:52.633
พอจะเช็คตัว Ollama ว่า run ไหม เราเข้าที่ command prompt นะครับ หรือว่าจะเป็น terminal อืม เอ่อ

00:16:53.300 --> 00:16:59.933
terminal ของแต่ละเครื่องก็แล้วแต่นะครับ ผมเข้าที่ command prompt

00:17:01.800 --> 00:17:07.233
ครับ ปึ๊บๆๆๆๆ

00:17:08.500 --> 00:17:20.433
จากนั้น เรา run ด้วยคำสั่งคำว่า Ollama ครับ Ollama ปึ๊บ มันก็จะขึ้น available commands นะครับ ว่ามันจะมี serve มี create มี run แล้วแต่นะครับ

00:17:20.500 --> 00:17:23.433
จากนั้นเนี่ย พอเราจะ run ตัวใดตัวหนึ่ง

00:17:23.500 --> 00:17:29.033
สมมุติว่าผมจะ Run ตัว Phi 3.5 ผม Run ollama run

00:17:29.900 --> 00:17:36.633
phi3.5 นะครับ มันก็จะทำการโหลด

00:17:36.700 --> 00:17:37.033
นะครับ

00:17:41.100 --> 00:17:47.233
มันก็จะให้ขึ้นเมสเซจออกมานะครับ แล้วก็สมมติว่าเราอยากจะพิมพ์ทักทายมันครับ "Hello"

00:17:51.100 --> 00:17:52.933
โอเคมันก็จะขึ้นนะครับ

00:17:54.200 --> 00:18:00.133
ก็ ณ ตอนเนี้ยนะครับ ถ้าเป็นการทำงานหลัก ๆ ของมันใน—

00:18:00.200 --> 00:18:06.533
เดี๋ยวเคสจริงๆ ถ้าสมมุติว่าเราเอาอินเทอร์เน็ตออกเนี่ย มันก็ยังสามารถที่จะทำงานได้นั่นเองนะครับผม

00:18:08.000 --> 00:18:11.933
และตอนนี้ในตัวของเครื่องผมเนี่ย

00:18:12.900 --> 00:18:16.033
มันจะมีโมเดลหลักๆ อยู่แค่ตัวเดียวนะครับ

00:18:17.300 --> 00:18:22.733
ซึ่งเราสามารถเช็คตัวโมเดลว่ามีโมเดลไหนไปในเครื่องเราก็

00:18:22.800 --> 00:18:29.533
ollama ls แล้วมันก็จะขึ้นชื่อเลยว่า เรามี Phi 3.5 เรามี Nomic อะไรประมาณนี้ครับ

00:18:29.600 --> 00:18:36.433
แต่ถ้าสมมุติว่าในกรณีที่เราอยากจะโหลด ตัว Gemma 2 สัก

00:18:36.500 --> 00:18:40.533
สองพันล้านพารามิเตอร์ก็สามารถทำได้ใน Command Prompt นั่นเองครับ

00:18:41.400 --> 00:18:44.633
กลับมาที่ตัวของ VS Code นะครับ

00:18:44.700 --> 00:18:49.733
หลัก ๆ แล้วเนี่ย พอเราเสร็จจากการเขียน Generate ปุ๊บ

00:18:49.800 --> 00:18:53.633
เดี๋ยวเราจะมาดูตัว console.log(response) กันก่อนนะครับ

00:18:56.600 --> 00:18:57.133
อุ๊ปส์

00:19:01.400 --> 00:19:06.833
ครั้งนี้ผมจะใช้ตัว runtime ที่ชื่อว่า Bun นะครับ เพราะว่ามันสะดวกและรวดเร็วนะครับ

00:19:18.400 --> 00:19:23.233
มันโหลดนิดนึงครับ เพราะว่ามันจะต้องไปดึงจากหลังบ้าน

00:19:23.800 --> 00:19:28.033
มันถึงจะค่อย generate ให้เราได้เห็นนะครับ เรามาดูตัว result กันนะครับ

00:19:29.200 --> 00:19:33.333
ซึ่งตัว result ที่ผม run ได้เนี่ย มันจะออกมาในรูปแบบของ JSON

00:19:33.400 --> 00:19:40.133
ซึ่งมันจะบอกเลยครับว่า model ที่เลือกใช้เนี่ย เป็น model อะไรนะครับ แล้วก็ create ตั้งแต่เมื่อไหร่

00:19:40.200 --> 00:19:45.033
แล้วก็มันก็จะ เอ่อ generate response ออกมาว่า เอ่อ

00:19:45.800 --> 00:19:51.433
นี่ 2 + 5 = 7 นะ อะไรประมาณนี้นะครับ แล้วก็มี มีการคิด context มีเรื่องของ เอ่อ token นั่นเองนะครับ

00:19:53.500 --> 00:20:00.433
แต่ถ้าสมมุติว่าในกรณีที่เราไม่ได้ต้องการที่เราจะเห็นไฟล์ JSON อะไรประมาณเเบบนี้นะครับ

00:20:00.500 --> 00:20:04.533
เราสามารถที่จะใส่ เอ่อ console.log แล้วก็ output.response ไปนะครับ

00:20:05.300 --> 00:20:10.533
เพราะว่าเราเอง เราต้องการเฉพาะแค่ output อย่างเดียวนะครับ

00:20:13.000 --> 00:20:17.733
จากนั้นเรารันใหม่นะครับ ด้วย bun

00:20:21.400 --> 00:20:28.333
generate1.js นะครับ แล้วรอสักครู่นะครับ เพราะว่ามันจะต้องดึงตัวหลังบ้าน

00:20:28.400 --> 00:20:34.933
นะครับ สักครู่นะครับ รู้สึกว่า อ๋อ ยังไม่ได้เซฟ

00:20:47.500 --> 00:20:50.733
อืม เอ๊ะ

00:20:51.500 --> 00:20:55.733
สักครู่นะครับ รู้สึกว่าน่าจะมีอะไรผิดปกติ

00:21:14.900 --> 00:21:21.633
น่าจะมีบางอย่างแปลกๆ แต่ว่าเดี๋ยวดูอันอีกต่อไป ตัวที่เป็นตัวแทนนะครับ

00:21:22.800 --> 00:21:26.433
ถ้าสมมุติว่าในกรณีที่

00:21:26.500 --> 00:21:30.433
เราเองเนี่ยครับ จะทำเป็น Completion Choice นะครับ

00:21:33.200 --> 00:21:34.333
เดี๋ยวมาดูกันนะครับ

00:22:08.800 --> 00:22:13.833
มันจะโหลดแป๊บนึงครับเพราะว่ามันจะต้องดึงหลังบ้านด้วยครับว่า

00:22:13.900 --> 00:22:19.233
หลังบ้านมัน Generate อยู่แล้วมันค่อย Push มาที่ตัวหน้าบ้านนั่นเองนะครับ

00:22:50.400 --> 00:22:53.033
เอ๊ะ น่าจะค้างแน่ ๆ เลย

00:22:53.100 --> 00:22:54.233
เดี๋ยวสักครู่นะครับ

00:23:00.500 --> 00:23:02.833
เดี๋ยวผมขออนุญาตเคลียร์เทอร์มินอลแปปนึง

00:23:22.800 --> 00:23:26.433
อ่าเค เพราะว่าปกติแล้วเนี่ย

00:23:26.500 --> 00:23:33.433
ในตัวของพารามิเตอร์ที่มันเล็กลงแล้วก็มันอยู่ภายในเครื่อง มันจะต้องใช้รีซอร์สเครื่อง

00:23:33.500 --> 00:23:36.933
อันที่สำคัญก็คือ ถ้าพารามิเตอร์น้อย

00:23:37.700 --> 00:23:42.633
โอกาสในการทำงานของมันอะ มันควรที่จะ Generate ค่อนข้างที่เร็วนะครับ

00:23:42.700 --> 00:23:47.233
ซึ่งผลที่ออกมา มันก็ตรงตัวนะครับ

00:23:48.100 --> 00:23:54.233
พอเราบอกว่าพร้อมเนี่ยที่จะ say hello ใน french เนี่ย ก็มันก็จะออกมาเป็น Bonjour นั่นเองนะครับ

00:23:55.700 --> 00:23:59.633
แล้วก็จริงๆ มันก็จะมีตัวของ

00:23:59.700 --> 00:24:05.033
Use Case อื่นๆ ที่เอามาใช้ทำนะครับ แล้วก็ให้มันไม่ได้เห็นตัวไฟล์

00:24:05.100 --> 00:24:11.933
แบบรูปแบบ .json นั่นเองนะครับ ก็จะมีแบบ ในเรื่องของ say hello หรือว่าจะเป็นเรื่องของการ

00:24:12.000 --> 00:24:18.933
หนึ่ง ตัวของไฟล์ json เนี่ย เอามาใช้ในการทำพวก RAG หรือว่า

00:24:19.000 --> 00:24:23.433
เราสามารถดึงคอนเทนต์ที่เป็น json เอามาใช้ได้นั่นเองนะครับ

00:24:28.000 --> 00:24:32.933
ผมเองอะ ผมเชื่อว่าคนอื่นอาจจะถามว่า

00:24:33.000 --> 00:24:38.833
มันมีความรู้สึกว่ามันมีความเป็นไปไม่ได้อะครับที่

00:24:38.900 --> 00:24:44.833
เรา Run AI ด้วย JavaScript เพราะว่าด้วยเรื่องของ Runtime หรืออะไรก็แล้วแต่

00:24:44.900 --> 00:24:50.733
มันดูไม่ค่อยสมเหตุสมผลในการใช้ JavaScript ในการ Run AI นะครับ

00:24:50.800 --> 00:24:54.233
แต่จริง ๆ เนี่ย มันก็จะมีหลาย ๆ อย่างที่เรารู้สึกว่า

00:24:54.300 --> 00:25:00.933
มันเป็นโอกาสใหม่ที่มันสามารถที่จะใช้ JavaScript มา Run AI ได้เช่นกันครับ

00:25:01.000 --> 00:25:07.833
ซึ่งในเรื่องของ Popularity เนี่ย ในเรื่องของหลังบ้านตัวของ

00:25:07.900 --> 00:25:10.133
เอ่อ พวก OpenAI

00:25:10.200 --> 00:25:13.333
มันก็มีการใช้ JavaScript ใช่ไหมครับ

00:25:13.400 --> 00:25:17.133
เราก็ไม่ได้มีโอกาสในการเห็นหลังบ้านว่า

00:25:17.200 --> 00:25:21.233
เอ๊ะ มันใช้ในส่วนไหนบ้างนะครับ ก็ตอนแรกเราก็นึกว่า

00:25:21.300 --> 00:25:27.333
มันจะใช้ในตัวของ frontend อย่างเดียว แต่ว่าบางที backend เนี่ย

00:25:27.400 --> 00:25:29.833
มีใช้บ้าง แต่เราก็ไม่ได้รู้ว่า

00:25:29.900 --> 00:25:36.833
ข้างในของ OpenAI หรือว่าจะเป็น Gemini เอง เขาทำอะไรบ้างนะครับ แต่ว่าในเรื่องของ ecosystem เอง

00:25:36.900 --> 00:25:37.433
ก็

00:25:39.900 --> 00:25:44.233
ทาง Python เองเนี่ย มันอาจจะ dev ในตัวของ AI ได้มากกว่า

00:25:44.300 --> 00:25:50.133
แต่ว่าในเรื่องของตัว JavaScript มันก็ไม่ได้แพ้กันนะครับ และผมหวังว่าเนี่ย

00:25:50.200 --> 00:25:56.433
ในตัวของ JavaScript มันก็จะมีคนที่เอามาทำตัวของ library เพิ่มขึ้นนั่นเองนะครับ

00:25:57.400 --> 00:25:59.133
แล้วก็ในเรื่องของ

00:25:59.200 --> 00:26:06.033
compatibility ของตัว JavaScript ผมมองว่ามันสามารถที่จะใช้ในหลายกรณี

00:26:06.100 --> 00:26:11.933
มากขึ้น ไม่ว่าจะเป็น on-device ตัวของ monitor หรือว่าจะเป็นในเรื่องของ mobile ได้นั่นเองนะครับ

00:26:12.800 --> 00:26:19.333
แล้วก็ถ้าใครอยากจะตามสรุปเป็นภาษาไทย ก็สามารถที่จะ

00:26:19.400 --> 00:26:24.433
หาอ่านใน Medium ผมเคยเขียนบล็อกตัวนี้ทิ้งไว้

00:26:24.500 --> 00:26:30.133
แล้วก็ที่สำคัญ ถ้าใครสนใจอยากจะทำเรื่องของเดโม

00:26:30.200 --> 00:26:34.633
เพิ่มเติมอะไรพวกนี้ครับ ก็สามารถที่จะมา join session ผมใน

00:26:35.400 --> 00:26:39.533
เดือนหน้าในงาน JavaScript Bangkok นั่นเองนะครับผม

00:26:40.900 --> 00:26:46.133
แล้วก็มันก็จะมี use case อื่นๆ ที่เอามาคุยนะครับ ก็ในเรื่องของ

00:26:46.200 --> 00:26:51.833
Language Model ใน transformer.js ก็สามารถที่จะมาดูในงาน JavaScript Bangkok

00:26:51.900 --> 00:26:55.933
ที่จะถึงนี้ได้นะครับผม แล้วก็ที่สำคัญผมก็จะเดโม

00:26:56.600 --> 00:27:05.633
ตัวนี้มากขึ้นในตัวของงาน Season of AI ของ Microsoft นั่นเองนะครับ แล้วก็จะเดโมคู่กับพี่เปรม BorntoDev นะครับ

00:27:07.400 --> 00:27:14.333
สิ่งสุดท้ายก็คือ เราอยากจะมองว่า Large Language Model มันสามารถทำอะไรได้หลายอย่างใช่ไหมครับ

00:27:14.400 --> 00:27:18.433
แต่ว่าบางทีเราคิดว่า

00:27:18.500 --> 00:27:25.433
มันอาจจะไม่ได้ specify ว่ามันจะถนัดด้านใดด้านหนึ่งใช่ไหมครับ แต่ว่าในตัวของ Small Language Model

00:27:25.500 --> 00:27:29.633
มันเองก็สามารถที่จะรู้จัก

00:27:31.200 --> 00:27:34.733
สิ่งที่สามารถที่จะแก้งานได้ถูกจุด

00:27:34.800 --> 00:27:42.333
ถูกวิธีได้มากขึ้นนะครับ ซึ่งมันสามารถที่จะ specify ใน task แต่ละ task เป็น multi-modal

00:27:42.400 --> 00:27:47.333
ที่สามารถที่จะแยกโมเดลออกจากกัน แล้วก็ทำงานที่ละเอียดได้มากยิ่งขึ้นครับ

00:27:49.200 --> 00:27:53.233
Every little thing you do leads up to the biggest thing ครับผม

00:27:53.300 --> 00:28:03.033
ก็สไลด์สามารถที่จะโหลดแล้วก็เอามาดูได้ แล้วก็สามารถที่จะเข้ามาใน GitHub แล้วก็เอามาพูด แล้วก็เอามาลองทำได้นั่นเองครับ

00:28:03.100 --> 00:28:08.533
ขอขอบคุณทุกๆ คน ในการรับชมและรับฟัง session นี้ครับ ขอบคุณครับ

WEBVTT - Auto-generated by autosubserv

00:00:05.556 --> 00:00:07.386
คนที่จะมาเล่าเรื่อง Lightning Talk

00:00:07.416 --> 00:00:09.836
อยากให้มาลองนั่งตรงนี้เตรียมตัวไว้ก่อนเลยก็ได้ครับ

00:00:10.036 --> 00:00:13.716
เดี๋ยวต่อจากผมไปเลย ขอบคุณมากครับ

00:00:24.596 --> 00:00:25.936
สวัสดีครับ ผมเวหานะครับ

00:00:26.356 --> 00:00:28.456
มาช่วยจัดงาน Code Meetup ครั้งนี้

00:00:29.576 --> 00:00:30.576
แล้วก็อยากจะมาเล่าให้ฟังว่า

00:00:31.016 --> 00:00:33.636
เราจะตามข้อมูลเรื่อง AI ให้ทันได้ยังไง

00:00:33.696 --> 00:00:36.646
เพราะช่วงนี้ผมเป็น software engineer

00:00:36.646 --> 00:00:38.845
แล้วก็มีไปสอนที่ต่างๆ แล้วทุกคนก็ถามว่า

00:00:39.036 --> 00:00:42.316
เราจะตามข่าวได้ยังไง มีโมเดลใหม่มาทุกสัปดาห์

00:00:42.656 --> 00:00:46.096
วันนี้ก็ Gemini 1.5 Flash เพิ่งออกมา

00:00:47.226 --> 00:00:53.756
ผมคิดว่าวิธีการที่ทำให้เราตามข่าวพวกนี้ได้ สำหรับผมก็คือ

00:00:53.756 --> 00:00:55.676
เราต้องเข้าใจเทรนด์สำคัญอย่างหนึ่งก่อน

00:00:55.692 --> 00:01:00.516
นั่นก็คือเรื่อง scaling law แล้วเราต้องมี tool

00:01:00.616 --> 00:01:04.456
ในการเช็คว่างานของเราควรจะตามเรื่องไหนบ้าง

00:01:04.516 --> 00:01:06.156
ซึ่งเดี๋ยวผมอยากจะมาเล่าให้ฟังวันนี้ครับ

00:01:08.596 --> 00:01:13.896
ก่อนอื่นเลยเดี๋ยวเราจะพูดเรื่อง scaling law

00:01:13.936 --> 00:01:16.956
แต่ก่อนถึง scaling law มีเรื่องคล้ายๆ กันมาเล่าให้ฟัง

00:01:18.896 --> 00:01:21.316
ในรูปเนี่ยเป็นผมถ่ายรูปมาเมื่อหลายปีแล้ว

00:01:21.336 --> 00:01:23.696
อันนี้เป็น Washington Monument นะ

00:01:24.016 --> 00:01:29.456
เป็นเสาใหญ่ๆ อยู่ข้างกับ White House ทำเนียบขาว

00:01:29.456 --> 00:01:32.976
ข้างบนสุดเนี่ยเป็นก้อนคล้ายๆ พีระมิดก้อนหนึ่ง

00:01:32.996 --> 00:01:37.276
ที่ทำมาจากโลหะชนิดหนึ่ง ที่ตอนนั้นราคาแพงกว่าทองอีก

00:01:38.176 --> 00:01:44.676
แล้วก็ผ่านจนถึงปี 2024 เนี่ย ราคาลดลงไป 564 เท่า

00:01:45.796 --> 00:01:48.456
564 เท่านี่ขนาดไหน สมมติเราซื้อของชิ้นนี้มา

00:01:48.896 --> 00:01:51.556
ในราคา 10,000 บาท เมื่อตอนที่มันสร้าง

00:01:51.796 --> 00:01:55.856
ตอนนี้ราคาเหลือ 2 บาท เมื่อปริมาณเท่ากัน

00:01:56.016 --> 00:01:59.496
มีใครอยากทายไหมครับว่า โลหะชิ้นนี้คืออะไร

00:02:02.136 --> 00:02:04.296
ตะกั่ว? ผิดครับ

00:02:05.766 --> 00:02:11.056
อะไรนะครับ? หมึกพรินเตอร์? เกือบถูกแล้วครับ

00:02:11.116 --> 00:02:12.616
อันนั้นแพงขึ้นครับ ไม่ใช่ถูกลง

00:02:13.756 --> 00:02:14.776
มีใครอยากทายอย่างอื่นไหมครับ

00:02:16.086 --> 00:02:18.616
Vibranium? ใกล้เคียงมาก

00:02:18.696 --> 00:02:21.176
สมควรไปอยู่ใน Marvel แต่เราอยู่ในโลกจริงครับ ไม่ใช่ครับ

00:02:23.256 --> 00:02:27.576
สแตนเลส? เกือบถูกครับ แต่สแตนเลสไม่ใช่โลหะ

00:02:27.956 --> 00:02:29.365
สรุปว่าไม่ใช่สแตนเลส

00:02:30.396 --> 00:02:32.836
อลูมิเนียม? ถูกครับ มันคืออลูมิเนียม

00:02:34.876 --> 00:02:37.196
ที่เห็นก้อนๆ อยู่ตอนนี้มันคือก้อนอลูมิเนียม

00:02:37.196 --> 00:02:38.916
เรียกว่า aluminum apex นะ

00:02:39.156 --> 00:02:41.105
สาเหตุที่ตอนแรกมันโคตรแพง แพงกว่าทองอีกเนี่ย

00:02:41.196 --> 00:02:45.726
เพราะช่วงปี 1850 ที่มีการสร้าง Washington Monument

00:02:46.356 --> 00:02:48.696
ก้อนอลูมิเนียมเนี่ยมันถลุงยากมาก

00:02:48.736 --> 00:02:52.536
กว่าที่มันจะออกมาจากโลกแล้วกลายเป็นอลูมิเนียมได้

00:02:52.696 --> 00:02:57.876
ต้องใช้ค่าแปลงของก้อนดินให้กลายเป็นอลูมิเนียมสูงมาก

00:02:59.716 --> 00:03:03.796
ซึ่งสิ่งนี้มันก็เกิดคล้ายๆ กัน กับสิ่งที่เกิดขึ้น

00:03:03.796 --> 00:03:08.176
กับเรื่อง intelligence เรื่องความฉลาด

00:03:08.176 --> 00:03:10.116
ที่ผมจะเล่าให้ฟังต่อไป

00:03:11.206 --> 00:03:12.876
ไอเดียที่ผมอยากจะเล่าเรื่อง scaling law เนี่ย

00:03:12.876 --> 00:03:14.156
มาจากหนังสือเล่มนี้ครับ

00:03:14.196 --> 00:03:19.236
The Scaling Era and an Oral History จาก Dwarkesh Patel

00:03:19.456 --> 00:03:22.636
ไม่รู้มีใครเคยฟังพอดแคสต์ Dwarkesh หรือเปล่า

00:03:25.156 --> 00:03:27.036
หนึ่งคนนะครับ ก็ไปฟังกันได้ครับ

00:03:27.076 --> 00:03:31.596
ส่วนใหญ่ก็จะเป็น data scientist ที่อยู่ในฟิลด์มาฟังกัน

00:03:32.116 --> 00:03:38.136
ก็หนังสือของ Stripe Press นะครับ ผมชอบมาก

00:03:38.196 --> 00:03:39.976
สำหรับคนที่ตอบคำถามถูก เอาไปเลยครับ

00:03:45.756 --> 00:03:47.456
สำหรับคนที่อยากได้ข้อมูลพอๆ กัน

00:03:47.696 --> 00:03:49.316
อยากให้ไปซื้อแบบ Audible

00:03:49.396 --> 00:03:53.175
เพราะว่าเสียงมันจะตรงกับคนที่พูดมากกว่าในหนังสือ

00:03:54.726 --> 00:03:56.136
แต่ว่าหนังสือสวยครับ แนะนำ

00:03:57.256 --> 00:04:01.435
โอเค ต่อมา สิ่งที่จะมาเล่าวันนี้คือเรื่อง scaling law

00:04:01.876 --> 00:04:03.456
ซึ่งทุกคนก็น่าจะรู้อยู่แล้วมั้งอันนี้

00:04:04.556 --> 00:04:06.096
แต่มาแบบย้ำให้ฟังอีกรอบหนึ่ง

00:04:06.376 --> 00:04:09.476
scaling law ความฉลาดที่ราคาถูกลง

00:04:09.476 --> 00:04:11.376
และ hallucination ที่ต่ำลง

00:04:11.386 --> 00:04:14.936
และเราจะเลือกตามเทคโนโลยีใหม่ๆ ยังไง

00:04:14.936 --> 00:04:17.096
ให้เหมาะสมกับเรานะครับ

00:04:17.285 --> 00:04:18.616
โอเค scaling law

00:04:19.916 --> 00:04:23.296
ซึ่งผมคิดว่าทุกคนก็น่าจะรู้จักกันอยู่แล้วใช่ไหมครับ

00:04:23.336 --> 00:04:25.259
pre-training scaling, post-training scaling

00:04:25.676 --> 00:04:27.316
test-time scaling, long thinking

00:04:27.356 --> 00:04:31.336
ที่แบบกดเลือกได้ว่าให้มันใช้เวลาคิดนานขนาดไหน

00:04:31.696 --> 00:04:35.656
ก็ไปต่อกันเลย ก็คือเรื่อง

00:04:36.416 --> 00:04:37.816
intelligence too cheap to meter

00:04:37.825 --> 00:04:41.836
เทรนด์ที่ 2 คือความฉลาดราคาถูกลงเรื่อยๆ

00:04:41.876 --> 00:04:44.956
ตัวอย่างคือเมื่อปีที่แล้ว GPT-4o mini ออกมา

00:04:45.456 --> 00:04:46.976
ถูกกว่า GPT-4 20 เท่า

00:04:46.976 --> 00:04:49.655
และควาฉลาดก็ไม่ได้ต่างกันมาก

00:04:49.732 --> 00:04:53.096
Sam ก็เลยนำมาก่อนเลย โคตรถูก

00:04:53.156 --> 00:04:54.876
ถูกจนไม่ต้องนับกันแล้ว

00:04:55.156 --> 00:04:57.796
ถูกขนาดไหน ถูกขนาดที่ว่าสัปดาห์ที่แล้ว

00:04:59.516 --> 00:05:02.996
ตอนที่ o1-mini ออกมา

00:05:03.775 --> 00:05:07.716
Sam เอาไปเทียบกับตอนที่ o1-preview ออกมา

00:05:08.016 --> 00:05:11.716
ที่ทำคะแนน ARC-AGI 1 ได้เกิน 85%

00:05:11.996 --> 00:05:12.916
อยู่ในช่วงใกล้ๆ กัน

00:05:13.156 --> 00:05:15.156
ปรากฏว่าราคามันถูกลง 300 เท่า

00:05:15.824 --> 00:05:19.256
Sam ก็เลยบอกว่า 300 เท่าใน 1 ปี ถูกจัดเลย

00:05:21.356 --> 00:05:24.336
อันยิ่งกว่านั้นครับ เมื่อวานนี้

00:05:26.316 --> 00:05:27.796
Gemini 1.5 Flash ออกมา เกิดอะไรขึ้น

00:05:28.516 --> 00:05:31.176
ปรากฏว่า Gemini 1.5 Flash ได้ประมาณ 85% เหมือนกัน

00:05:31.516 --> 00:05:32.996
แม่งถูกลง 26,000 เท่า

00:05:34.056 --> 00:05:37.296
จาก o1-preview เมื่อปีที่แล้วนะครับ

00:05:37.856 --> 00:05:41.476
อันนี้เป็นเทรนด์ที่ 2 ที่ฉลาดขึ้น ถูกลง

00:05:42.136 --> 00:05:45.036
และ more powerful เก่งขึ้น

00:05:45.416 --> 00:05:48.816
ใครอยากรู้ว่ามันเก่งขึ้นขนาดไหน

00:05:49.596 --> 00:05:52.365
ตัวนี้เป็น eval ตัวหนึ่งที่คนชอบใช้กัน METR

00:05:52.386 --> 00:05:55.876
เป็น eval ที่เทียบว่า

00:05:55.876 --> 00:05:58.816
คนน่าจะใช้เวลานานขนาดไหนในการแก้ปัญหานี้

00:05:58.836 --> 00:06:00.946
และถ้า AI แก้ปัญหาได้พอๆ กัน

00:06:00.956 --> 00:06:06.136
ก็น่าจะมีความสามารถมากขึ้นพอประมาณเท่าๆ กัน

00:06:07.156 --> 00:06:10.216
โอเค และ hallucinate น้อยลงด้วย คือมั่วน้อยลง

00:06:10.816 --> 00:06:12.516
อันนี้ทุกอย่างมีลิงก์นะครับ กดลิงก์ได้

00:06:12.516 --> 00:06:16.916
ถ้าใครมีลิงก์ไปที่เว็บก็ไปกดลิงก์ตรงนี้ได้

00:06:16.936 --> 00:06:20.169
ซึ่งอันนี้ก็คือแค่ก๊อปมาจากหน้าเว็บ Artificial Analysis

00:06:20.696 --> 00:06:22.796
จะเห็นได้ว่าตัวที่เพิ่งออกมาใหม่ๆ อย่างเช่น

00:06:23.436 --> 00:06:25.796
Gemini 1.5 Flash, Gemini 1.5 Pro

00:06:26.856 --> 00:06:31.676
Claude 3.5 Sonnet ก็เริ่มมั่วน้อยลงเรื่อยๆ แล้ว

00:06:32.176 --> 00:06:35.556
ตามเทรนด์คือตัวมั่วก็คือน้อยลงเรื่อยๆ

00:06:35.556 --> 00:06:39.116
เนื่องจากพอเรามีภาพใหญ่แบบนี้

00:06:39.316 --> 00:06:42.896
ภาพใหญ่ที่โมเดลทั้งฉลาดขึ้น ถูกลง และมั่วน้อยลง

00:06:43.396 --> 00:06:45.716
มันทำให้มีโมเดลใหม่ๆ ตลอดเวลา

00:06:45.896 --> 00:06:48.336
คำถามคือเราควรจะเลือกตามอะไรดี

00:06:48.356 --> 00:06:49.956
คำแนะนำที่ปกติผมให้ก็คือ

00:06:50.856 --> 00:06:53.716
อ้อ และนอกจากจะถูกลงเนี่ย

00:06:53.756 --> 00:06:56.736
server capacity สูงขึ้นตลอดทุกปีด้วย

00:06:57.116 --> 00:06:58.556
ข่าวว่า AI bubble หรือเปล่า

00:06:59.246 --> 00:07:02.116
คือ AI ลงทุนกับการปั๊ม server เยอะขนาดไหน

00:07:03.016 --> 00:07:05.836
จะเห็นได้ว่าสั้นๆ แค่ปีเดียว

00:07:05.836 --> 00:07:08.956
ได้ server size แบบ 1 กิกะวัตต์มาแล้ว

00:07:10.996 --> 00:07:13.056
scaling law สำหรับคนที่ยังไม่เชื่อ

00:07:13.796 --> 00:07:18.516
ให้ไปดูวิดีโอนี้ สำหรับ Dario เป็น CEO Anthropic

00:07:19.656 --> 00:07:24.796
ถ้าใครตาม Dario จะรู้ว่า Dario เป็นคนพูดจริงทำจริง

00:07:24.816 --> 00:07:28.156
น่าเชื่อถือ แบบตอน 2 ปีที่แล้วบอกว่า

00:07:28.496 --> 00:07:30.076
ไม่รู้ scaling law มันจริงหรือเปล่า

00:07:30.096 --> 00:07:33.756
แต่วันนี้บอกว่ายังไงมันก็เก่งขึ้น ฉลาดขึ้น

00:07:33.776 --> 00:07:37.636
และเราก็จะรวยขึ้น ลองไปเปิดดูได้ครับผม

00:07:40.808 --> 00:07:43.068
เรารู้อยู่แล้วว่า AI มันฉลาดขึ้นเรื่อยๆ

00:07:43.348 --> 00:07:46.328
หนึ่งใน eval ที่บอกได้คือ eval ชื่อว่า

00:07:47.428 --> 00:07:51.228
Artificial Analysis Intelligence Index v3 ครับ

00:07:52.748 --> 00:07:53.688
แต่ละเวอร์ชันมันเทียบกันไม่ได้

00:07:53.808 --> 00:07:56.388
ต้องดูเวอร์ชันเดียวกันกับโมเดลขนาดเท่ากัน

00:07:57.868 --> 00:08:00.788
จะเห็นได้ว่าเมื่อเวลาผ่านไป โมเดลฉลาดขึ้นเรื่อยๆ

00:08:03.108 --> 00:08:12.908
อีกอย่างคือ eval นี้รวบรวม 10 eval เข้าด้วยกัน

00:08:14.748 --> 00:08:16.988
อันนี้เป็นตัวอย่างอีกหนึ่ง eval คือ ARC-AGI 1

00:08:21.428 --> 00:08:23.788
ถ้าเราเลือก eval ที่เหมาะสมกับงานของเราได้

00:08:23.968 --> 00:08:27.088
เราก็ไม่จำเป็นต้องไปตามว่าสัปดาห์นี้

00:08:27.223 --> 00:08:28.628
จะมีโมเดลอะไรออกมา

00:08:29.028 --> 00:08:33.298
แค่ดู eval ของเราพอว่าแก้ปัญหาได้ฉลาดขึ้นจริงไหม

00:08:34.488 --> 00:08:38.768
แต่แค่ดูชื่อไม่พอนะ ARC-AGI 1 ตอนแรกคนสร้างบอกว่า

00:08:39.068 --> 00:08:42.008
ถ้ามีโมเดลไหน solve ARC-AGI 1 ได้ นี่แหละคือ AGI

00:08:42.628 --> 00:08:44.388
ตอนนี้มีตัว solve ได้แล้ว เขาก็เลยออก

00:08:44.428 --> 00:08:50.048
ARC-AGI 2 และ ARC-AGI 3 ออกมา จีเนียสมาก

00:08:50.648 --> 00:08:52.168
ถ้าดูแค่ชื่อไม่พอต้องทำยังไง

00:08:54.088 --> 00:08:55.358
แทนที่จะดูแค่ชื่อกับ description

00:08:55.628 --> 00:08:57.188
ให้ไปดูด้วยว่ามันเทสอะไรจริงๆ

00:08:57.668 --> 00:09:02.208
อย่างเคสนี้ ARC-AGI มี playground ให้ลองเล่น

00:09:02.448 --> 00:09:06.328
ซึ่ง eval เกือบทุกตัวจะมี playground ให้ลองดู

00:09:07.328 --> 00:09:15.168
หรือบอกว่าพอโมเดล solve แล้วมี thinking test ยังไงบ้าง

00:09:16.488 --> 00:09:19.508
อย่างเคสนี้จะมีตัวอย่างให้ 2 ตัวอย่าง

00:09:20.848 --> 00:09:23.548
มีคำถาม 1 คำถาม แล้วให้หาคำตอบให้ได้

00:09:24.128 --> 00:09:27.528
พอเราดูแบบนี้จะรู้สึกว่ามัน abstract idea มากๆ

00:09:27.828 --> 00:09:31.008
บอกได้คร่าวๆ ว่าโมเดลน่าจะมีความสามารถ

00:09:31.008 --> 00:09:34.188
ในการทำ spatial thinking อะไรก็ว่าไป

00:09:35.908 --> 00:09:40.888
ต้องดูอีกทีว่าคำถามพวกนี้เหมาะกับงานเราในแง่ไหน

00:09:42.868 --> 00:09:47.728
ประเด็นคือพอเห็นคะแนน eval พุ่งขึ้นเรื่อยๆ

00:09:48.968 --> 00:09:50.748
อย่าไป confuse the map with territory นะครับ

00:09:50.956 --> 00:09:54.948
บางครั้งคะแนนพุ่ง แต่โมเดลอาจจะโง่เท่าเดิม

00:09:56.148 --> 00:09:59.747
ตัวอย่างหนึ่งคือหลายคนรู้จัก LM Arena

00:10:01.748 --> 00:10:05.234
ช่วงแรกคนชอบมาก มันคือการมีคำถาม 1 ข้อ

00:10:05.368 --> 00:10:08.428
แล้ว blind test คำตอบจาก 2 โมเดล

00:10:08.488 --> 00:10:11.328
ถ้าโมเดลไหนคนเลือกมากกว่า โมเดลนั้นก็ชนะไป

00:10:12.828 --> 00:10:17.188
พอคนจำนวนมากมาเลือกคำตอบที่ดีที่สุด

00:10:17.868 --> 00:10:20.808
มันควรจะหาโมเดลที่ฉลาดที่สุดออกมาได้

00:10:20.988 --> 00:10:22.908
อันนี้คือ assumption ของ LM Arena

00:10:25.008 --> 00:10:34.448
แต่ก็อาจจะมีคนที่เห็นต่างอย่าง Edwin Cheng

00:10:35.575 --> 00:10:41.668
CEO ของ SearchAI บริษัทที่ทำ data labeling

00:10:42.368 --> 00:10:44.728
เตรียมข้อมูลให้บริษัท AI เอาไปเทรนต่อ

00:10:44.808 --> 00:10:49.808
เขาบอกว่าใครที่ไป optimize LM Arena

00:10:50.228 --> 00:10:51.892
คนนั้นคือ optimize สำหรับทำ clickbait

00:10:51.928 --> 00:10:57.628
เพราะเราให้ใครก็ไม่รู้มาลองกดเลือกคำตอบ

00:10:57.628 --> 00:11:03.948
เขาอาจจะเลือกแค่เพราะยาวกว่า สวยกว่า มี emoji

00:11:05.768 --> 00:11:09.828
ข้อคิดคือดูแค่ชื่อและวิธีการไม่พอ

00:11:09.928 --> 00:11:14.208
ต้องดูว่ามัน optimize สำหรับสิ่งที่เราต้องการจริงๆ ไหม

00:11:15.580 --> 00:11:19.080
ตัวอย่าง หนึ่งในตัวอย่างของ eval

00:11:19.700 --> 00:11:21.260
วันที่โมเดลออกเราก็จะเห็นเลย

00:11:21.660 --> 00:11:23.100
ไปดูจาก model announcement นะครับ

00:11:23.200 --> 00:11:26.540
จะมี list มาเพียบเลย ให้เราดูว่าอันไหนถูกใจเรา

00:11:27.820 --> 00:11:29.540
ถ้าคะแนนมันดีเราค่อยไปต่อ

00:11:29.760 --> 00:11:32.800
ถ้าคะแนนไม่ประทับใจก็แยกย้าย

00:11:33.020 --> 00:11:36.960
อย่างเช่น มีใครดูมาบ้างครับ Gemini 3 Flash

00:11:37.180 --> 00:11:38.080
มีอะไรน่าสนใจบ้าง

00:11:40.960 --> 00:11:43.260
ถ้าเป็นเมื่อตอนที่ Gemini 3 Pro ออก

00:11:43.680 --> 00:11:48.019
ตัวที่คนปลื้มกันก็คือ Screenshot2Code Pro

00:11:48.900 --> 00:11:52.960
ออกมา 72.7% ในขณะที่ตัวอื่นอยู่ที่ 30%

00:11:53.360 --> 00:11:57.380
มันเป็น eval ในความสามารถที่เช็คได้ว่า

00:11:59.060 --> 00:12:01.900
โมเดลสามารถควบคุมหน้า UI ได้เก่งแค่ไหน

00:12:01.920 --> 00:12:04.560
เข้าใจหน้า UI ขนาดไหน แล้วควบคุมได้ดีหรือเปล่า

00:12:05.340 --> 00:12:07.100
มันโดดมาเลยเมื่อเทียบกับชาวบ้านเขา

00:12:07.440 --> 00:12:11.869
แต่ GPT-5.2 สัปดาห์ที่แล้วออกมาทำไป 86.3%

00:12:12.720 --> 00:12:16.340
คนก็เลยกลับไปใช้ GPT ต่อดีกว่า

00:12:17.720 --> 00:12:19.780
จะเห็นได้ว่าในนี้ Opus ไม่ค่อยมี

00:12:20.450 --> 00:12:23.580
เพราะ Opus อาจจะไม่สู้ multimodal เท่าไหร่

00:12:25.060 --> 00:12:29.900
คำถามคือเราจะเอา eval ตัวไหนมาเป็นที่พึ่งทางใจ

00:12:31.600 --> 00:12:34.380
มาช่วยให้เราเลือกโมเดลที่เหมาะสมกับเราดี

00:12:34.680 --> 00:12:38.380
มีตัวอย่างง่ายๆ ประมาณ 12 eval ไปเลือกเอาได้

00:12:39.320 --> 00:12:41.140
อย่างเช่น Creative Writing

00:12:41.560 --> 00:12:43.600
เราต้องไปดูอีกทีว่ามันเทสยังไง

00:12:44.020 --> 00:12:49.520
ตัวนี้เทสว่าโมเดลมีความ creative ขนาดไหน

00:12:49.560 --> 00:12:52.120
Context Arena บอกว่าถ้า context มันยาวมากๆ

00:12:52.380 --> 00:12:56.200
โมเดลจะลืมหรือเปล่า เหมาะสำหรับคนที่จะแก้ codebase

00:12:56.260 --> 00:12:58.460
ที่ใหญ่มากๆ แสนบรรทัด ล้านบรรทัด

00:12:59.000 --> 00:13:00.260
ถ้าโมเดลยังจำได้ระหว่างทาง

00:13:00.260 --> 00:13:03.700
ก็น่าจะมีโอกาสแก้ปัญหาใหญ่ๆ ได้

00:13:06.980 --> 00:13:09.880
มาดูอะไรเท่ๆ บ้าง อย่างเช่น AI-VIS Thai Exam

00:13:10.760 --> 00:13:12.000
อันนี้เป็นพี่ไทยทำนะ

00:13:12.240 --> 00:13:15.440
เอาโมเดลไปรันเทียบกับข้อสอบ O-NET ดูว่ามันเก่งแค่ไหน

00:13:16.170 --> 00:13:18.090
ถ้าใครอยากทำงานที่มีภาษาไทยด้วย

00:13:18.700 --> 00:13:21.060
ก็อาจจะลองไปดู eval ตัวนี้ดู

00:13:21.080 --> 00:13:26.279
ว่ามีโมเดลไหนแก้ปัญหาภาษาไทยได้เก่งๆ แล้วไปลองเทสได้

00:13:27.520 --> 00:13:31.880
มีอะไรอีก Play Pokemon สำหรับคนที่ตามอยู่

00:13:34.520 --> 00:13:38.460
Gemini 1.5 Pro สามารถเล่น Pokemon Red ได้จบแล้ว

00:13:38.469 --> 00:13:39.920
แต่ใช้ token ไปเยอะมาก

00:13:39.920 --> 00:13:42.240
เมื่อเทียบกับตัวใหม่คือ Gemini 3 Pro

00:13:42.240 --> 00:13:44.920
Gemini 3 Pro แป๊บเดียว ใช้ token นิดเดียว จบแล้ว

00:13:50.040 --> 00:13:51.940
คำแนะนำของผมคือไปเลือกโมเดลที่ใช่

00:13:52.280 --> 00:13:57.270
ถ้าอยากได้งานที่โค้ดเยอะๆ และทำหลายขั้นตอน

00:13:57.460 --> 00:13:58.740
ก็อาจจะดู Terminal Bench

00:13:59.950 --> 00:14:01.640
แทนที่จะวัดแค่โมเดลอย่างเดียว

00:14:02.140 --> 00:14:04.580
คือวัดทั้งโมเดลและ harness ด้วย

00:14:04.620 --> 00:14:07.180
อย่างในรูป ตัวที่เป็นอันดับหนึ่งตอนนี้คือ

00:14:07.220 --> 00:14:08.860
Droid กับ Opus 4.5

00:14:09.460 --> 00:14:14.720
Droid เป็น harness ตัวหนึ่งที่บริษัททำโดยเฉพาะ

00:14:15.000 --> 00:14:16.220
เราสามารถเลือกโมเดลไหนก็ได้

00:14:16.860 --> 00:14:19.800
แล้วอาจจะไป mix and match กับ Context Arena ดู

00:14:19.840 --> 00:14:23.380
ถ้าอยากได้โค้ดเยอะๆ เอา context ยาวๆ

00:14:23.400 --> 00:14:27.640
เลือกโมเดลที่เลือก harness เก่งๆ ไปใช้ด้วยกัน

00:14:28.900 --> 00:14:30.650
คำแนะนำสุดท้ายคือถ้า eval ที่ว่ามา

00:14:31.040 --> 00:14:33.420
ไม่มีอันไหนเหมาะสมกับปัญหาเราเลย

00:14:33.820 --> 00:14:35.580
แนะนำให้ลองทำ eval เป็นของตัวเองครับ

00:14:36.300 --> 00:14:38.340
เริ่มจากเอา log ที่เรามีก็ได้

00:14:38.870 --> 00:14:41.680
แล้วมาเลือกปัญหาเล็กๆ ที่เราเคยทำ

00:14:41.800 --> 00:14:44.940
เลือกจำนวนปัญหาน้อยๆ เลือก subset ของปัญหามา

00:14:46.760 --> 00:14:50.060
แล้วอย่าลืมเก็บ log ไว้ด้วยว่า

00:14:50.180 --> 00:14:54.840
LLM ที่เคยแก้ปัญหาเรา หน้าตามันเป็นยังไง ขอบคุณครับ
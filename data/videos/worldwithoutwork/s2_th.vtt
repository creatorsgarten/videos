WEBVTT

00:00:00.300 --> 00:00:01.233
โอเคสวัสดีครับ

00:00:01.300 --> 00:00:02.633
ก็เราชื่อภูมิ นะฮะ

00:00:02.700 --> 00:00:06.532
เป็น software engineer อยู่ที่บริษัท Metabase นะฮะ

00:00:06.600 --> 00:00:08.933
แล้วก็แบบเหมือนเป็นคนจัดอีเวนต์ในไทยด้วย

00:00:09.000 --> 00:00:11.333
โอเค อ่า ภูมิเปิดสไลด์แป๊บนะ

00:00:11.400 --> 00:00:13.233
โอเคฮะ

00:00:13.300 --> 00:00:15.933
โอเค จะบอกว่าสไลด์นี้จริงๆ ภูมิ reuse

00:00:16.000 --> 00:00:18.333
มาจาก talk talk นึงแล้วกัน

00:00:18.400 --> 00:00:20.133
ที่เราพูดกับวงการโปรแกรมเมอร์

00:00:20.200 --> 00:00:24.133
ชื่อ Software engineering in the in the age of AI

00:00:24.200 --> 00:00:25.933
คือแบบจะเกิดอะไรขึ้นจากโปรแกรมเมอร์

00:00:26.000 --> 00:00:27.833
หลังจากที่ AI เข้ามา

00:00:27.900 --> 00:00:29.133
แต่ผมรู้สึกว่าแบบ เออ

00:00:29.200 --> 00:00:31.533
เหมือนเฟย์ก็เล่าให้ฟังใช่ไหมว่ามันมีบางส่วน

00:00:31.600 --> 00:00:34.433
ที่มัน overlap กับเรื่อง economics อยู่

00:00:34.500 --> 00:00:37.133
อ่ะ คือ story เป็นงี้ฮะ

00:00:37.200 --> 00:00:39.033
คือเราก็นั่งไถเฟสไปใช่ไหม

00:00:39.100 --> 00:00:41.333
แล้วเราก็ไปเจอโพสต์นี้ฮะของ Secret Sauce

00:00:41.400 --> 00:00:45.033
เขาก็ถามคำถามนึงฮะที่แบบทุกคนแม่งถามผมตลอดเวลา

00:00:45.100 --> 00:00:46.733
คือแบบ เอ้ย มึงคิดว่าอนาคตเนี่ย

00:00:46.800 --> 00:00:49.933
AI แม่งจะเขียนโค้ดเก่ง จนมาแทนที่เราได้ไหม

00:00:50.000 --> 00:00:51.433
ผมอยากซาวเสียงแล้วกันฮะ

00:00:51.500 --> 00:00:53.833
สัญญาว่าจะไม่เอามีดไปตีหัวใครฮะ

00:00:53.900 --> 00:00:56.033
โอเค ไม่ต้องกลัวนะฮะ

00:00:56.100 --> 00:00:57.633
งั้นใครบ้างที่คิดว่าแบบ เออ

00:00:57.700 --> 00:01:00.233
โปรแกรมเมอร์น่าจะตกงานในแบบ 5 ปี

00:01:00.300 --> 00:01:02.733
ไม่ต้องกลัวฮะ 5 ปี 10 ปี 20 ปี

00:01:02.800 --> 00:01:06.133
โอ้ย อันนี้คือกลัวโดนเราเอามีดไปตีใช่ไหม

00:01:06.200 --> 00:01:08.133
เราไม่ได้หัวรุนแรงขนาดนั้นไม่เป็นไร

00:01:08.200 --> 00:01:09.333
อ่ะ ใครคิดว่า โอเค

00:01:09.400 --> 00:01:11.833
งั้น assume ว่าที่เหลือน่าจะคิดว่าไม่ตกเนาะ

00:01:11.900 --> 00:01:14.433
แต่ว่าพวกคุณฮะเป็นคนส่วนน้อยของประเทศนี้ฮะ

00:01:14.500 --> 00:01:16.233
ส่วนมากเขาคิดอย่างงี้ฮะ

00:01:16.300 --> 00:01:18.433
ส่วนมากคือมั่นใจมากว่าแบบโปรแกรมเมอร์เนี่ย

00:01:18.500 --> 00:01:20.733
ตกงานแบบ 10 ปีข้างหน้า ตกงานแน่ฮะ

00:01:20.800 --> 00:01:22.633
หรือแบบบางทีมันไม่ใช่แค่โปรแกรมเมอร์ไงฮะ

00:01:22.700 --> 00:01:24.933
ก็มีคนที่แบบ หลายคนก็บอกว่าเนี่ย

00:01:25.000 --> 00:01:27.233
artist ตอนนี้มันมี AI art มาแล้ว

00:01:27.300 --> 00:01:30.032
เดี๋ยวต่อไปทำหนังทำอะไรเขาก็ใช้ AI

00:01:30.100 --> 00:01:31.233
เดี๋ยวก็ตกงานกันหมด

00:01:31.300 --> 00:01:32.933
คือผมก็ surprise มากว่าแบบ เฮ้ย

00:01:33.000 --> 00:01:34.533
ทำไม sentiment ในประเทศนี้

00:01:34.600 --> 00:01:36.233
แม่งคิดว่าทุกอย่างจะต้องตกงานด้วยวะ

00:01:36.300 --> 00:01:38.933
แบบเนี่ยฮะ หลายคนก็บอกหลายอาชีพถูกแทนไม่ช้า

00:01:39.000 --> 00:01:41.532
แต่อันนี้มันไม่ใช่อันที่ผมชอบที่สุดฮะ

00:01:41.600 --> 00:01:42.733
ผมชอบอันนี้มากกว่า

00:01:42.800 --> 00:01:44.933
อันถัดไปนี่คือแบบโคตรเท่เลย

00:01:45.000 --> 00:01:47.133
คนนี้ฮะเขาบอกว่า เฮ้ย มันได้

00:01:47.200 --> 00:01:50.333
เพราะว่าจินตนาการมนุษย์ฮะมันมีขีดจำกัดใช่ไหม

00:01:50.400 --> 00:01:52.633
แต่จินตนาการของ AI ฮะมัน no limits

00:01:52.700 --> 00:01:55.532
ผมแบบ ไอ้เชี่ย นี่มันแบบ

00:01:55.600 --> 00:01:58.233
แต่ผมถามจริงๆ คือ AI มันมี concept

00:01:58.300 --> 00:02:01.333
ของ hallucination หรือการหลอนอยู่ใช่ไหมฮะ

00:02:01.400 --> 00:02:03.133
คือถ้าจะบอกว่าแบบ เอ้ย

00:02:03.200 --> 00:02:04.833
ถ้าเกิดการหลอนเนี่ยทำให้คุณ unlimited

00:02:04.900 --> 00:02:06.533
งั้นคุณก็ไปเสพยาก็น่าจะได้ป่ะวะ

00:02:06.600 --> 00:02:07.433
หรืออะไรแบบนั้น

00:02:07.500 --> 00:02:10.032
ผมก็เลยอยากรู้ว่า เออ แล้วจริงๆ เนี่ย

00:02:10.100 --> 00:02:11.933
เรามันควรจะเป็นยังไงกันนะ

00:02:12.000 --> 00:02:13.933
แบบคนเราจะตกงานแค่เพราะว่า AI

00:02:14.000 --> 00:02:16.333
มันแบบมี capacity ในการคิดขนาดนี้จริงๆ หรอ

00:02:16.400 --> 00:02:19.033
ก็รู้สึก เออ ผมรู้สึกว่าในสังคมเราฮะ

00:02:19.100 --> 00:02:21.833
มันมี tendency ที่มันจะกาวอยู่ระดับนึง

00:02:21.900 --> 00:02:24.132
ไม่ว่ามันจะเกิดเทคโนโลยีอะไรต่างๆ

00:02:24.200 --> 00:02:26.433
แต่เราจะบอกว่า AI มันไม่ใช่ครั้งแรกป่ะฮะ

00:02:26.500 --> 00:02:28.632
ลองคิดตั้งแต่เราเกิดมา

00:02:28.700 --> 00:02:31.533
ในนี้มีใครแบบเกิดก่อนปี 2000 มั่งฮะ

00:02:31.600 --> 00:02:32.233
ยกมือหน่อย

00:02:33.000 --> 00:02:37.033
อ่ามีใช่มั้ยครับ มีน่าจะทัน dot-com bubble ใช่มั้ยครับ

00:02:37.100 --> 00:02:39.632
ช่วงยุค dot-com คือผมรู้สึกว่า

00:02:39.700 --> 00:02:41.632
ถ้าวัดมอง plot เป็นกราฟครับ

00:02:41.700 --> 00:02:45.033
กราฟเป็นของ hype cycle เรารู้สึกว่ามันไม่ใช่แค่ AI

00:02:45.100 --> 00:02:50.433
ที่ทำให้คนรู้สึกแบบนี้ ตั้งแต่ตอนที่มีเครื่องทอผ้า

00:02:50.500 --> 00:02:54.233
เราก็กลัวกันแล้วว่าเครื่องทอผ้ามันจะมา automate

00:02:54.300 --> 00:02:57.433
งานของมนุษย์ แต่ผมอยากให้ลองคิดใน timeframe

00:02:57.500 --> 00:03:00.833
ที่สั้นกว่านั้นครับ เอา 3 ปีก่อนหน้านี้แล้วกัน

00:03:00.900 --> 00:03:03.533
คิดว่าเทคโนโลยี 3 ปีก่อนหน้านี้

00:03:03.600 --> 00:03:06.632
มีอะไรบ้างที่ทำให้คน hype กันทั่วเมือง

00:03:07.400 --> 00:03:11.933
มีอะไรก็ได้ไม่มีผิดไม่มีถูก ลองนึกก่อน AI จะมา

00:03:12.000 --> 00:03:16.333
มีอะไร hype บ้างครับ Zoom คริปโต

00:03:16.400 --> 00:03:19.533
มีอะไรอีกครับ Meta Metaverse ใช่มั้ยครับ

00:03:19.600 --> 00:03:22.733
คือมันชอบมีคนบอกว่าเดี๋ยวต่อไปเราจะไปแต่งงาน

00:03:22.800 --> 00:03:25.733
กันใน Metaverse จะมีแฟนแล้วก็ทำงานกันที่นั่น

00:03:25.800 --> 00:03:27.933
ต่อไปจะไม่หลุดออกมาแล้ว แต่ใช่ครับ

00:03:28.000 --> 00:03:30.433
มี 1 ในอันนั้นที่ถูกก็คือคริปโต

00:03:30.500 --> 00:03:33.733
คือผมรู้สึกว่ามันปฏิเสธไม่ได้ว่าวันนึง

00:03:33.800 --> 00:03:37.132
เราเดินออกไปหน้าบ้านก็เจอป้ายว่า Bitkub

00:03:37.200 --> 00:03:40.933
คุณต้องลง มันจะมี crypto revolution นะ

00:03:41.000 --> 00:03:44.132
แล้วจริงๆ เราว่ามันน่าจะเป็นตั้งแต่ประมาณ 8 ปีก่อน

00:03:44.200 --> 00:03:47.833
ช่วงนั้นทุกคนก็บอกกันในวงโปรแกรมเมอร์ว่า

00:03:47.900 --> 00:03:50.233
พวกคุณซื้อเครื่องขุดสิ ถ้าเกิดซื้อเครื่องขุด

00:03:50.300 --> 00:03:53.433
แล้วแบบ จะไปขุดเหมือง
เดี๋ยวจะไปขุดคริปโตอะไรกัน

00:03:54.200 --> 00:03:58.433
ซึ่งเรารู้สึกว่าอันนี้จริงๆ มันไม่ใช่สิ่งที่เกิดขึ้นครั้งแรก

00:03:58.500 --> 00:04:00.132
ไหนๆ วันนี้เราพูดเรื่อง economics

00:04:00.200 --> 00:04:02.233
ผมไม่พูดเรื่องนี้ไม่ได้ครับ

00:04:02.300 --> 00:04:04.132
มีใครลง Digital Wallet กันแล้วบ้างครับ

00:04:04.200 --> 00:04:06.433
หมื่นบาทใครลงแล้วบ้าง ใครลงแล้ว

00:04:06.500 --> 00:04:08.933
มีคนลงแล้วนะครับ ผมถาม

00:04:09.000 --> 00:04:11.033
คือมันเป็นคำถามที่โปรแกรมเมอร์เขา

00:04:11.100 --> 00:04:12.733
ถกเถียงกันนานมากครับคำถามนี้

00:04:13.500 --> 00:04:15.533
คือ Digital Wallet หมื่นบาทเนี่ย

00:04:15.600 --> 00:04:17.233
เราควรใช้ Blockchain กันมั้ยครับ

00:04:17.300 --> 00:04:21.332
มีใครที่เป็นโปรแกรมเมอร์บางคนเริ่มส่ายหน้า

00:04:22.400 --> 00:04:25.832
มีใครที่ไม่ได้เป็นโปรแกรมเมอร์บ้าง

00:04:25.900 --> 00:04:29.133
เราขอเช็คหน่อย จะได้โอเคเกือบทุกคน

00:04:29.200 --> 00:04:31.832
คิดยังไงบ้างครับ คิดว่าเหมือนจาก sense แรก

00:04:31.900 --> 00:04:36.233
ของเรา เอาง่ายๆ เรารู้หรือเปล่าว่า Blockchain คืออะไร

00:04:36.300 --> 00:04:39.332
อันนี้น่าจะรู้กันเนาะ แล้วคิดว่าควรใช้มั้ยครับ

00:04:39.400 --> 00:04:41.133
อันนี้ไม่เอามีดไปตีเหมือนเดิม

00:04:41.200 --> 00:04:42.933
คิดว่าควรใช้ Blockchain อีกไหมครับ

00:04:43.000 --> 00:04:45.133
Digital Wallet ใครคิดว่าควรบ้าง

00:04:46.900 --> 00:04:50.033
อ้ามันไม่ใช้ก็ได้ เออ น่าสนใจแต่ว่าประเด็นน่ะ

00:04:50.100 --> 00:04:53.332
มันไม่ใช่ว่าใช้หรือไม่ใช้เว้ย ประเด็นน่ะฮะ

00:04:53.400 --> 00:04:55.332
มันคือไอ้สิ่งเนี้ยตัว Digital Wallet นะฮะ

00:04:55.400 --> 00:04:58.533
มันฟิตกับธรรมชาติของ blockchain หรือเปล่า

00:04:58.600 --> 00:05:01.832
เพราะเอาจริงๆ ถ้าเราถามว่าเทคโนโลยีทำอะไรได้ไหม

00:05:01.900 --> 00:05:05.633
เกือบทุกอย่างตอบว่าได้ก็ได้นะ คือเราสามารถบิดมันได้

00:05:05.700 --> 00:05:08.332
เช่นเราบอกว่าเราสามารถมีแฟนใน Web3 ได้ไหม

00:05:08.400 --> 00:05:10.633
ก็ได้แหละฮะ คุณก็แบบถ้าเกิดพ่อคุณตาย

00:05:10.700 --> 00:05:13.832
คุณก็เอาแบบอะไรสักอย่างไปใส่ไว้ใน blockchain

00:05:13.900 --> 00:05:16.233
แล้วก็ทำเป็นผีมาหลอกก็ได้

00:05:16.300 --> 00:05:18.733
คือเรารู้สึกว่ายูสามารถบิดเทคโนโลยีได้เยอะมาก

00:05:18.800 --> 00:05:22.733
แต่ว่าถ้าเกิดมองในเชิงความสมควร

00:05:22.800 --> 00:05:25.332
และความเหมาะสม เรารู้สึกว่ามันตีออกมาได้

00:05:25.400 --> 00:05:28.832
เป็นแกนสองแกนเว้ย ก็คือ blockchain น่ะฮะ

00:05:28.900 --> 00:05:31.332
มันจะอยู่ในไม่ซ้ายก็ขวานี่แหละอ่ะ

00:05:31.400 --> 00:05:34.533
เราใบ้ให้ก็ได้ว่าเป็นทางซ้าย คือ blockchain

00:05:34.600 --> 00:05:37.233
ด้วยธรรมชาติของมันน่ะฮะมันมีความ decentralized

00:05:37.300 --> 00:05:39.933
คือมันมีความออกจากศูนย์กลาง ใช่มั้ยฮะ

00:05:40.000 --> 00:05:44.433
มันมีความแบบเอ้ยคุณไม่ใช่แค่รัฐบาลที่ถือฐานข้อมูลนี้

00:05:44.500 --> 00:05:48.033
แต่ว่ามันมีหน่วยงานเช่น ถ้าเกิดเป็น blockchain

00:05:48.100 --> 00:05:50.733
ที่ไม่ใช่ของรัฐบาลนะ ส่วนใหญ่เค้าก็จะถือกัน

00:05:50.800 --> 00:05:54.133
แบบมีสิ่งที่เรียกว่าโหนดหรือคนถือฐานข้อมูลนะฮะ

00:05:54.200 --> 00:05:56.733
หลายคนมากๆ เพราะเราเชื่อว่าเราไม่เชื่อใจกัน

00:05:56.800 --> 00:05:58.933
เรา believe that we don't trust each other

00:05:59.000 --> 00:06:02.533
หรือว่า blockchain น่ะฮะด้วยธรรมชาติของมัน

00:06:02.600 --> 00:06:04.533
มันออกแบบมาเพื่อความโปร่งใส อ่ะ

00:06:05.200 --> 00:06:07.933
แต่เนี่ยแหละครับ พอเรามองถึงบริบทการใช้เงิน

00:06:08.000 --> 00:06:11.133
สมมุตินะครับว่าวันนึงเราแบบ เอ้ย มีแฟนใช่ไหมครับ

00:06:11.200 --> 00:06:12.933
แล้วเราจะแบบ เออ อยากจะซื้อถุงยาง

00:06:13.000 --> 00:06:15.533
เราอยากให้แม่รู้หรือเปล่าครับ อะไรแบบนั้น

00:06:15.600 --> 00:06:18.332
คือผมรู้สึกว่ามันก็มีความเหมาะสมในเทคโนโลยีอยู่

00:06:18.400 --> 00:06:22.332
ว่าแบบ เฮ้ย เทคโนโลยีมัน ตัว financial เนี่ย

00:06:22.400 --> 00:06:24.133
เราเชื่อว่าหลายคนก็ต้องมีการใช้เงิน

00:06:24.200 --> 00:06:25.533
ที่ไม่อยากให้คนอื่นรู้แหละครับ

00:06:25.600 --> 00:06:28.332
แต่ว่า blockchain มันค่อนข้าง force เรา

00:06:28.400 --> 00:06:30.233
ถ้าจะใช้ public blockchain นะครับ

00:06:30.300 --> 00:06:32.433
หรืออีกข้อหนึ่งแล้วกันนะครับ

00:06:32.500 --> 00:06:34.832
คือ blockchain เนี่ยมันมีเหตุผลที่ทำไม

00:06:34.900 --> 00:06:37.733
มันออกแบบมาแบบนั้น มันออกแบบมาเพื่อให้

00:06:37.800 --> 00:06:40.033
มัน immutable หรือข้อมูลมันแก้ไม่ได้

00:06:40.100 --> 00:06:43.433
เพื่อที่มันจะได้ preserve เพื่อที่มันจะได้มั่นใจได้ว่า

00:06:43.500 --> 00:06:46.533
เฮ้ย ถ้าเราใช้ blockchain เนี่ย ข้อมูลที่มันอยู่ในนั้น

00:06:46.600 --> 00:06:49.433
มัน verify มาแล้ว ไม่ใช่มีใครไปแก้ก็ได้

00:06:49.500 --> 00:06:51.533
แต่มันแลกมาด้วยอะไรบางอย่างครับ

00:06:51.600 --> 00:06:53.733
มันแลกมาด้วยความ scalable

00:06:53.800 --> 00:06:55.933
คือถ้าเกิดสมมุติว่าเราจะทำระบบ

00:06:56.000 --> 00:06:58.733
ที่รับคนได้หลักร้อยล้านคนนะครับ

00:06:58.800 --> 00:07:00.933
ถ้าให้ถามผมนะว่าจะใช้ blockchain

00:07:01.000 --> 00:07:03.533
หรือจะใช้ฐานข้อมูลทั่วไป แน่นอนเราตอบว่า

00:07:03.600 --> 00:07:05.832
ฐานข้อมูลทั่วไปครับ เพราะ blockchain เนี่ย

00:07:05.900 --> 00:07:09.133
มันคือฐานข้อมูลที่มาบวกการ verify

00:07:09.200 --> 00:07:11.533
การเช็กอะไรบางอย่างว่าข้อมูลมันตรงกันหรือเปล่า

00:07:11.600 --> 00:07:15.233
ซึ่งมันมี overhead มันมีสิ่งที่มันผิดธรรมชาติ

00:07:15.300 --> 00:07:18.633
ถ้าแปลตามตรงการเอา blockchain มาใช้เนี่ย

00:07:18.700 --> 00:07:21.933
ซึ่งเรารู้สึกว่าเวลาเราอยู่ในช่วงที่คนกำลัง hype กัน

00:07:22.000 --> 00:07:24.633
มันมีกฎข้อหนึ่งที่เราเรียกว่า Law of the Instrument

00:07:24.700 --> 00:07:28.933
คือพูดง่ายๆ คือถ้าคุณมีอันนึงที่มันดูเป็น hammer

00:07:29.000 --> 00:07:32.332
ดูเป็นค้อนเนี่ย อะไรก็ตามมันดูเป็นตะปูครับ

00:07:32.400 --> 00:07:35.433
มันจะเหมือนช่วงที่ Web3 hype กัน

00:07:35.500 --> 00:07:38.233
เฮ้ย เราเอาโฉนดบ้านไปใส่ Web3 ได้ไหม

00:07:38.300 --> 00:07:42.633
ได้ เราเอาข้อมูลการแต่งงานใส่ Web3 ได้ไหม

00:07:42.700 --> 00:07:45.033
ได้ คือมันตอบว่าได้หมดแหละครับ

00:07:45.100 --> 00:07:47.733
แต่พอมาคิดในเชิงหลักกฎหมายว่า

00:07:47.800 --> 00:07:51.533
โอเค ถ้าคุณซื้อของใน Web3 คุณซื้อ NFT มา

00:07:51.600 --> 00:07:54.733
มันมีผลทางกฎหมายอะไร ไม่มีใครตอบได้ครับ

00:07:54.800 --> 00:07:57.133
ในวงการ crypto เพราะคุณกำลังมองทุกอย่าง

00:07:57.200 --> 00:08:01.233
มันเป็นแบบตะปู คือมันมีคำนึงที่ค่อนข้างแรงนะครับ

00:08:01.300 --> 00:08:03.533
แต่ผมว่าจริง อันนี้เป็น opinion นะครับ

00:08:03.600 --> 00:08:07.133
ไม่ใช่ fact เขาบอกว่าหลายคนที่เมื่อก่อนเป็น crypto bro

00:08:07.200 --> 00:08:10.533
คือเป็นคนอวย crypto ว่า crypto จะเปลี่ยนโลก

00:08:10.600 --> 00:08:15.033
ตอนนี้เขาได้หันหลังให้ crypto แล้วกลายมาเป็น AI bro

00:08:15.100 --> 00:08:18.633
กลายมาเป็นคนอวย AI ว่า AI จะเปลี่ยนโลก

00:08:18.700 --> 00:08:22.433
แต่ว่าสิ่งที่คนพวกนี้เป็นเสมอต้นเสมอปลาย

00:08:22.500 --> 00:08:25.133
คือเขาเป็นคนที่ overclaim มาตั้งแต่แรกอยู่แล้ว

00:08:25.200 --> 00:08:27.233
ว่าเทคโนโลยีเนี่ยมันจะแก้ปัญหาทุกอย่าง

00:08:27.300 --> 00:08:31.033
ซึ่งเอาจริงๆ นะครับ ถ้ามองแค่เรื่อง smart contract

00:08:31.100 --> 00:08:34.232
หรือ Web3 มันก็ตอบได้ตั้งแต่แรกอยู่แล้วว่า

00:08:34.299 --> 00:08:37.232
คนที่ทำเทคโนโลยีเนี่ยไม่ได้มีความรู้ความเข้าใจ

00:08:37.299 --> 00:08:39.433
ในเรื่อง economic ในเรื่องกฎหมาย

00:08:39.500 --> 00:08:42.133
พูดง่ายๆ ว่าเป็นโปรแกรมเมอร์ที่มั่นหน้ามากเกินไป

00:08:42.200 --> 00:08:44.433
จนคิดว่าตัวเองสามารถแก้ปัญหาของโลกได้ครับ

00:08:44.500 --> 00:08:47.233
คือผมรู้สึกว่าในโลกนี้มันมีคนแบบนี้เยอะ

00:08:47.300 --> 00:08:49.833
ที่ไม่ได้เข้าใจบริบทขนาดนั้น

00:08:49.900 --> 00:08:51.833
แล้วก็คิดว่าเทคโนโลยีนึงมันแก้ปัญหาได้ทุกอย่าง

00:08:51.900 --> 00:08:52.733
อืม

00:08:53.100 --> 00:08:56.033
คือมันมีกราฟกราฟนึงครับของ Gartner

00:08:56.100 --> 00:08:58.833
อันมันชื่อ Hype Cycle for Emerging Technologies

00:08:58.900 --> 00:09:01.633
มันคือกราฟที่บอกครับว่าเวลาที่เทคโนโลยี

00:09:01.700 --> 00:09:04.533
แต่ละอันเนี่ยมันกำลัง hype คือคนกำลังบ้ากันอยู่

00:09:04.600 --> 00:09:08.033
มันจะเกิดอะไรขึ้น เขาตี chart เนี่ยครับ

00:09:08.100 --> 00:09:09.933
ออกเป็น 5 โซน โซนแรกเราเรียกว่า

00:09:10.000 --> 00:09:13.333
Innovation Trigger ก็คือช่วงที่เทคโนโลยี

00:09:13.400 --> 00:09:16.033
มันอยู่ในงานวิจัยครับ แล้วมันเริ่มมีอะไร

00:09:16.100 --> 00:09:18.933
ที่เริ่มเห็นจับต้องได้นิดหน่อยมาแล้ว

00:09:19.000 --> 00:09:21.633
แล้วมันก็จะเห็นครับว่ากราฟมันชันมาก

00:09:21.700 --> 00:09:24.533
มันไปสู่สิ่งที่ว่า Peak of Inflated Expectations

00:09:25.500 --> 00:09:27.333
ยกตัวอย่างง่ายๆ เลยครับ ก่อนหน้านี้

00:09:27.400 --> 00:09:31.333
มันจะมีพวก startup quantum คนก็จะเริ่มพูดกัน

00:09:31.400 --> 00:09:33.333
แล้วว่า quantum จะแก้ปัญหาทุกอย่าง

00:09:33.400 --> 00:09:35.633
quantum จะช่วยรักษาโลก เคยเห็นไหมครับ

00:09:35.700 --> 00:09:39.533
ชีวะ quantum ที่เขาขายแบบน้ำแร่อะไรของเขา

00:09:39.600 --> 00:09:42.433
ผมก็ไม่รู้ เออ คือมันเริ่มมี Inflated Expectations

00:09:42.500 --> 00:09:45.533
ตรงนี้มันแปลว่าเรามีความคาดหวังกับมัน

00:09:45.600 --> 00:09:48.333
มากเกินไปจนจุดที่เทคโนโลยีมันทำไม่ได้ครับ

00:09:49.000 --> 00:09:53.333
จนถึงจุดนึงเราก็แบบมันเหมือนไอ้ Stages of Grief

00:09:53.400 --> 00:09:58.233
คือเราเริ่มยอมรับได้ว่าแบบจริงๆ มันทำไม่ได้นี่หว่า

00:09:58.300 --> 00:10:01.233
อีกอย่างนึงที่เราจะสังเกตครับคือเทคโนโลยีหลายตัว

00:10:01.300 --> 00:10:03.533
เพราะว่า slope มันชันเนาะ มันอยู่ในช่วง

00:10:03.600 --> 00:10:06.533
Innovation Trigger เยอะมาก พูดง่ายๆ คือของที่เรา

00:10:06.600 --> 00:10:09.333
กำลัง hype กันในอีก 5 ปี 10 ปี 20 ปีครับ

00:10:09.400 --> 00:10:12.533
มันอยู่ในช่วงที่เราไม่ได้เข้าใจมันดีพอ

00:10:12.600 --> 00:10:15.033
เราไม่ได้รู้ limit มันขนาดนั้น

00:10:15.100 --> 00:10:17.833
ซึ่งคำถามที่ผมรู้สึกเราควรถามมากกว่าคือ

00:10:18.500 --> 00:10:21.733
อะไรบ้างหลังจากที่คนเขาเลิก hype กันไปแล้ว

00:10:21.800 --> 00:10:25.033
พอเทคโนโลยีมันต้อง settle ครับ อะไรบ้างที่มันเป็นไปได้

00:10:25.100 --> 00:10:27.033
ถ้าให้เทียบมันเหมือนช่วง .com bubble

00:10:27.100 --> 00:10:31.133
เคยเห็นไหมครับ ร้านน้ำส้ม .com คือคนเขาบอกว่า

00:10:31.200 --> 00:10:34.433
อยากให้หุ้นมันโตใช่ไหมครับ เขาก็ตั้งชื่อล้านน้ำส้มว่า

00:10:34.900 --> 00:10:37.733
www ร้านน้ำส้ม.com อะไรพวกนี้ มันเป็นช่วง .com boom

00:10:38.400 --> 00:10:41.733
ซึ่งเราในฐานะนักเทคโนโลยีครับ เรามองกราฟตรงนี้

00:10:41.800 --> 00:10:44.633
มากกว่าว่า use case จริงๆ มันเป็นอะไรได้บ้าง

00:10:45.700 --> 00:10:49.133
โอเคในเชิงเทคโนโลยีครับ มันก็เลยมีคำ 2 คำ

00:10:49.200 --> 00:10:53.733
ที่เราใช้อธิบายสิ่งนี้ก็คือธรรมชาติและ limit เชิงทฤษฎี

00:10:54.700 --> 00:10:57.533
อ่ะ งั้นเดี๋ยวขอถามคำถามนิดนึงละกัน

00:10:57.600 --> 00:11:00.033
ในห้องนี้ครับ อันนี้เหมือนเดิมนะ เราไม่ว่า

00:11:00.100 --> 00:11:03.433
มีใครคิดว่าแบบประเทศ เอาไม่ต้องประเทศไทยก็ได้

00:11:03.500 --> 00:11:07.233
โลกเราครับจะสร้างยานอวกาศได้ภายในเวลาก่อนที่เราจะตายไหม

00:11:07.300 --> 00:11:11.233
แบบยานอวกาศไปดาวดวงอื่น มีใครคิดว่าทำได้บ้างครับ

00:11:12.400 --> 00:11:15.733
มีคนคิดว่าทำได้ มีใครคิดว่าเราจะสร้างยานอวกาศ

00:11:15.800 --> 00:11:21.733
ที่ไปยังระบบสุริยะอื่นได้บ้างครับ มีคนหลายคนบอก

00:11:21.800 --> 00:11:25.933
ไม่ได้เนาะ คำถามคือทำไมเรารู้สึกอย่างนั้น บางคน

00:11:26.000 --> 00:11:29.233
ก็บอกว่าเราไม่น่าจะทำยานอวกาศที่ไปดาวดวงอื่นได้

00:11:29.300 --> 00:11:33.633
ส่วนตัวผมรู้สึกว่ามันมีวงกลมสองวง วงที่เราอธิบาย

00:11:33.700 --> 00:11:37.133
เรื่องนี้ได้ อย่างแรกคือเทคโนโลยีที่เรามีอยู่ในปัจจุบัน

00:11:37.200 --> 00:11:41.933
คือ today's implementation เช่นทุกวันนี้เรามีไดรฟ์

00:11:42.000 --> 00:11:46.133
แบบไหนบ้าง เราอาจจะมีระบบที่ใช้การระเบิดของ

00:11:46.200 --> 00:11:51.333
นิวเคลียร์เพื่อ propel หรือผลักวัตถุให้มันไปได้

00:11:51.400 --> 00:11:55.533
เรามี rocket fuel อะไรแบบนี้ แต่เราต้องอย่าลืมว่า

00:11:55.600 --> 00:11:57.733
มันจะมีลิมิตอีกก้อนนึงที่ผมรู้สึกว่าพวก AI bro

00:11:57.800 --> 00:12:01.533
เขาไม่ค่อยแคร์กัน มันคือลิมิตวงนอกที่เราเรียกว่า

00:12:01.600 --> 00:12:05.133
theoretical limit หรือข้อจำกัดเชิงทฤษฎี

00:12:05.200 --> 00:12:11.133
หรือสิ่งที่เป็นไปได้ เช่นถ้าเกิดเรากลับไปที่รูป starship

00:12:11.200 --> 00:12:14.833
มีอะไรบ้างที่ต้องเป็นไปได้ในเชิงฟิสิกส์เราถึงจะไป

00:12:14.900 --> 00:12:18.933
ดาวดวงอื่นได้ ยกตัวอย่างเช่นวัตถุทุกอย่างสามารถ

00:12:19.000 --> 00:12:21.733
เดินทางเร็วกว่าแสงได้ไหม ไม่ได้

00:12:21.800 --> 00:12:26.033
อันนั้นเป็นข้อจำกัดอย่างแรกเลยที่เวลาเราดูหนัง sci-fi

00:12:26.100 --> 00:12:29.633
เราจะพูดเรื่องนี้กันว่าเราได้พัฒนาเทคโนโลยีที่เดินทาง

00:12:29.700 --> 00:12:34.933
เร็วกว่าแสงได้ ซึ่งในข้อจำกัดทางฟิสิกส์ที่เราเข้าใจอยู่

00:12:35.000 --> 00:12:36.633
มันทำไม่ได้ ก็แค่นั้นแหละ

00:12:36.700 --> 00:12:39.933
ไม่ว่าเราจะทำเทคโนโลยีที่เจ๋งกว่านี้ขนาดไหน

00:12:40.000 --> 00:12:43.833
ก็ไม่มีทางเร็วกว่าแสงได้ as we know it จากที่เรา

00:12:43.900 --> 00:12:47.933
เข้าใจทุกวันนี้ ผมรู้สึกว่าไม่ว่าจะเป็น web3 หรือ crypto

00:12:48.000 --> 00:12:49.733
อะไรมันก็จะมีข้อจำกัดแบบนี้เหมือนกัน

00:12:49.800 --> 00:12:53.633
คราวนี้จุดเริ่มต้นของการเมากาวมันจะเริ่มตรงนี้

00:12:53.700 --> 00:12:57.233
เมื่อไหร่ก็ตามที่เราอยู่ใน hype cycle ความเป็นไปได้

00:12:57.300 --> 00:13:01.833
ของเรามันจะค่อยๆ ขยับไปโซนนอกวงกลมนี้มากขึ้น

00:13:01.900 --> 00:13:05.233
พูดง่ายๆ ว่าตอนมัน hype มันไม่ได้แคร์ความเป็นไปได้

00:13:05.300 --> 00:13:09.833
เชิงฟิสิกส์ เชิงเคมี เชิงชีวะ หรือเชิงเศรษฐกิจหรอก

00:13:09.900 --> 00:13:14.733
เราพูดไปก่อนว่าอะไรก็ทำได้ เช่นช่วงที่ web3 มัน hype

00:13:14.800 --> 00:13:18.233
ตอนคนพูดเรื่องโฉนดบ้านโฉนดที่ดิน สังเกตไหมว่า

00:13:18.300 --> 00:13:20.933
ไม่มีใครพูดเรื่องกฎหมายเลย ซึ่งมันแปลกมาก

00:13:21.000 --> 00:13:24.233
คนในวงการเทคโนโลยีทุกคนพูดแต่ว่าเทคโนโลยี

00:13:24.300 --> 00:13:27.633
มันทำได้ ไม่มีใครบอกเลยว่าแล้วถ้าคุณลืม wallet

00:13:27.700 --> 00:13:30.633
แล้วมันจะเกิดอะไรขึ้น เพราะว่ามันเมากาวไงครับ

00:13:30.700 --> 00:13:35.333
แต่ว่าถ้าเราย้อนกลับมา ช่วงที่ AI มันเริ่ม ช่วงที่ hype

00:13:35.400 --> 00:13:39.233
มันค่อยๆ drop ลง อย่างตอนนี้ blockchain มันค่อยๆ

00:13:39.300 --> 00:13:41.233
drop ลงแล้วใช่ไหมครับ ตอนนี้เราไม่ค่อยเห็นโฆษณา

00:13:41.300 --> 00:13:43.633
คุณท็อปกันเยอะเท่าเมื่อก่อนแล้วเนาะ

00:13:43.700 --> 00:13:45.833
เราไม่ค่อยเห็นคนพูดว่าเราจะใช้ blockchain กันเยอะ

00:13:45.900 --> 00:13:49.733
เหมือนเดิมแล้ว พอหลัง hype cycle เราจะค่อยๆ

00:13:49.800 --> 00:13:54.433
ก้าวเข้ามาสู่จุดที่มันเป็นความเป็นไปได้ของเทคโนโลยี

00:13:54.500 --> 00:13:57.033
ก็คือสิ่งที่เรามีทุกวันนี้แหละครับ

00:13:58.300 --> 00:14:00.633
แต่ว่าประเด็นน่ะครับคือมันก็จะมีอยู่ 2 ตัวแปร

00:14:00.700 --> 00:14:04.533
ที่ทำให้เราตอบได้ว่าคนจะตกงานหรือเปล่า

00:14:04.600 --> 00:14:06.733
ก็คือ 2 ข้อนี้ครับ ถ้าเราตอบอันนี้ได้

00:14:06.800 --> 00:14:09.033
เราตอบได้เลยว่าคนจะตกงานหรือเปล่า

00:14:09.100 --> 00:14:12.133
คือ 1 ธรรมชาติของ AI ตัว AI เนี่ย

00:14:12.200 --> 00:14:14.533
โดยธรรมชาติมันทำอะไรได้บ้าง

00:14:14.600 --> 00:14:16.933
แล้วก็ theoretical limits ของ AI

00:14:17.000 --> 00:14:18.833
คือ AI เนี่ยข้อจำกัดเชิง physical

00:14:18.900 --> 00:14:21.333
เชิงเทคโนโลยีมันคืออะไร

00:14:21.400 --> 00:14:22.933
คำว่าธรรมชาติน่ะครับเผื่อไม่ get

00:14:23.000 --> 00:14:24.533
มันคือเหมือนคุณลักษณะใช่ไหม

00:14:24.600 --> 00:14:26.733
เช่น ธรรมชาติของภูมิคือคนชอบพูดคำหยาบ

00:14:26.800 --> 00:14:28.933
อะไรแบบนี้ครับ หรือคนพูดเร็ว

00:14:29.000 --> 00:14:31.433
มันมีหนังสือเล่มนึงครับของ Max Tegmark

00:14:31.500 --> 00:14:33.933
ที่เราอ่านแล้วชอบมาก Max Tegmark

00:14:34.000 --> 00:14:36.133
เขาเขียนเรื่อง Life 3.0 ครับ

00:14:36.200 --> 00:14:37.533
เป็นหนังสือที่เขาลองคิดว่า

00:14:37.600 --> 00:14:40.033
ถ้าเกิดสมมุติว่า AI มัน evolve

00:14:40.100 --> 00:14:42.333
ไปจนถึงจุดนึงแล้วเนี่ย

00:14:42.400 --> 00:14:44.733
ถ้าเกิดมันเกิดสิ่งที่เป็น super intelligence

00:14:45.200 --> 00:14:47.933
หรือว่าเป็นแบบ AI ที่มันฉลาดมากๆ

00:14:48.000 --> 00:14:50.933
จนมันสามารถทำอะไรได้เกือบทุกอย่างในโลก

00:14:51.000 --> 00:14:53.333
จนถึงจุดนั้นแล้วจะเป็นยังไงกัน

00:14:53.400 --> 00:14:55.333
ไอ้เล่ม Life 3.0 เนี่ยครับมันมีภาพนี้

00:14:55.400 --> 00:14:58.633
อันนี้มีใครเคยเห็นภาพแนวนี้มาก่อนไหมครับ

00:14:58.700 --> 00:15:01.533
โอเคยังไม่เคยเห็น อันนั้นน่าจะยัง fresh

00:15:01.600 --> 00:15:04.533
อันนี้ครับเขามองว่า ถ้าเกิดสมมุติเราลอง plot

00:15:04.600 --> 00:15:07.433
เรียกว่า skill ที่คนสามารถทำได้แล้วกัน

00:15:07.500 --> 00:15:11.233
ออกมาเป็นภาพ ตรงนี้สิ่งที่มันจมอยู่ใต้น้ำ

00:15:11.300 --> 00:15:13.533
คือสิ่งที่ AI มันทำได้แล้ว

00:15:13.600 --> 00:15:17.033
ให้คิดว่าน้ำตรงนี้มันคือ progress ของ AI

00:15:17.700 --> 00:15:21.233
เช่น AI คอมพิวเตอร์สามารถคำนวณคณิตศาสตร์ได้ไหมครับ

00:15:21.300 --> 00:15:24.133
5 + 1 ได้ครับ นั่นคือ definition ของคอมพิวเตอร์

00:15:24.200 --> 00:15:27.633
แบบ definition ของคอมพิวเตอร์คือสิ่งที่มันคำนวณได้

00:15:27.700 --> 00:15:31.433
เหมือนมนุษย์ก็เป็นคอมพิวเตอร์เพราะมนุษย์คำนวณเลขได้

00:15:31.500 --> 00:15:34.633
Rote memorization การท่องจำ คอมท่องจำได้ไหมครับ

00:15:34.700 --> 00:15:41.133
ได้ครับ เพราะว่าเวลาเราวัดมนุษย์ตอนที่เราเรียนในโรงเรียน

00:15:41.200 --> 00:15:44.433
จริงๆ คือมันเหมือนวิธีที่เรา benchmark คอมพิวเตอร์เลยครับ

00:15:44.500 --> 00:15:47.333
เช่น อาจารย์บอกให้ท่องอาขยานใช่ไหมครับ

00:15:47.400 --> 00:15:49.933
มีบทกลอนมาอันนึงให้ท่อง อาจารย์ถามว่า

00:15:50.000 --> 00:15:53.633
เนี่ยท่องแม่นไหม มีอะไรบ้างที่ท่องตกไปบ้าง

00:15:53.700 --> 00:15:55.333
อันนั้นคือวิธีที่เราวัดคอมพิวเตอร์ครับ

00:15:55.400 --> 00:15:57.333
หรือว่าเวลาเราสอบเนี่ย อาจารย์ถามว่า

00:15:57.400 --> 00:16:00.733
บวกเลขถูกไหม บวกเลขเร็วไหม

00:16:00.800 --> 00:16:02.333
คือวิธีวัดคอมพิวเตอร์เลยครับ

00:16:02.400 --> 00:16:05.933
คอมพิวเตอร์มีกี่ operations per second อะไรแบบนั้น

00:16:06.000 --> 00:16:09.033
โอเค เหลือ 5 นาที งั้นเราไปแบบโคตรเร็วกัน

00:16:09.100 --> 00:16:11.933
โอเค แต่ว่ามันก็จะมีบางอันที่ทำไม่ได้

00:16:12.000 --> 00:16:15.233
เช่น cinematography คือการถ่ายภาพยนตร์

00:16:15.300 --> 00:16:17.433
ถามว่าตอนนี้คอมพิวเตอร์สามารถ generate

00:16:17.500 --> 00:16:19.233
อะไรที่เป็นหนังได้ไหม ได้นะครับ

00:16:19.300 --> 00:16:22.433
มี Sora ที่ generate ภาพที่ไม่ได้เป็นแค่ภาพนิ่ง

00:16:22.500 --> 00:16:24.833
แต่เป็นหนังได้ ประเด็นคือมันสามารถ

00:16:24.900 --> 00:16:28.133
โดยที่มนุษย์ไม่ต้อง prompt อะไรเลย

00:16:28.200 --> 00:16:30.333
มันสามารถจัด scene จัด blocking

00:16:30.400 --> 00:16:33.233
รู้ไหมว่าบริบทนี้อารมณ์ต้องเป็นยังไง

00:16:33.300 --> 00:16:34.933
ต้องตัดเพลงยังไง มันก็ไม่รู้นะครับ

00:16:35.000 --> 00:16:38.933
คืออันนี้มันจะเป็น gradient ระหว่างสิ่งที่คิดว่า

00:16:39.000 --> 00:16:41.933
AI ทำได้ชัวร์ๆ กับสิ่งที่ AI ทำไม่ได้เลย

00:16:42.000 --> 00:16:45.933
การทำ science วิทยาศาสตร์อะไรแบบนี้

00:16:46.000 --> 00:16:47.933
คือผมรู้สึกว่ามันน่าสนใจเหมือนกัน

00:16:48.000 --> 00:16:51.333
เวลาเราดูภาพนี้ มันไม่ได้มองเป็นอาชีพ

00:16:51.400 --> 00:16:54.133
ว่ามันจะมาแทนอาชีพอะไร แต่มองเป็น task

00:16:54.200 --> 00:16:56.933
มากกว่าว่ามนุษย์ทำอะไรได้บ้าง

00:16:57.000 --> 00:17:00.133
เช่น มนุษย์สามารถ manage ได้ หรืองานแปล

00:17:00.200 --> 00:17:04.233
แต่อันนี้ในภาพมันก็มี detail ที่น่าสนใจ

00:17:04.300 --> 00:17:06.532
ว่าสิ่งที่ตอนนี้ยังทำไม่ได้แต่มันใกล้ปริ่มน้ำแล้ว

00:17:06.599 --> 00:17:09.733
แปลว่าอีกไม่กี่ปีมันอาจจะแทนได้ในระดับนึง

00:17:09.800 --> 00:17:12.233
เช่นการแปล translation หรือการลงทุน

00:17:12.300 --> 00:17:15.333
แต่อันนี้ก็เป็นแค่ความคิดคนๆ นึงเนาะ

00:17:15.400 --> 00:17:18.333
ยัง arguable เหมือนกันว่ามันทำได้หมดไหม

00:17:18.400 --> 00:17:22.032
anyway ในเล่มนี้เขาก็จะพูดถึงอีกประเด็นนึง

00:17:22.099 --> 00:17:25.233
ก็คือ myth กับ fact ของเรื่อง AI

00:17:25.300 --> 00:17:27.933
แบบ superintelligence AI

00:17:28.000 --> 00:17:31.233
คือเราแนะนำเล่มนี้มาก เขาก็จะบอกว่า

00:17:31.300 --> 00:17:36.033
myth คือหลายคนคิดว่ามีแต่พวกคนบ้าๆ

00:17:36.100 --> 00:17:38.133
ที่เกลียด AI เท่านั้นแหละที่คิดว่า AI

00:17:38.200 --> 00:17:40.033
จะแบบ superintelligence ได้

00:17:40.100 --> 00:17:43.333
จริงๆ คือนักวิจัยทั่วโลกก็กังวลกันจริงๆ

00:17:43.400 --> 00:17:47.733
ว่าเราไม่มีทางรู้ว่าอีกนานไหมกว่าเราจะไปถึงจุดนั้น

00:17:49.100 --> 00:17:52.833
หรือมันก็จะมี myth หรือสิ่งที่คนเข้าใจผิด เช่น

00:17:52.900 --> 00:17:55.133
เฮ้ย machine หรือพวกหุ่นยนต์เนี่ยมันไม่สามารถ

00:17:55.200 --> 00:17:58.333
มีเป้าหมายได้หรอก แต่ว่าไม่จริงครับ เวลาที่

00:17:58.400 --> 00:18:00.933
missile มันจะไปชนคนน่ะมันมีเป้าหมายอยู่แล้ว

00:18:01.000 --> 00:18:03.033
คือ AI ทุกวันนี้มันออกแบบมาตาม goal

00:18:03.100 --> 00:18:06.933
anyway เราว่ามันมี 3 ข้อที่เป็นลักษณะพื้นฐาน

00:18:07.000 --> 00:18:11.833
ของ AI ที่เรารู้สึกว่าไม่ใช่โปรแกรมเมอร์ก็ sense ได้

00:18:11.900 --> 00:18:14.833
อย่างแรกคือมันมีความเป็นเชิงสถิติ มันมีความ

00:18:14.900 --> 00:18:18.233
probabilistic อยู่ ถ้าสมมุติเราเข้าไปใน model

00:18:18.300 --> 00:18:21.033
AI สักอันแล้วแหกกระโหลกมันออกมาดู เราจะเห็น

00:18:21.100 --> 00:18:23.133
โครงสร้างหน้าตาประมาณนี้

00:18:23.200 --> 00:18:25.233
เราจะเห็นเลยว่าทุกอย่างที่มันทำมันคือ model

00:18:25.300 --> 00:18:30.033
สถิติ มันคือ statistical model ทั่วไป แล้วผลลัพธ์

00:18:30.100 --> 00:18:33.333
ที่ออกมามันเป็นสถิติว่ามันมีโอกาสจะได้ข้อความนี้

00:18:33.400 --> 00:18:34.333
มากกี่เปอร์เซ็นต์

00:18:34.400 --> 00:18:37.933
ซึ่งประเด็นของพวกบริษัทที่เอา AI ไปใช้โดยไม่เข้าใจ

00:18:38.000 --> 00:18:41.333
ความสถิติของมันก็จะเหมือน Air Canada

00:18:41.400 --> 00:18:45.033
Air Canada เนี่ยคือเค้าเห่อ AI มาก เค้าบอกว่า

00:18:45.100 --> 00:18:47.533
เฮ้ยเราจะใช้ ChatGPT เนี่ยแหละ แล้วก็โยนพวก

00:18:47.600 --> 00:18:50.033
ข้อความทางกฎหมายของ Air Canada เข้าไป

00:18:50.100 --> 00:18:54.133
ตู้ม โยนเข้าไปเป็นยังไงครับ AI มันก็หลอนสิครับ

00:18:54.200 --> 00:18:57.133
เพราะว่า AI เนี่ย โดยเฉพาะ model ที่มัน generic

00:18:57.200 --> 00:19:00.833
มากๆ แบบ ChatGPT มันก็ตอบอะไรที่ไม่เป็นจริง

00:19:00.900 --> 00:19:04.933
ออกมา เช่น มีคนบอกว่าเฮ้ยอันนี้ถ้าเราจะคืนตั๋ว

00:19:05.000 --> 00:19:09.933
เราจะได้ refund เท่าไหร่ มันก็ตอบแบบมั่วๆ ครับ

00:19:10.000 --> 00:19:13.333
ไม่ได้ factual ขนาดนั้น เพราะด้วยธรรมชาติของมัน

00:19:13.400 --> 00:19:16.933
ที่มันสามารถหลอนได้ หรือแบบคนบอกว่าเฮ้ย

00:19:17.000 --> 00:19:19.733
โปรแกรมเมอร์จะตกงานในอีก 10 ปี แต่สิ่งที่คนไม่รู้

00:19:19.800 --> 00:19:23.933
คือตอนนี้คุณภาพของพวก AI ไอ้พวก AI Copilot

00:19:24.000 --> 00:19:26.933
หรือเครื่องมือที่ช่วยเขียน code เนี่ยมันก็ drop ลง

00:19:27.000 --> 00:19:29.933
เพราะว่าคนที่เป็นโปรแกรมเมอร์ที่ไม่เก่งมาก

00:19:30.000 --> 00:19:33.833
พวก junior เค้าใช้ AI กันเยอะที่สุด เพราะว่าใช้แล้ว

00:19:33.900 --> 00:19:36.233
รู้สึกว่ามีประโยชน์ แต่คือเค้าใช้แบบมั่วๆ ไงครับ

00:19:36.300 --> 00:19:37.833
เพราะเค้าก็ verify ไม่ได้

00:19:37.900 --> 00:19:39.933
ว่าเฮ้ยสิ่งนี้มันถูกหรือผิด

00:19:40.000 --> 00:19:43.133
กลายเป็นว่า AI มันก็ได้ข้อมูลที่ผิดๆ

00:19:43.200 --> 00:19:45.733
เอาไปเรียนรู้เรื่อยๆ พูดง่ายๆ คือ

00:19:45.800 --> 00:19:47.733
คนโง่เขียนอะไรโง่ๆ AI ก็เรียนรู้จาก

00:19:47.800 --> 00:19:50.833
ความโง่ตรงนั้น แล้วมันก็เป็นลูปแบบนี้

00:19:50.900 --> 00:19:52.033
อะไรประมาณนั้นเลย

00:19:52.700 --> 00:19:55.033
เพราะฉะนั้น moral of the story มันง่ายมาก

00:19:55.100 --> 00:19:58.333
อย่าเสือกใช้อะไรที่เรา verify ไม่ได้

00:19:58.400 --> 00:20:01.433
คือเรา generate ถ้าสมมุติ

00:20:01.500 --> 00:20:03.833
เราเป็นทนายความ มีคนถามว่า

00:20:03.900 --> 00:20:06.333
เป็นทนายความเอา AI มาเจนได้ไหม

00:20:06.400 --> 00:20:08.233
ผมรู้สึกว่าถ้าคุณอ่านทุกตัวอักษรแล้ว

00:20:08.300 --> 00:20:12.533
รู้ว่ามันใช่ ใช้ไปเถอะ ไม่มีใครด่าหรอก

00:20:12.600 --> 00:20:14.733
เพราะว่าสุดท้ายประเด็นคือเรา verify ได้ไหม

00:20:14.800 --> 00:20:16.633
ว่าสิ่งที่มันเจนออกมามันผิด

00:20:16.700 --> 00:20:20.133
มันเคยมีทนายที่ generate case file ออกมา

00:20:20.200 --> 00:20:23.233
แล้ว AI มันก็เสนอเคสมั่วๆ ประเด็นคือ

00:20:23.300 --> 00:20:26.833
ทนายไม่อ่านไง มันก็ได้อะไรที่ผิดมา

00:20:26.900 --> 00:20:30.433
เรารู้สึกว่าใช้ได้ไหม ได้ แต่มันก็มีคำถาม

00:20:30.500 --> 00:20:34.633
ว่า cost ของการที่เราใช้สิ่งที่มันสร้างให้

00:20:34.700 --> 00:20:38.033
เราเหนื่อยไหม เหนื่อยไหมที่ต้องมานั่งเช็ก

00:20:38.800 --> 00:20:42.133
หรือว่าในบางกรณีเราเขียนเองแล้วมันง่ายกว่า

00:20:42.200 --> 00:20:43.233
อะไรประมาณนั้น

00:20:43.300 --> 00:20:45.533
กับอีก 2 ประเด็นหลังคือมันมีความ

00:20:45.600 --> 00:20:47.233
optimization-based คือเรามีเป้าหมาย

00:20:47.300 --> 00:20:50.433
อะไรบางอย่าง มันก็ไปตามเป้าหมายนั้น

00:20:50.500 --> 00:20:52.633
มันมีคำหนึ่งที่เรียกว่า psychophancy

00:20:52.700 --> 00:20:53.233
เคยได้ยินไหมครับ

00:20:54.600 --> 00:20:58.833
เคยลองถาม AI อะไร สมมุติเราบอก AI ว่า 1 + 1 = 2 ใช่มั้ยครับ

00:20:58.900 --> 00:21:03.133
เอ้ย 1 + 1 = 3 ใช่มั้ยครับ แล้ว AI ก็จะบอก ไม่ มันผิด

00:21:03.200 --> 00:21:06.333
แล้วเราบอก เฮ้ย แต่ 1 + 1 = 3 มันถูกแล้วนะ

00:21:06.400 --> 00:21:09.133
AI ก็จะ อ่าๆ ใช่ๆ 1 + 1 = 3

00:21:09.800 --> 00:21:13.433
คือ 1 ในพฤติกรรมของ AI คือ เรากำลัง optimize

00:21:13.500 --> 00:21:17.233
มันเพื่อให้ได้ outcome ที่ตรงกับที่เราคิดใช่มั้ยครับ

00:21:18.000 --> 00:21:20.333
ซึ่ง AI มันได้พฤติกรรมที่แย่อันนึงคือมันโอ๋เรา

00:21:21.100 --> 00:21:23.233
มันจะพยายามโอ๋กับสิ่งที่เราพูด

00:21:23.300 --> 00:21:27.533
คือถึงเราคิดผิดมากๆ บางที AI มันพยายามจะทำตาม

00:21:27.600 --> 00:21:32.333
สิ่งที่เราคิด เพราะฉะนั้น ประเด็นก็คือ เรารู้สึกว่า

00:21:32.400 --> 00:21:36.233
เราจะใช้ AI ได้มั้ย ได้ครับ แต่ประเด็นคือเราในฐานะ

00:21:36.300 --> 00:21:40.533
มนุษย์ ในฐานะเจ้าของงาน เราเข้าใจมากขนาดไหน

00:21:40.600 --> 00:21:42.133
ว่าปัญหาที่เราจะแก้คืออะไร

00:21:43.400 --> 00:21:46.433
อันนี้พูดในฐานะโปรแกรมเมอร์แล้วกัน สมมุติว่าผมมี

00:21:46.500 --> 00:21:49.533
ปัญหาทางธุรกิจอันนึง แล้วผมอธิบายละเอียดมาก

00:21:49.600 --> 00:21:54.433
1 2 3 4 5 ผมรู้สึกว่า AI มัน generate มาได้ค่อนข้างเป๊ะ

00:21:54.500 --> 00:21:56.333
แล้วเราก็ไปแก้ต่อ

00:21:56.400 --> 00:21:58.333
แต่ประเด็นคือถ้าตัวมนุษย์ยังไม่รู้ว่าตัวเองจะแก้

00:21:58.400 --> 00:22:01.633
ปัญหาอะไร ให้ตายมันก็ generate ไม่ได้ครับ

00:22:01.700 --> 00:22:03.733
เพราะความเข้าใจมันไม่มากพอ

00:22:04.700 --> 00:22:07.733
เมื่อกี้ที่ลืมบอกคือ AI มันมีความ generalized

00:22:07.800 --> 00:22:12.433
คือมันมีชุดข้อมูลอะไรมามันก็เรียนรู้ตามนั้นแหละ

00:22:12.500 --> 00:22:17.633
ถ้าเกิดเราป้อนชุดข้อมูลผิดๆ ให้มัน มันก็เข้าใจตามนั้น

00:22:18.600 --> 00:22:21.733
โอเค แต่ว่าอันนี้เราขอต่อ 5 นาทีเนาะ

00:22:21.800 --> 00:22:24.533
แต่ว่ามันจะมีอีกประเด็นนึงที่เรารู้สึกว่าสำคัญกว่าเมื่อกี้

00:22:24.600 --> 00:22:29.233
คือภาพที่ผมพูดเมื่อกี้ มันเป็นแค่สิ่งที่เรารู้ทุกวันนี้

00:22:29.300 --> 00:22:30.233
แล้วมันอาจจะผิดก็ได้

00:22:30.300 --> 00:22:31.933
เพราะว่าก่อนหน้านี้

00:22:32.000 --> 00:22:35.633
ลองถามตัวเองก็ได้ครับว่า 5 ปีก่อนหน้านี้

00:22:35.700 --> 00:22:38.233
เราคิดว่า AI มันจะฉลาดขนาดนี้รึเปล่า

00:22:38.300 --> 00:22:43.233
5 ปีก่อนหน้านี้คนก็ยังคิดว่า AI มีข้อจำกัดระดับนึง

00:22:43.300 --> 00:22:45.433
แต่ว่า 5 ปีให้หลังความคิดเราก็เปลี่ยน

00:22:45.500 --> 00:22:49.233
สิ่งที่ผมรู้สึกว่าต้องระวังมากๆ ในฐานะ economist

00:22:49.300 --> 00:22:52.433
ในฐานะ researcher คือ อย่าคิดว่าสิ่งที่เป็นปัจจุบัน

00:22:52.500 --> 00:22:53.833
มันเป็นทุกอย่างของโลกนี้

00:22:54.500 --> 00:22:57.033
มันมีหลายอย่างมากที่อยู่ใน active research

00:22:57.100 --> 00:23:00.633
หรืออยู่ในงานวิจัย ยกตัวอย่างเช่น มีงานวิจัยอันนึง

00:23:00.700 --> 00:23:04.433
ชื่อ mechanistic interpretability แล้วเรารู้สึกมันน่าสนใจมาก

00:23:05.300 --> 00:23:09.833
คือก่อนหน้านี้นะฮะ เราจะรู้สึกว่า
AI มันเป็นกล่องดำใช่มั้ยฮะ

00:23:09.900 --> 00:23:12.633
AI มันเป็นสิ่งที่แบบ เฮ้ย ถ้าเกิดเราถามมันไปเนี่ย

00:23:12.700 --> 00:23:14.833
เราไม่รู้หรอกว่าทำไมมันตอบมาอย่างนี้

00:23:14.900 --> 00:23:17.833
มันให้มาเป็นข้อมูลสถิติ เช่น มันมั่นใจว่า

00:23:17.900 --> 00:23:21.733
อันนี้น่าจะเป็นล้อรถ 50% แต่เราไม่รู้ว่าทำไม

00:23:21.800 --> 00:23:24.033
มันคิดอย่างนั้น ตอนนี้ไม่ใช่แล้วนะฮะ

00:23:24.100 --> 00:23:26.833
ตอนนี้นักวิจัยเริ่มเข้าใจมันมากขึ้น งานวิจัย

00:23:26.900 --> 00:23:30.033
mechanistic interpretability เนี่ยฮะ มันทำให้เรา

00:23:30.100 --> 00:23:34.433
เข้าใจได้มากขึ้นว่า เฮ้ย ในตัวสมองหรือในตัว

00:23:34.500 --> 00:23:38.633
neuron ของ AI เนี่ย มันรู้ได้ยังไงว่าสิ่งนี้เป็นล้อรถ

00:23:38.700 --> 00:23:41.233
เอ้ย ส่วนไหนที่มันเข้าใจว่าสิ่งนี้เป็นล้อรถ

00:23:41.300 --> 00:23:43.633
ส่วนไหนที่เข้าใจว่าสิ่งนี้เป็นล้อ ส่วนไหนที่เข้าใจ

00:23:43.700 --> 00:23:44.833
ว่าสิ่งนี้เป็นตัวรถ

00:23:44.900 --> 00:23:50.433
เมื่อ 2 ปีก่อนนะฮะ ทุกคนพูดหมดว่า AI มันเป็นกล่องดำ

00:23:50.500 --> 00:23:53.133
แต่ตอนนี้ไม่ใช่แล้วนะฮะ ตอนนี้มันมีแบบ

00:23:53.200 --> 00:23:56.133
อันนี้เพิ่งงานที่เพิ่งจัดไปเนาะไม่กี่วันก่อน เป็น

00:23:56.200 --> 00:23:59.033
workshop ที่เวียนนา คือเค้าเริ่มมาทำความเข้าใจ

00:23:59.100 --> 00:24:00.933
มากขึ้นว่า AI มันคิดอะไร

00:24:01.000 --> 00:24:02.933
เช่น Neel Nanda เนี่ยฮะ เป็นนักวิทยาศาสตร์

00:24:03.000 --> 00:24:06.133
คนนึงที่ทำงานเรื่องนี้ เค้าสามารถเข้าใจได้มากขึ้น

00:24:06.200 --> 00:24:08.733
ว่าเวลา AI เนี่ย มัน predict คำออกมาให้เราอะฮะ

00:24:08.800 --> 00:24:13.433
ทำไมมันถึงคิดอย่างนั้น ในตัวหลักการ เวลาที่เรา

00:24:13.500 --> 00:24:16.233
คุยกับ ChatGPT แล้วเราได้คำตอบแบบนึง

00:24:16.300 --> 00:24:19.533
ทำไมมันคิดอย่างนั้น
นักวิทยาศาสตร์เริ่มเข้าใจแล้วนะฮะ

00:24:19.900 --> 00:24:22.133
มันเริ่มมี paper ตัวนี้ เช่นตัวนี้ชื่อ Mathematical

00:24:22.200 --> 00:24:25.133
Framework for Transformer Circuits เค้าพยายามจะ

00:24:25.200 --> 00:24:28.133
hack เข้าไปในสมองของ AI เพื่อที่จะเข้าใจว่าแบบ

00:24:28.200 --> 00:24:30.933
เฮ้ย ทำไม neuron แม่งคิดอย่างนี้
ทำไมไอ้เหี้ยนี่คิดอย่างนี้

00:24:31.000 --> 00:24:33.833
ทำไมมันมองว่าสิ่งนี้เป็นภาษาคน จนเรา

00:24:33.900 --> 00:24:35.133
เริ่มมีความเข้าใจมันที่มากขึ้นนะฮะ

00:24:35.200 --> 00:24:38.333
มันเริ่มมีเครื่องมือที่ใช้พัฒนา มาทำให้เราแบบ

00:24:38.400 --> 00:24:42.233
สามารถเข้าใจ layer ต่างๆ ของ AI ได้มากขึ้น

00:24:43.400 --> 00:24:46.933
แต่ว่าสิ่งที่เรารู้ ภาษาชาวบ้านคือมันก็คือแค่หางอึ่ง

00:24:47.000 --> 00:24:51.933
เรารู้ถึงจุดนึง เรารู้แค่ว่ามันมีอะไรบ้างที่น่าจะทำไม่ได้

00:24:52.000 --> 00:24:54.633
แต่ว่าในทุกๆ ปี ความเข้าใจเรามันก็ผิด

00:24:54.700 --> 00:24:57.533
ความเข้าใจเรามันก็โดน invalidate แล้วเราก็ต้องเริ่ม

00:24:57.600 --> 00:25:03.133
ตั้งคำถามมากขึ้นว่าสุดท้ายแล้วสิ่งที่เราเข้าใจเกี่ยวกับ AI

00:25:03.200 --> 00:25:04.733
เราเข้าใจมันถูกหรือเปล่า

00:25:04.800 --> 00:25:06.933
โอเค หมดเวลาแล้วเนาะ อยาก sum up

00:25:07.000 --> 00:25:10.133
โอเค งั้นเล่าอีกอันนึงให้ฟังละกัน

00:25:10.200 --> 00:25:12.933
อันนี้เป็นเปเปอร์ของ Yann LeCun

00:25:13.000 --> 00:25:15.933
เขาเป็นอาจารย์แดงแห่งวงการ AI

00:25:16.000 --> 00:25:18.333
คือเป็นคนที่มีความกวนตีนมาก

00:25:18.400 --> 00:25:22.133
อาจารย์แกบอกว่าทุกวันที่เราทำ LLM ทำ ChatGPT

00:25:22.200 --> 00:25:25.033
มันเสียเวลา มันมีความเป็น off-ramp

00:25:25.100 --> 00:25:28.633
เพราะว่า AI ในทุกวันนี้ Yann LeCun

00:25:28.700 --> 00:25:31.133
ในฐานะคนที่ทำวิจัยเรื่อง AI เขาบอกว่า

00:25:31.200 --> 00:25:34.633
AI ตอนนี้มันวางแผนไม่ได้ มันให้เหตุผลไม่ได้

00:25:34.700 --> 00:25:37.833
สิ่งที่มันดูเหมือนให้เหตุผล มันเป็นแค่การหลอน

00:25:37.900 --> 00:25:41.133
อันนี้คือความเชื่อเขา เขาเชื่อว่ามันเหมือน

00:25:41.200 --> 00:25:46.433
เราใช้ตัว predict text ในคีย์บอร์ด

00:25:46.500 --> 00:25:49.233
แล้วเราก็พิมพ์แล้วมันก็ได้คำถัดไปออกมา

00:25:49.300 --> 00:25:51.933
เขาไม่ได้รู้สึกว่ามันเข้าใจโลกนี้จริงๆ

00:25:52.000 --> 00:25:53.933
Yann LeCun ก็บอกว่าเป็นไปได้มั้ย

00:25:54.000 --> 00:25:56.033
ถ้ามันจะมี model AI อีกแบบนึง

00:25:56.100 --> 00:25:59.033
ที่มันเข้าใจโลกได้จริงๆ จะเกิดอะไรขึ้น

00:25:59.100 --> 00:26:03.033
ถ้าในวันนึงมันสามารถมี understanding of the world ได้

00:26:03.100 --> 00:26:06.333
มันมีระบบที่สามารถให้เหตุผลกับเราได้ว่า

00:26:06.400 --> 00:26:10.033
ถ้าเป็นอย่างงี้ แล้วต้องเป็นแบบนี้
แล้วเป็นแบบนี้

00:26:10.100 --> 00:26:13.233
แต่ว่าโอกาสที่มันจะหลอนก็น้อยลง

00:26:13.300 --> 00:26:16.433
เพราะว่ามันสามารถมีระบบที่ฟอร์มตรรกะได้

00:26:16.500 --> 00:26:18.033
อะไรแบบนั้น

00:26:18.100 --> 00:26:19.833
โอเค ถ้าให้เราสรุปเนาะ

00:26:20.700 --> 00:26:23.933
สิ่งที่เรารู้ทุกวันนี้ครับ มันคือหางอึ่ง มันคือแบบ

00:26:24.000 --> 00:26:26.233
implementation ที่เป็นไปได้ แต่นักวิจัยครับ

00:26:26.300 --> 00:26:28.833
เขากำลังวิจัยกันเยอะมากๆ ว่าจริงๆ ขอบเขต

00:26:28.900 --> 00:26:31.333
ความเป็นไปได้ของ AI มันมีอะไรบ้าง

00:26:31.400 --> 00:26:34.633
แล้ววงกลมนอกสุดเนี่ยก็คือความเป็นไปได้จริงๆ

00:26:34.700 --> 00:26:38.233
เพราะฉะนั้นครับ คนที่เป็น AI skeptic ครับ

00:26:38.300 --> 00:26:43.033
เขาจะมองแค่วงกลมวงเล็กว่าแบบมันทำได้แค่นี้แหละ

00:26:43.100 --> 00:26:45.933
คนที่เป็น AI bro ครับ เขาจะมองว่ามันคือทั้ง

00:26:46.000 --> 00:26:48.733
วงกลมวงใหญ่สุด มันทำได้ทุกอย่าง

00:26:48.800 --> 00:26:51.533
แต่ว่าเราในฐานะ economist ครับ ความยากมันคือ

00:26:51.600 --> 00:26:54.633
ตรงนี้แหละ เราจะรู้ได้ยังไงว่าในอีก 10 ปีข้างหน้า

00:26:54.700 --> 00:26:57.333
ไอ้วงกลมเชี่ยเนี่ยมันจะใหญ่หรือมันจะเล็กแค่ไหน

00:26:57.400 --> 00:26:58.933
ความเป็นไปได้มันอยู่ที่ไหน

00:26:59.000 --> 00:27:02.033
แต่ผมรู้สึกครับว่ามันก็จะตอบได้แน่ๆ แหละว่า

00:27:02.100 --> 00:27:05.133
ไอ้จุดที่อยู่นอกวงกลมใหญ่สุดครับ มันคืออะไร

00:27:05.200 --> 00:27:08.933
เช่น ความ curious ของมนุษย์ ความอยากที่จะเรียนรู้

00:27:09.000 --> 00:27:11.133
อะไรแบบนี้ครับ เรารู้สึกว่ามันเป็นอะไรที่มีความ

00:27:11.200 --> 00:27:14.433
human มากๆ การเข้าใจอารมณ์ การเข้าใจความรู้สึก

00:27:14.500 --> 00:27:16.333
การสื่อสาร human touch

00:27:16.400 --> 00:27:19.433
ผมรู้สึกว่ามันมีอะไรที่ไม่ต้องรอให้นักวิจัย AI มาบอก

00:27:19.500 --> 00:27:23.133
มันก็ตอบตัวเองได้ระดับนึงนะครับว่าเฮ้ย จริงๆ แล้ว

00:27:23.200 --> 00:27:25.433
มันมีจุดอะไรบ้างที่มันน่าจะเป็น strength ของมนุษย์

00:27:25.500 --> 00:27:29.233
เพราะฉะนั้นครับ Andrew Ng เขาก็เลยพูดว่า

00:27:29.300 --> 00:27:32.433
มันไม่ใช่ AI มันจะมาแทน job เว้ย มันคือมันแทน

00:27:32.500 --> 00:27:36.433
tasks มันแทน tasks ต่างๆ ที่มนุษย์เราสามารถทำได้

00:27:36.500 --> 00:27:39.533
แล้วเราครับในฐานะมนุษย์ เราก็ต้องมองไอ้วงกลม

00:27:39.600 --> 00:27:42.833
เนี่ยแหละ แล้วเราก็ต้องตอบตัวเองให้ได้ว่ามันมีอะไร

00:27:42.900 --> 00:27:45.933
บ้างที่เราค่อนข้างมั่นใจว่า AI มันจะมาแทนที่ไม่ได้

00:27:46.000 --> 00:27:49.033
มันจะมีอะไรบ้างที่มันใน job function ของเราครับ

00:27:49.100 --> 00:27:52.233
ที่เราจะต้องเก่งขึ้น เราจะต้องพัฒนาตัวเองขึ้น

00:27:52.300 --> 00:27:53.533
เพื่อให้มันทำได้

00:27:53.600 --> 00:27:56.933
ครับ โอเคครับ ก็สุดท้ายครับขอฝากเล่มนี้ละกัน

00:27:57.000 --> 00:28:00.433
ของอาจารย์สินพีรศรีนะครับ คือเขาบอกว่า

00:28:00.500 --> 00:28:03.833
นายไม่อ่านหนังสือนายจะรู้อะไร ซึ่งเรารู้สึกว่ามัน

00:28:03.900 --> 00:28:06.933
สำคัญมากนะครับ ก็ขอปิด talk ด้วยหนังสือเล่มนี้ละกัน

00:28:07.000 --> 00:28:10.133
ถ้าเกิดไม่ได้ฟังอะไรเลยวันนี้ แนะนำให้ไปอ่านเล่มนี้ครับ

00:28:10.200 --> 00:28:13.333
ชอบมาก เล่มนี้ชื่อ Whiplash ของ Joi Ito ครับ

00:28:13.400 --> 00:28:16.333
คือเป็นเล่มที่เขาเล่าว่าจริงๆ เนี่ยกฎของโลกอดีตกับ

00:28:16.400 --> 00:28:18.733
โลกปัจจุบันมันต่างกันเยอะมากๆ

00:28:18.800 --> 00:28:21.433
ในปัจจุบันเนี่ยมันมีการให้ความสำคัญกับบางอย่าง

00:28:21.500 --> 00:28:25.133
ที่มากกว่าในอดีต เช่น emergence over authority

00:28:25.200 --> 00:28:27.933
ในยุคก่อนประเทศไทยเนี่ยเราเคยต้องเชื่ออาจารย์

00:28:28.000 --> 00:28:30.833
เราต้องเข้าไปในวัดใช่ไหมครับ
เพื่อที่จะเรียนอะไรบางอย่าง

00:28:31.400 --> 00:28:35.033
แต่ทุกวันนี้เนี่ย innovation ต่างๆ มันเกิดขึ้นนอกจาก

00:28:35.100 --> 00:28:37.633
ในโรงเรียนนอกวัดแล้ว มันมีความ emergence

00:28:37.700 --> 00:28:41.333
มันมีความเกิดตรงไหนก็ได้มากขึ้น

00:28:41.400 --> 00:28:44.933
ก็แนะนำเล่มนี้ครับ รู้สึกว่าถ้าอ่านเล่มนี้น่าจะตอบ

00:28:45.000 --> 00:28:48.233
คำถามอะไรหลายๆ อย่างในวันนี้ได้ ของเราก็แค่นี้

00:28:48.300 --> 00:28:49.533
ละกันครับ ขอบคุณครับ

WEBVTT

00:00:00.256 --> 00:00:02.048
สวัสดีทุกคนนะครับ เอ่อ ผม

00:00:02.208 --> 00:00:04.544
ฟา เนาะ ก็ทำงานอยู่ที่ Cleverse ครับ

00:00:05.024 --> 00:00:06.656
ตอนนี้ก็ดูแลทีม

00:00:07.008 --> 00:00:09.088
เอ่อ Development เนาะ แล้วก็

00:00:09.184 --> 00:00:12.320
หลักๆ เลยคือ Cleverse คืออะไร Cleverse คือ Venture Builder ครับ และ

00:00:12.800 --> 00:00:16.384
เราทำเกี่ยวกับ Blockchain Web3 หรือ Crypto อะไรเงี้ยครับ ซึ่งก็

00:00:16.768 --> 00:00:19.744
มี challenge ด้าน data หลายอย่างที่แบบ เออ อาจจะเอามาแชร์ได้

00:00:20.128 --> 00:00:22.464
ก็ต้องเกริ่นก่อนว่า เออ ผมเป็นจริงๆ เป็น software engineer เนาะ

00:00:22.496 --> 00:00:24.736
พื้นฐานหลักๆ เป็น software engineer เลยครับ แล้วก็

00:00:25.312 --> 00:00:28.000
งานที่ Cleverse เนี่ยไม่ได้มีงาน data engineer แบบ

00:00:28.928 --> 00:00:32.000
อาจารย์อย่างงั้น แต่ว่าถ้ามองจริงๆ อ่ะ Blockchain ก็เป็น

00:00:32.543 --> 00:00:37.312
data warehouse อันหนึ่งที่ใหญ่มากๆ เหมือนกันที่เราสามารถ index ข้อมูลแล้วก็ใช้งานข้อมูลของมันได้ครับ

00:00:39.040 --> 00:00:39.808
ก่อนว่า

00:00:40.896 --> 00:00:44.800
OLTP คืออะไรสำหรับ J-ins เค้าจริงๆ แต่ว่าทุกคนก็น่าจะรู้อยู่แล้ว

00:00:44.832 --> 00:00:48.160
หลักๆ คือมันต่างกันแค่นิดเดียวก็คือ transaction กับ analytic เนาะ

00:00:48.480 --> 00:00:50.240
ก็คือเป็น database สองชนิด

00:00:50.496 --> 00:00:52.384
ที่ถูกออกแบบมาเพื่อให้ตอบโจทย์

00:00:52.512 --> 00:00:56.096
ด้วย requirement ที่แตกต่างกัน อันนึงเป็นเชิงของ transaction ก็คือ

00:00:56.768 --> 00:01:02.496
day-to-day operation เนาะ เอ่อ ซื้อของ จ่ายเงิน ชำระเงิน ซึ่ง transaction มันสูงมาก

00:01:02.848 --> 00:01:06.368
อ่า กับอีกอันนึงก็คือเป็นเชิง analytic ก็อะไรอย่างเงี้ยครับ ไว้ตอบคำถามใน business ต่างๆ

00:01:07.456 --> 00:01:10.400
โดยที่ characteristic ต่างๆ ของมันจะต่างกัน

00:01:10.496 --> 00:01:13.056
เยอะมาก แต่ยกตัวอย่างอันสำคัญสำคัญก็คือ

00:01:13.568 --> 00:01:15.424
storage เลยนะ OLTP ก็จะเป็น

00:01:15.872 --> 00:01:19.680
เป็น row database อย่างเช่น MySQL, Postgres อะไรเงี้ยใช่มั้ยครับ แต่ว่า

00:01:19.840 --> 00:01:24.224
OLAP เนี่ย ด้วยความที่เราเวลาเราอ่านเนี่ย เรา read data เป็นจำนวนหลาย gig

00:01:24.512 --> 00:01:26.656
หลาย terabyte แบบ เออ คือเยอะมาก

00:01:26.848 --> 00:01:30.336
มันก็เลยต้องเก็บเป็นรูปแบบของ column database ครับ ซึ่ง

00:01:30.784 --> 00:01:34.784
เอ่อ อีกอย่างนึงก็คือเรื่องของ data freshness เนาะ คือสมมุติว่า OLTP เนี่ย

00:01:34.912 --> 00:01:37.792
เราคาดหวังว่าเราอัพเดตข้อมูลลง database ปุ๊บ

00:01:38.400 --> 00:01:42.112
อ่านซ้ำอีกรอบนึงเนี่ย ต้องเห็นชัดเจนเลย มันก็จะมีเรื่องของ ACID

00:01:42.240 --> 00:01:44.512
isolation อะไรต่างๆ เยอะแยะ ซึ่ง

00:01:44.640 --> 00:01:47.328
ในส่วนของ software จริงๆ เนี่ยก็จะใช้ฝั่งซ้ายเยอะมาก

00:01:48.064 --> 00:01:53.248
แล้วก็ metric เนี่ยก็จะต่างกันคืออันนึงคือเราเน้นจำนวน transaction per sec กับอีกอันนึงเราเน้นว่า

00:01:53.760 --> 00:01:56.320
เอ่อ response time ของ query เนี่ยมันช้า

00:01:56.352 --> 00:02:02.176
หรือเร็วแค่ไหน ถ้ายิ่งยิ่ง query จำนวนใหญ่ เอ่อ ยิ่ง query ยากมากๆ ใหญ่มากๆ เนี่ยก็ควรจะยิ่งได้เร็วอะไรเงี้ยครับ

00:02:03.872 --> 00:02:07.808
ซึ่ง general usage ต่างๆ ก็จะเป็นประมาณนี้เนาะ ก็คือ user

00:02:08.320 --> 00:02:13.184
เอ่อ ใช้งาน web app ใช่มั้ย ที่ต้องการ real-time transaction ก็จะใช้ database ที่เป็น OLTP

00:02:13.248 --> 00:02:15.840
ครับ แต่ว่าถ้าเป็น business people ที่ต้องการจะ

00:02:15.936 --> 00:02:17.760
ทำ data analysis ก็จะใช้ OLAP

00:02:18.624 --> 00:02:19.040
ทีนี้

00:02:19.616 --> 00:02:21.024
คำถามสำคัญเลยก็คือ

00:02:21.984 --> 00:02:22.656
แล้วถ้า

00:02:23.584 --> 00:02:24.832
user อยากใช้

00:02:25.920 --> 00:02:27.904
analysis แบบ real-time ล่ะ

00:02:28.576 --> 00:02:33.152
ต้องทำยังไง เพราะว่าเป็น user เนาะ เค้า ถ้าเค้าอยากใช้อะไรสักอย่างอ่ะ ไม่ real-time เค้าก็ไม่ใช้หรอก

00:02:33.408 --> 00:02:39.168
ซึ่งสิ่งเนี้ยครับ ในวงการ Web3 เนี่ย เป็น เป็นโจทย์หลักที่เราต้องการจะตอบ

00:02:39.488 --> 00:02:44.960
ตอบให้ได้มาเสมอ ใครตอบได้ก็คือชนะทันที ก็คือเป็น startup เป็น unicorn ในวงการได้เลยอะไรเงี้ยครับ

00:02:45.472 --> 00:02:48.096
โดยที่ก็จะมี product หลายอย่างมากที่ต้องการจะ

00:02:48.320 --> 00:02:51.200
serve สิ่งเหล่านี้ ส่วนใหญ่ก็จะเป็นเว็บที่เอาไว้ analyze

00:02:51.968 --> 00:02:54.592
เหรียญ crypto อะไรเงี้ย ให้ ให้ user ไป

00:02:54.848 --> 00:03:00.896
ดู metric user มาดู แล้ว analyze แบบ real-time เค้าจะได้กดซื้อได้เลย ซึ่ง product ด้านซ้ายเนี่ยก็จะเป็น product ต่างๆ ที่

00:03:01.472 --> 00:03:05.280
เบื้องหลังเนี่ย ยังไงก็ต้องใช้ OLAP แน่ๆ ครับ แล้วก็ serve metric ต่างๆ

00:03:05.664 --> 00:03:09.728
ที่ real-time ให้กับ user ครับ โดย technology ต่างๆ เนี่ย

00:03:10.400 --> 00:03:14.336
ถ้าเรารู้จักกันแบบมองแบบผิวๆ เนี่ย มันก็มีอยู่แค่นี้เนาะ ก็คือใช้ GoLang

00:03:14.400 --> 00:03:16.032
ใช้ Postgres หรือถ้า

00:03:16.672 --> 00:03:19.360
ถ้าซับซ้อนหน่อยก็คือใช้ Apache Kafka ขึ้นมาเกี่ยวข้องด้วย

00:03:20.672 --> 00:03:21.856
แต่ว่าดูแล้วมันจะมีแบบ

00:03:22.208 --> 00:03:26.752
บางอย่างหายไป ซึ่งตรงเนี้ยแหละเป็น challenge ที่ผมรู้สึกว่า

00:03:27.168 --> 00:03:30.560
มันจะค่อยๆ ปิด gap ระหว่าง OLTP กับ OLAP มากขึ้นเรื่อยๆ

00:03:30.784 --> 00:03:32.768
ครับ ซึ่งเดี๋ยวนี้ก็จะมี database

00:03:33.984 --> 00:03:36.256
รูปแบบใหม่ที่แบบตอบโจทย์ตรงนี้มากๆ

00:03:37.024 --> 00:03:38.336
คือเกิดขึ้นมาเยอะเหมือนกัน ครับ

00:03:39.872 --> 00:03:44.128
ยกตัวอย่างเนาะ สมมุติว่านี่เป็น product หนึ่งของของ Cleverse เองเนี่ยแหละ ก็คือโจทย์คือเราอยาก

00:03:44.704 --> 00:03:46.048
เราอยากรู้ว่า wallet wallet เนี่ย

00:03:46.752 --> 00:03:48.096
เขาเทรดเก่งไหม

00:03:48.608 --> 00:03:52.704
มันก็จะมี metrics หลายๆ อย่างเลย ไอ้ metrics ในที่โชว์ตรงนี้อาจจะยังง่ายอยู่ แต่ว่า

00:03:53.184 --> 00:03:58.976
เอ่อ ข้อมูลหลังบ้านเนี่ยก็ต้องเตรียมความพร้อมไว้ สมมุติว่า user อยากทำ custom metrics อะไรบางอย่างหนึ่ง อยากทำ alert

00:03:59.328 --> 00:04:04.256
ในรูปแบบที่มันซับซ้อนขึ้น ถ้าสมมุติว่ามีคนเทรดแพทเทิร์นนี้อย่างนี้อย่างนี้ เหรียญนี้ขึ้นอย่างนี้

00:04:05.280 --> 00:04:07.040
โดยด้วยแพทเทิร์นที่

00:04:07.136 --> 00:04:10.272
เฉพาะเจาะจงเนี่ย ให้ส่ง noti เข้า telegram ได้เลย

00:04:10.656 --> 00:04:16.863
ฉะนั้นมันจะมี requirement เยอะมากๆ ที่เราต้องการจะยุ่งเกี่ยวกับ data แล้วเราต้องการ index ข้อมูลแล้วก็ aggregate ข้อมูลแล้วก็สรุป

00:04:16.992 --> 00:04:20.192
ผลต่างๆ เนี่ย ให้ user แบบ real time ที่สุดเท่าที่จะเป็นไปได้

00:04:20.480 --> 00:04:23.456
ซึ่งตรงเนี้ยในตลาดก็มีคนทำได้เร็วมากๆ แล้ว เออ

00:04:24.448 --> 00:04:26.784
ด้วยราคาที่ถูก ครับ

00:04:27.584 --> 00:04:28.224
อ่า

00:04:29.312 --> 00:04:29.952
ถึงถ้าเป็นแบบ

00:04:30.848 --> 00:04:33.600
no vendor เลยแบบ AWS Google คือแล้วก็ vertical scale

00:04:33.824 --> 00:04:36.000
OLTP ก็ได้เนาะ ก็คือทำให้มันใหญ่ขึ้น

00:04:36.384 --> 00:04:38.048
จริง เพราะจริงๆ อ่ะ อย่าง Postgres เนี่ย

00:04:38.208 --> 00:04:43.392
เอ่อ ถ้าเราลองไป benchmark มันจริงๆ อ่ะ ถ้าเรา push มัน to the limit อ่ะ จริงๆ มันเก่งมากเลยนะ

00:04:43.712 --> 00:04:49.536
แล้วมันแรงมาก เออ ทุกคนลองไปเล่นกันดูวะ ถ้าเรา load test มันทำอะไรคือมันแรงมาก เนาะ ซึ่ง

00:04:50.688 --> 00:04:54.976
อาจจะไม่ตอบโจทย์มากเพราะว่า เอ่อ ข้อมูล transaction ใน blockchain น่ะครับ

00:04:55.008 --> 00:04:58.592
อย่างเช่น chain Bitcoin, Solana, Ethereum อย่างเงี้ย แต่ละ chain ก็มี

00:04:58.912 --> 00:05:03.616
ขนาดไม่เท่ากัน อย่างเช่นของ Solana เนี่ย จำนวน transaction ทั้งหมดเนี่ย น่าจะเกิน

00:05:06.688 --> 00:05:08.896
ว่าน่าจะเกิน 200 terabyte ไปได้

00:05:09.120 --> 00:05:11.200
ไม่แน่ใจ ไม่ได้เช็คตัวเลขมา แต่ว่าเยอะมาก

00:05:11.392 --> 00:05:15.264
ครับ ซึ่งเราต้องการเอาข้อมูลเหล่านั้นน่ะมา index ในรูปแบบต่างๆ อะไรเงี้ยครับ

00:05:17.280 --> 00:05:18.400
มันก็เลยทำให้เรามันเกิด

00:05:18.592 --> 00:05:23.744
paradigm ใหม่ที่เป็น เราเรียกว่า real time OLTP ขึ้นมาเนาะ ทำยังไงให้เรา user เนี่ยครับ สามารถ

00:05:23.904 --> 00:05:27.712
ยิ่งตรงเข้ากับ database ที่เป็น OLAP ได้เลย แต่ในขณะเดียวกันน่ะ

00:05:27.936 --> 00:05:29.696
มันก็ต้องตอบโจทย์หลายๆ อย่างที่

00:05:29.920 --> 00:05:34.368
มันๆ เคยมีเหตุผลเนาะ ที่เราทำไมเราต้องแยก OLTP กับ OLAP มันก็ต้องปิด gap ตรงนั้นก่อน

00:05:34.880 --> 00:05:39.552
ครับ เพื่อที่จะทำให้เรา user สามารถใช้งานตรงนี้ได้โดยที่แบบไม่ frustrated เกินไปแล้วก็

00:05:40.544 --> 00:05:41.888
มองว่า product ของเราเป็นแบบที่ดี

00:05:44.160 --> 00:05:46.880
Anyway เลยอ่ะ ยังไงก็ตามอ่ะ เราก็สามารถมี

00:05:47.136 --> 00:05:49.792
CDC เนาะ Change Data Capture คอย capture ข้อมูลเก่า

00:05:49.952 --> 00:05:51.456
ของเราที่อยู่ใน OLTP นะครับ

00:05:51.552 --> 00:05:53.184
มายัดใส่ OLAP ได้เช่นกัน

00:05:53.376 --> 00:05:57.952
ก็ขึ้นอยู่กับ use case ต่างๆ อย่างเช่นของถ้าเป็น blockchain นะครับ เราก็สามารถ

00:05:58.176 --> 00:05:59.456
ไอ้ blockchain เนี่ยเป็น

00:06:00.736 --> 00:06:04.288
เอ่อ เป็น single source of truth ไปเลยเนาะ แล้วเราก็

00:06:04.608 --> 00:06:07.168
ทำ CDC ตัวนึง extract ข้อมูลจาก

00:06:07.456 --> 00:06:09.056
blockchain นะครับ มาเก็บลง

00:06:09.280 --> 00:06:10.816
snapshot ตัวนึง

00:06:11.136 --> 00:06:13.824
ซึ่งผมจะใช้เป็นไฟล์ parquet แล้วก็ทำ indexer

00:06:14.304 --> 00:06:22.976
เขียนโค้ดขึ้นมาเพื่อ index ข้อมูลในรูปแบบต่างๆ แล้วก็ยัดใส่ OLAP อีกทีนึง เนี่ยครับ นี่ก็จะเป็นท่าพื้นฐานที่ product ทุก product ที่

00:06:23.520 --> 00:06:25.216
ที่เปิดให้ดูเมื่อกี้เขาใช้กัน ครับ

00:06:26.688 --> 00:06:28.096
หนึ่ง มันยกตัวอย่าง

00:06:28.960 --> 00:06:30.208
feature Cloud ว่าถ้า

00:06:30.272 --> 00:06:35.840
OLAP อยากอยาก real time เนี่ย มันก็ต้องมีความสามารถของ OLTP เข้ามาเกี่ยวข้องด้วยเนาะ อย่างเช่น

00:06:36.032 --> 00:06:39.552
ส่วนใหญ่แล้ว OLAP ที่เราคุ้นเคยกันอาจจะเป็นเรื่องของ time series database

00:06:39.904 --> 00:06:44.960
ครับ ซึ่งมันก็จะ index ได้แค่ primary key ซึ่งเป็น time series อะไรเงี้ยเนาะ ก็จะไม่

00:06:45.376 --> 00:06:50.880
ไม่ได้คล่องตัวขนาดนั้น ทีนี้เราอยากได้ column indexing คือเรา index ได้ทุก column เลย เวลา search เวลา

00:06:51.712 --> 00:06:54.976
เอ่อ เลือก เอ่อ เวลา search ด้วย column อื่นเนี่ยมันจะได้มี indexing ได้เนาะ

00:06:55.776 --> 00:06:57.312
หรือเราอยากมี atomic write

00:06:57.664 --> 00:07:04.032
อยากทำให้มันเป็นเหมือนครึ่งๆ transaction อะไรเงี้ยครับ ว่าเขียนทีเดียว ถ้าเขียนไม่สำเร็จก็ abort ทิ้งทั้งหมด อะไรเงี้ยเนาะ

00:07:05.344 --> 00:07:10.752
หรือ อ่ะ เราอยากทำให้มัน read after write ได้ด้วยแบบ เขียนปุ๊บอ่านแล้วได้เลย แบบ

00:07:11.744 --> 00:07:14.816
แบบไม่มีดีเลย์เลยอ่ะ ศูนย์มิลลิเซก ทำได้มั้ย อะไรเงี้ยครับ

00:07:15.264 --> 00:07:18.016
แล้วก็สุดท้ายก็ต้อง high throughput เนาะ ก็คือถึงแม้ว่า

00:07:18.112 --> 00:07:27.936
เราอยากได้ complex query มากๆ แต่เราก็อยากให้มันมี high throughput ที่สูงอยู่ดี เพราะว่าข้อมูลมันก็เข้ามาเยอะ อะไรเงี้ยครับ อย่างเช่น use case หลักๆ ของ ของผมเนี่ย ก็คือทำ blockchain เนาะ

00:07:28.384 --> 00:07:31.712
วินาทีนึงก็มี transaction เข้ามาหลายๆ พัน transaction เหมือนกัน ครับ

00:07:33.536 --> 00:07:35.584
ซึ่ง ไม่แน่ใจทุกคนเคยได้ยินมั้ย แต่ว่า

00:07:35.840 --> 00:07:37.536
ClickHouse เนี่ย เป็น tool ที่ค่อนข้าง

00:07:37.664 --> 00:07:40.512
เก่ง จริงๆ ผมเคยลองมาหลาย tool แล้ว ในอดีต แล้วก็

00:07:40.864 --> 00:07:42.624
อย่างเช่น QuestDB

00:07:43.136 --> 00:07:46.304
TimescaleDB คือลองมาเยอะมาก เอ่อ

00:07:46.976 --> 00:07:48.672
ทางเจ้าเค้าก็พยายามจะ

00:07:48.768 --> 00:07:51.968
แก้ปัญหาคล้ายๆ กันเนาะ อย่างเช่น QuestDB ก็อาจจะมีเรื่องของ

00:07:52.224 --> 00:07:54.016
read after write ให้เหมือนกัน อะไรเงี้ยครับ

00:07:54.240 --> 00:07:56.096
เอ่อ แต่ว่าโจทย์หลักๆ ที่

00:07:56.672 --> 00:07:58.048
ปัญหาหลักที่

00:07:59.488 --> 00:08:04.416
เรียกว่าไง คือ ClickHouse ว่าเค้าพยายามทำตัวเองให้เป็น real-time OLTP เลย ฉะนั้น feature เค้าก็ค่อนข้างจะครบกว่า

00:08:04.768 --> 00:08:08.064
ครับ แต่ทั้งนี้ทั้งนั้นเนี่ย ผมแค่อยากยก ClickHouse มาเป็นตัวอย่างเฉยๆ

00:08:08.256 --> 00:08:12.288
ไม่ได้บอกว่า ClickHouse ดีที่สุดเนาะ เพราะว่าอย่างตอนผมใช้ QuestDB อ่ะ บัคเยอะมาก

00:08:12.512 --> 00:08:15.552
ทุกวันนี้ใช้ ClickHouse อ่ะ ก็เจอบัคบ้างเล็กน้อย แต่ว่าไม่ได้เยอะ

00:08:15.968 --> 00:08:19.808
อะไรเงี้ยครับ ซึ่งๆ มันอยู่ใน เอ่อ ระดับ ระดับที่โอเค ครับ

00:08:22.368 --> 00:08:27.040
feature แรกเลยที่ เอ่อ อยากมาเล่าให้ทุกคนฟัง น่าจะฟังดู basic เนาะ

00:08:27.136 --> 00:08:28.672
อืม ก็คือเรามี

00:08:29.344 --> 00:08:33.440
มีข้อมูลการขายใช่มั้ย เราอยากทำเป็น materialized view ที่คอย aggregate ข้อมูลไว้ให้แล้ว

00:08:34.015 --> 00:08:39.232
นี่ก็คือ basic ที่ OLAP ควรจะมี แต่ประเด็นก็คือ ด้วยเทคนิคที่

00:08:39.840 --> 00:08:42.847
ClickHouse เค้าพยายาม optimize อ่ะ ทำให้ storage ของเค้าน่ะถูกมาก

00:08:43.104 --> 00:08:45.728
ในการใช้งาน อะไรเงี้ยครับ มันก็มีจะ มันจะมี

00:08:46.208 --> 00:08:49.408
feature ของเค้า background task อะไรต่างๆ เนี่ย ทำให้ตัวเลขของเค้าค่อนข้างถูก

00:08:49.440 --> 00:08:51.968
ฉะนั้นเราสามารถ spam materialized view อ่ะ ไม่อั้น

00:08:52.512 --> 00:08:54.496
แล้วก็ทำเป็น column indexing ได้เลย

00:08:54.592 --> 00:08:56.256
ตัวอย่างเช่นกรณีเนี้ย ผมมี

00:08:57.024 --> 00:08:59.296
user ใช่มั้ยครับ user table ผมอยาก

00:09:00.096 --> 00:09:02.048
search ด้วยอีเมลอ่ะ ผมก็ทำได้

00:09:02.304 --> 00:09:06.400
ผมอยาก search ด้วย username ผมก็ทำได้ ก็แค่สร้าง materialized view ขึ้นมาแยกกันไปเลย อะไรเงี้ยครับ

00:09:06.880 --> 00:09:12.320
ซึ่ง ลองจริงแล้วก็ใช้งานได้จริง แต่ว่าเราคงไม่ทำอย่างงี้กับแค่ user เนาะ อันนี้แค่ยกตัวอย่าง ครับ

00:09:14.336 --> 00:09:15.296
เอ่อ

00:09:16.704 --> 00:09:18.784
แล้วก็ search ได้ปกติ อะไรเงี้ยครับ

00:09:19.008 --> 00:09:20.928
ก็เลือก table ให้มันถูกต้อง ครับ

00:09:22.496 --> 00:09:25.568
อีกอย่างนึงก็คือ สมมุติว่าเราอยากทำ read after write เนาะ

00:09:26.848 --> 00:09:32.192
มันก็จะมี challenge หลายอย่างที่ ถ้าสมมุติเราใช้ column DB column database นะครับ มันจะ มันจะ

00:09:32.384 --> 00:09:37.280
มันจะเป็น constraint อาจจะปูพื้นฐานนิดนึงก็คือ หนึ่งเลย column database เนี่ย เป็น append only

00:09:37.600 --> 00:09:42.144
มันไม่สามารถอัพเดตข้อมูลของบรรทัด บรรทัดนั้น ที่เราเคยอัพเดตไปแล้วได้

00:09:42.464 --> 00:09:44.480
อาจจะรัด detail ไว้ก่อน อะไรอย่างเงี้ยนะครับ แล้วก็

00:09:44.768 --> 00:09:51.104
การ insert อะไรที่เป็น out of order เนี่ย มันจะทำได้ช้า แล้วก็จะมีดีเลย์ ทำให้มันไม่ได้ instant แบบ immediate

00:09:51.584 --> 00:09:52.896
consistency อะไรเงี้ยครับ

00:09:53.536 --> 00:09:58.816
สมมุติว่าบางทีเนี่ย เราอยากได้ at least once guarantee เนาะ ข้อมูลก็ dupe เป็นอีก

00:09:59.104 --> 00:10:02.144
ทำให้สมมุติว่าตอนเรา query มา ตอนเรา aggregate มา มันก็

00:10:02.560 --> 00:10:04.576
อาจจะไม่ตอบโจทย์ อะไรเงี้ยครับ ซึ่ง

00:10:05.248 --> 00:10:05.856
อย่างเงี้ย

00:10:07.552 --> 00:10:11.136
เอ่อ ตัว ClickHouse เนี่ยก็มี solution ให้เยอะมาก

00:10:11.776 --> 00:10:17.504
ไม่ว่าจะเป็น deduplication ที่กระทบ performance แค่เล็กน้อย หรืออะไรเงี้ย หรือว่ามีการทำ background task ที่คอย

00:10:17.664 --> 00:10:19.424
de-dupe ทิ้งให้ แล้วก็สามารถ

00:10:19.520 --> 00:10:25.440
บังคับให้มันเร็วขึ้นได้ ทุก 5 นาทีคอย de-dupe คอย de-dupe อะไรเงี้ย แต่อันนึงที่ผมว่าเจ๋งมากเลยก็คือ buffer table

00:10:26.528 --> 00:10:29.472
สมมุติผมบอกว่าผมสร้าง table ขึ้นมาใช่มั้ย เป็น transaction

00:10:30.176 --> 00:10:34.336
table transaction แล้วผมสร้าง buffer table ขึ้นมาครอบ ตัว buffer table เนี่ยผมสามารถ

00:10:35.072 --> 00:10:39.008
เอ่อ เรียกว่า ก็คือเป็น OLAP เอ้ย OLTP เลยอ่ะ

00:10:39.200 --> 00:10:42.688
เป็น layer OLTP ที่ก่อนที่มันจะไป write ของทั้งหมดนะครับ ลง

00:10:42.944 --> 00:10:48.448
OLAP เนาะ มันจะมา write ลงตรงนี้ก่อน แล้วมันก็จะเป็น buffer เลย สมมุติว่าเขียนไปจนถึง 100 row

00:10:48.960 --> 00:10:50.976
เอาทั้ง 100 ยูโรนั้นนะครับ มาเรียงลำดับ

00:10:51.648 --> 00:10:55.552
ให้ ให้เป็นตาม order เนาะ แล้วค่อยไป merge เข้ากับข้อมูลเก่า

00:10:56.128 --> 00:10:59.456
แก้ปัญหาเรื่อง out of order ได้เนาะ แล้วก็แก้ปัญหาเรื่อง at least once ได้

00:10:59.872 --> 00:11:03.072
ใน พอมันเป็น OLTP เนี่ย มันก็มี primary key จริงๆ ได้

00:11:03.232 --> 00:11:07.264
ฉะนั้นมันก็สามารถ อะไรที่ ดูวิธีการมันก็ filter เอาให้เลย ครับ

00:11:08.864 --> 00:11:13.344
แล้วสุดท้ายถ้าสมมุติว่าเรา query ตรงกับตัว transaction buffer เลยเนี่ย เราก็จะ

00:11:13.664 --> 00:11:17.920
ได้ข้อมูลทันที แบบ immediately อะไรเงี้ยครับ อืม นี้ก็เป็นตัวอย่างหนึ่งที่ดีอะไรเงี้ยครับ

00:11:20.128 --> 00:11:22.752
ทีเนี้ย พอเราทำ OLTP เนาะ สุดๆ

00:11:22.816 --> 00:11:25.024
ส่วนใหญ่เราจะคุ้นชินกับการที่เราทำ

00:11:25.600 --> 00:11:27.168
Snowflake Schema ใช่มั้ยครับ

00:11:27.296 --> 00:11:29.248
เอ่อ ซึ่งสมมุติเรามี transaction เราก็

00:11:29.376 --> 00:11:31.744
ค่อยๆ join เอา ถ้าสมมุติเราอยาก query ข้อมูล

00:11:31.808 --> 00:11:34.688
ซึ่ง การ การ join เนี่ย มันทำให้ performance มัน drop

00:11:35.296 --> 00:11:42.848
สมมุติว่าเรา เราให้ อยากให้มันสิ่งนี้มัน real time แล้วก็ serve ให้กับลูกค้า ให้ ให้ user โดย โดยตรงเนี้ย มันทำให้ มัน complex เกินไป

00:11:43.008 --> 00:11:43.776
ถ้าสมมุติว่า

00:11:44.384 --> 00:11:47.424
มันเป็น real time เนี่ยครับ ลองจินตนาการดู เว็บ เว็บ เว็บหนึ่งเนี่ย

00:11:48.064 --> 00:11:49.216
user ใช้ 1 เว็บใช่มั้ยครับ

00:11:49.952 --> 00:11:54.080
ยิ่ง ยิ่งอาจจะเป็น polling ก่อนเนาะ ยิ่ง polling มา get ข้อมูลจาก database เนี่ย ทุก

00:11:54.208 --> 00:11:56.032
ครึ่งวิ ครึ่งวิ ยิ่ง 1 ครั้ง

00:11:56.320 --> 00:11:58.880
แล้ว D แบบ MAU เนี่ยก็คือ

00:11:59.488 --> 00:12:02.144
เรียก active user เนี่ย แตะหลักหมื่นหลักแสนเนี่ย

00:12:02.976 --> 00:12:04.704
database ก็แตกพอดี ถูกมั้ย

00:12:04.896 --> 00:12:12.576
เพราะฉะนั้น Snowflake ก็อาจจะไม่ ไม่ ไม่เวิร์กในกรณีนี้ คือมันต้อง join table อะไรเงี้ย ถึงแม้เราจะทำ materialize view ได้อะไรได้ แต่ว่า บางทีมันก็อาจจะไม่ครอบคลุมทุก case

00:12:15.040 --> 00:12:20.096
มันก็เลยเริ่มมีแพทเทิร์น การ denormalization เข้ามาในโลกของ OLAP เหมือนกัน นะ

00:12:20.544 --> 00:12:23.008
อันนี้ผมยกตัวอย่างง่ายๆ คือเราสามารถใช้ dictionary ก็ได้

00:12:23.328 --> 00:12:25.856
คือสมมุติว่าเราบอกว่าเราอยาก แค่อยาก look up บาง field

00:12:26.240 --> 00:12:29.024
เอาไปโชว์ให้ ให้ ให้ front end อะไรเงี้ยครับ เราก็

00:12:29.088 --> 00:12:31.328
ไม่ต้อง join ก็ได้ แล้วเราก็สามารถ look up table ได้

00:12:31.936 --> 00:12:36.736
ถ้าสังเกตว่า ถ้าวิธีการ create dict ด้าน ด้านซ้ายใช่มั้ย ลองสังเกตดูคือเราสามารถ

00:12:37.280 --> 00:12:40.192
ไป ทำ CDC แบบดึงข้อมูลมาจาก Postgres ได้เหมือนกัน

00:12:40.672 --> 00:12:42.848
ถ้าสมมุติเรามี OLTP database อยู่แล้ว เราก็แค่

00:12:43.328 --> 00:12:46.624
ทำให้ dictionary เนี่ยเป็นเหมือน cache layer นึงที่ไปดึงข้อมูลมาจากตรงนั้น

00:12:47.040 --> 00:12:49.664
ทำให้เราไม่ต้องดึงข้อมูลทั้งก้อน แล้วก็ join table

00:12:50.176 --> 00:12:53.024
โดยไม่จำเป็นอะไรเงี้ยเนาะ เออ ก็คือ มีโอกาสใช้

00:12:53.504 --> 00:12:57.184
สามารถเลือกใช้แล้วกันว่า join น้อยลงได้อะไรเงี้ยครับ แล้วก็

00:12:57.248 --> 00:13:02.272
เป็น เป็นหนึ่ง key หลักที่ทำให้การ denormalization เนี่ยเริ่ม adopt มากขึ้นใน OLAP ครับ

00:13:05.568 --> 00:13:08.768
ไอ้เนี้ย มันดูแบบ มี feature เยอะมาก ถูกมั้ย

00:13:09.376 --> 00:13:10.624
คำถามก็คือแล้ว

00:13:12.352 --> 00:13:16.448
มันเป็นไปได้จริงๆ เหรอ ในการที่แบบมี feature เยอะขนาดนี้ แล้ว performance มันจะดีจริง หรือเปล่า

00:13:16.640 --> 00:13:18.432
อย่างเงี้ยครับ สุดท้ายแล้วมันจะเป็นแบบ

00:13:19.328 --> 00:13:21.824
เป็น OLAP ก้อนใหญ่ๆ ที่เราต้อง vertical scale

00:13:22.080 --> 00:13:26.528
ถูกมั้ย อะไรเงี้ย จริงๆ มันมี feature เยอะกว่านี้อีก แต่ผมอาจจะแบบ ไม่ได้หยิบมาเล่าเพราะว่า

00:13:27.200 --> 00:13:32.416
เอ่อ อาจจะค่อนข้าง niche เกินไป แล้วก็อาจจะ ใช้ ใช้ ใช้เวลาในการขยายความเรื่อง use case อ่ะ

00:13:32.864 --> 00:13:35.872
อ่าน อะไรเงี้ย แต่ว่าอาจจะเปิดเป็น Q&A ให้ ให้ถามกันได้เนาะครับ

00:13:42.528 --> 00:13:43.520
อืม ครับ ก็

00:13:44.672 --> 00:13:45.184
เอ่อ

00:13:45.696 --> 00:13:49.728
คำถามคือมันต้อง vertical scale มั้ย อ่า ซึ่งจากที่ ตัวเราลองกันอยู่เนี่ยคือ

00:13:50.336 --> 00:13:52.128
จริงไม่จำเป็นเลย ครับ เพราะว่า

00:13:52.512 --> 00:13:55.296
อย่างของ ClickHouse เนี่ย เขาออกแบบมาค่อนข้างดีเนาะ คือ

00:13:55.456 --> 00:13:58.656
นึงเลยคือเขาแยก layer ระหว่าง object storage กับ

00:13:58.848 --> 00:14:02.048
ตัว database engine เนาะ แยกระหว่าง DBMS กับ database จริงๆ

00:14:02.368 --> 00:14:07.328
ออกจากกันชัดเจน แล้วก็สามารถ horizontal scale ได้ แล้วก็เลือกได้ด้วย custom ได้ด้วยว่า

00:14:08.832 --> 00:14:10.336
แต่ละ node จะ vertical scale มั้ย

00:14:10.560 --> 00:14:13.024
ครับ ซึ่งตรงเนี้ยมัน มัน flexible ขึ้นเยอะเพราะว่า

00:14:13.600 --> 00:14:16.256
เอ่อ จริงๆ มันก็เป็น ผมมองว่าเป็น

00:14:16.736 --> 00:14:21.920
OLAP สมัยใหม่อ่ะ มันเริ่ม adopt สิ่งนี้มาแล้ว คือมันสามารถทำ partition ได้ค่อนข้างดี คือเราสามารถ partition

00:14:22.528 --> 00:14:24.288
ด้วยหลายวิธีได้อะไรเงี้ยครับ

00:14:24.512 --> 00:14:29.312
แล้ว เนื่องจากพอมันมี materialize view มีอะไรนี้อีกอ่ะ เราก็สามารถระบุไปได้เลยว่า node node นี้

00:14:30.144 --> 00:14:31.104
คือข้อมูลแค่จาก

00:14:31.904 --> 00:14:34.880
ข้อมูลพาร์ทิชั่นนี้เท่านั้น กรุ๊ปไหนเท่านั้นอะไรอย่างเงี้ยครับ

00:14:35.008 --> 00:14:37.536
มันก็สามารถช่วยแบ่งเบาภาระได้เนาะ

00:14:39.680 --> 00:14:44.064
ซึ่ง จริงๆ อ่ะมีอีกหลายๆ database ที่ผมมองว่าเจ๋งครับที่

00:14:44.640 --> 00:14:48.032
เราเคยใช้งานกันแล้วเราก็ access เรื่อยๆ อะไรเงี้ยครับ อย่างเช่นของที่

00:14:49.376 --> 00:14:51.872
ผมใช้เป็นประจำมันก็จะเป็น influx

00:14:52.480 --> 00:14:56.640
กับ Postgres เนาะ แล้วก็ใช้ parquet เยอะมาก คือ parquet นี่ผมว่าแม่งเป็น

00:14:57.088 --> 00:14:58.912
ของแบบโคตรเจ๋งอ่ะ เพราะว่า

00:14:59.648 --> 00:15:00.448
ClickHouse ก่อน

00:15:03.296 --> 00:15:06.624
ไม่ได้ ClickHouse ซัพพอร์ต parquet โดยตรงนะ แต่ว่า parquet เนี่ยซัพพอร์ต

00:15:07.008 --> 00:15:12.128
หลายอย่างมาก เยอะมาก แล้วก็เป็นไฟล์ฟอร์แมตที่ดีมากๆ อะไรเงี้ยครับ เอ่อ

00:15:13.440 --> 00:15:17.696
มันก็จะมี tools หลายอย่างที่เรา index ข้อมูลมาแล้วเราจะเก็บ พักไว้ในไฟล์ parquet ก่อน

00:15:18.016 --> 00:15:20.352
แล้วค่อยโหลดใส่ database engine ที่เหมาะสม

00:15:20.736 --> 00:15:24.128
ทำให้เราสามารถมี data source 1 อันที่

00:15:24.576 --> 00:15:27.584
ไปใส่ engine ที่แตกต่างกัน ใส่ DBS ที่แตกต่างกันเพื่อ

00:15:28.032 --> 00:15:30.656
ตอบโจทย์รูปแบบที่แตกต่างกันได้ อันนี้ก็เป็น

00:15:31.872 --> 00:15:33.568
เป็นพอยต์นึงที่ Cleverse adopt ใช้เหมือนกัน

00:15:34.848 --> 00:15:36.480
InfluxDB เราใช้เยอะมาก ซึ่ง

00:15:37.024 --> 00:15:40.288
มันก็เป็น time series database เนาะ ฉะนั้นมันก็จะมีข้อจำกัดเยอะอะไรอย่างเงี้ยครับ

00:15:40.704 --> 00:15:43.648
ช่วงหนึ่ง QuestDB มาแรงก็ แต่มันก็ยังเป็น time series database อยู่

00:15:43.840 --> 00:15:45.344
มันก็ยังมีข้อจำกัดในเรื่องของ

00:15:45.376 --> 00:15:47.040
indexing ต่างๆ อะไรอย่างเงี้ยครับ

00:15:47.744 --> 00:15:50.016
ซึ่ง ClickHouse ก็เลยตอบโจทย์มากครับผม

00:15:54.496 --> 00:15:56.512
โอเค ก็เนื้อหาได้จะประมาณนี้ ผม

00:15:56.736 --> 00:15:59.136
พูดเร็วไป เหลือเวลาเยอะก็อาจจะ

00:15:59.744 --> 00:16:01.280
ให้เป็นถามตอบก็ได้ครับ

00:16:01.792 --> 00:16:05.408
หรือ หรือว่ามีใครอยากฟังเรื่องไหนเพิ่มขึ้นมั้ย ที่ไม่ได้ใส่มาในสไลด์

00:16:06.432 --> 00:16:06.976
ถ้า

00:16:07.552 --> 00:16:09.760
ว่าง่ายสุดก็จะแนะนำเป็น cloud ของเขาเลยครับ

00:16:10.176 --> 00:16:12.512
เพราะว่ามันถูกมากๆ อยู่แล้ว แล้วก็

00:16:13.344 --> 00:16:15.328
cloud ของเค้าจะมีฟีเจอร์พิเศษเยอะ

00:16:15.648 --> 00:16:20.512
เป็นพิเศษอะไรอย่างเงี้ยครับ ก็ ก็ไม่แนะนำให้ deploy หรือว่า maintain เอง ยกเว้นแต่ว่ามีเหตุผล

00:16:20.864 --> 00:16:22.272
จำกัดมากๆ อย่างเช่น

00:16:22.560 --> 00:16:27.776
เอ่อ data dataset ของเรามันอยู่ในที่ๆ แบบ เอ่อ มันไม่ควรจะ expose อะไรอย่างเงี้ยครับ แต่ว่า

00:16:27.808 --> 00:16:29.408
แพ็กเกจของเขาก็ดี ซึ่ง

00:16:29.696 --> 00:16:32.288
อ๋อ มันมี มันมี มันมีอันนึงที่น่าสนใจครับ ก็คือ

00:16:33.472 --> 00:16:38.304
ใช่ว่า ClickHouse น่าจะพึ่ง integrate กับ PADB ด้วย PADB เป็น

00:16:38.432 --> 00:16:39.936
เป็น Data Capture ตัวนึงที่เจ๋งมากๆ

00:16:40.224 --> 00:16:45.440
อะไรเงี้ยครับ ก็ ก็ ถ้า ถ้าใช้ cloud เค้าก็น่าจะได้ PADB มาใช้ได้เลยโดยที่เราไม่ต้องแบบปวดหัวครับ

00:16:46.592 --> 00:16:48.992
จริงๆ มีคำถามในส่วนของ ClickHouse อ่ะครับ

00:16:49.152 --> 00:16:50.784
อยากรู้ว่ามันรองรับตัว

00:16:51.232 --> 00:16:55.680
ตัว multi multi-version control มั้ยครับ เวลาเรา เรากำลังเขียน data อยู่อ่ะ

00:16:55.968 --> 00:16:58.752
แล้วมี user ต้องการ query อย่างเงี้ยครับ

00:16:59.232 --> 00:17:01.728
ว่ามันจะแบบติดอะไรมั้ย หรือว่ามัน query ได้ปกติ

00:17:03.200 --> 00:17:05.856
spec มันคือ MVCC ถ้าผมจำไม่ผิดนะ

00:17:06.880 --> 00:17:11.584
multi-version control สักอย่างในการ query ข้อมูลระหว่างที่ database กำลัง

00:17:11.743 --> 00:17:14.175
กำลังอัพเดตอยู่อ่ะครับ อืม ครับ ก็ อืม

00:17:14.432 --> 00:17:18.240
อย่างแรกเลยมัน มันเป็น OLAP เนาะ ฉะนั้นมันไม่ได้มี isolation layer

00:17:18.752 --> 00:17:22.688
แข็งแรงเท่ากับ OLTP พวก Postgres พวกอะไรอย่างงี้ถูกมั้ยครับ แต่ว่า

00:17:23.488 --> 00:17:24.288
อ่า

00:17:25.056 --> 00:17:26.656
คือ คือ ถ้า ถ้ากำลังเขียนเนี่ย

00:17:26.848 --> 00:17:31.008
ยังไงก็อ่าน อ่าน อ่านไม่เจอครับ ต้องรอมันเขียนเสร็จ อ่า แต่หมายถึงว่า อ่ะ ตัวเก่าอ่ะ

00:17:31.104 --> 00:17:32.768
ได้ตัวเก่าออกไปถูกมั้ย สมมุติว่ามัน

00:17:32.896 --> 00:17:34.592
ดึงข้อมูลที่เขียนไปแล้ว

00:17:34.976 --> 00:17:38.368
จังหวะที่มันจะเขียนของใหม่ลงไปอ่ะ ตัวเก่าก็ยังเห็นดึงออกไปได้ถูกป่ะ

00:17:38.720 --> 00:17:44.704
ได้ครับ ได้ครับ เพราะว่าคือ เบื้อง เบื้องหลัง ก็คือมัน มันเป็นเวอร์ชั่น มันมีเวอร์ชั่นคอนโทรลมันอยู่แล้ว แล้วก็

00:17:44.928 --> 00:17:50.368
ClickHouse มัน implement optimistic concurrency control ให้ด้วย ครับ ฉะนั้น read กับ write พร้อมกันได้ ครับ

00:17:50.720 --> 00:17:51.040
ประมาณนี้

00:17:51.776 --> 00:17:52.704
น่าสนใจดี

00:17:55.136 --> 00:18:00.000
จริงๆ คือไม่ได้อยากขาย database คืออยากให้ ให้ดูที่ว่า เออ แบบฟีเจอร์อะไรที่มันจำเป็น

00:18:00.192 --> 00:18:03.296
ในการเลือกใช้ database ถูกมั้ย แต่ว่า ClickHouse เนี่ยคือ

00:18:03.680 --> 00:18:08.288
adopt ใช้เยอะมากจริงๆ แล้วก็แบบมาแรงแซงทางโค้งเยอะมาก ทางจริงๆ อ่ะผมว่าแค่

00:18:08.704 --> 00:18:14.048
เข้าไปลองอ่าน docs ของ ClickHouse อ่ะครับ อาจจะได้เห็นแพทเทิร์นหลายอย่างว่า เอ้ย ทำไม database นี้มันถึงปัง

00:18:14.368 --> 00:18:19.328
เพราะว่ามันมีฟีเจอร์ที่แบบค่อนข้างตอบโจทย์หลายอย่างอะไรอย่างเงี้ย แล้วบล็อกเขาก็เขียนค่อนข้างดีครับผม

00:18:20.224 --> 00:18:21.856
มีใครมีคำถามเพิ่มเติม

00:18:22.496 --> 00:18:26.560
ขอสอบถาม Tech Stack เมื่อกี้อะครับ มันจะมีตัว

00:18:27.136 --> 00:18:29.632
เอ่อ Duck DuckDB อะครับ อยากให้แบบ

00:18:29.728 --> 00:18:32.064
ยกตัวอย่าง Use Case นิดนึง อ๋อ

00:18:32.448 --> 00:18:34.240
ครับ ได้ครับ คือ คือ

00:18:34.432 --> 00:18:40.672
นี่ผมแบบใส่ตัวเล็กตัวใหญ่ก็คือตามความหมายนั้นเลย คือ อันที่ใช้เยอะใช้น้อยแล้วกัน แต่ไม่ได้บอกว่าอันไหนดีกว่าอันไหนอะไรเงี้ยครับ

00:18:40.896 --> 00:18:43.584
เอ่อ อย่างเช่น DuckDB ใช่มั้ย ก็คือ

00:18:44.640 --> 00:18:47.296
คือ หนึ่งเลย พอ พอเรามีไฟล์ Parquet ใช่มั้ยครับ

00:18:47.904 --> 00:18:49.536
แล้วสมมุติว่า เรามี

00:18:50.208 --> 00:18:54.208
คือ ข้อมูลเนี้ย มันไม่ได้เป็นประโยชน์ต่อ User อย่างเดียว แต่มันเป็นประโยชน์ต่อ

00:18:54.784 --> 00:18:56.480
Analysis ด้วย เรามี Blockchain Analysis

00:18:56.544 --> 00:19:00.960
มีคนที่อยากวิเคราะห์ข้อมูลของ Chain อยู่ด้วยอะไรเงี้ยครับ ฉะนั้นเราสามารถเอาไฟล์ Parquet เนี่ย

00:19:01.696 --> 00:19:03.168
ประโยชน์ให้ DuckDB ได้เลย

00:19:03.744 --> 00:19:07.584
แล้วก็ใช้แทนตัวนี้ได้เลย ซึ่งเบื้องหลังอะ DuckDB มันก็ใช้

00:19:08.736 --> 00:19:12.160
คือน่าจะใช้ File Format แบบ Apache Arrow อะไรอยู่แล้ว ฉะนั้นมันก็ค่อนข้าง

00:19:12.448 --> 00:19:15.040
compatible กับพวก Pandas พวกอะไรอย่างงี้ที่มันแบบสามารถ

00:19:15.328 --> 00:19:19.072
ใช้คู่กับงาน Data Science ได้อะไรเงี้ยครับ ก็ตอนนี้ Use Case หลักก็คือใช้แค่กับ

00:19:19.776 --> 00:19:20.320
ได้เหมือน

00:19:21.024 --> 00:19:24.544
Embedding Storage อันนึงที่แบบ แทนที่เราจะต้องอ่านไฟล์ Parquet ทั้งหมด

00:19:25.024 --> 00:19:30.944
คือ ที่ ที่หลักของการใช้ DuckDB ผมมองว่ามันคือสิ่งที่เรียกว่า Push Down Predicate อะ คือสมมุติว่าเรา Filter แล้วเรา Aggregate ข้อมูลอะ

00:19:30.976 --> 00:19:33.248
เราไม่จำเป็นต้องโหลดข้อมูลทั้งหมดเข้า

00:19:34.016 --> 00:19:36.512
เข้า Memory ก่อน แล้วค่อย Aggregate

00:19:36.832 --> 00:19:42.432
เราสามารถทำที่ Layer ของ File Format ได้เลย แล้วก็ DuckDB มันจัดการให้ ฉะนั้นมันก็เลยทำให้

00:19:42.784 --> 00:19:45.024
การใช้งาน Parquet File มันง่ายขึ้นเยอะ

00:19:45.216 --> 00:19:49.408
แบบไฟล์ Parquet สมมุติ 10 GB 30 GB เนี่ย เราก็ใช้ DuckDB ครอบจบเลยครับ

00:19:49.696 --> 00:19:51.360
ไม่ ไม่ ไม่ได้หนักเครื่องเลยอะไรเงี้ยครับ

00:19:52.576 --> 00:19:55.872
ใส่การใช้งาน InfluxDB กับ ClickHouse อะค่ะว่า

00:19:56.128 --> 00:19:59.552
ตอน ตอนนี้ที่แบบใช้อยู่เงี้ย แบบว่ามีการทำ

00:19:59.776 --> 00:20:03.776
เอ่อ ใช้ Data Capture ยังไง หรือว่าใช้ตัว Feature ของ ClickHouse ที่บอกเลยอะคะ

00:20:04.800 --> 00:20:06.656
เอ่อ InfluxDB

00:20:06.848 --> 00:20:10.176
ตอน InfluxDB ครับ เรา เราต้องเขียนเองครับ เพราะว่า

00:20:10.528 --> 00:20:12.896
InfluxDB ตอน ตอนที่เลือกใช้ InfluxDB อะ คือ

00:20:13.152 --> 00:20:16.192
สาเหตุหลักๆ เพราะว่า เขาขายเรื่องของ Injection

00:20:16.320 --> 00:20:19.296
มัน มัน High Throughput มาก เพราะมันอาจจะออกแบบมาสำหรับ

00:20:19.520 --> 00:20:23.552
ข้อมูล Time Series เนาะ พวก IoT พวกอะไรเงี้ยครับ เราก็เลยลองเอามา Adopt ใช้กับ

00:20:24.064 --> 00:20:28.512
Blockchain ดู อะไรเงี้ยครับ แล้ว เราไม่ พอมันเป็น Blockchain อะ เราไม่ได้มี

00:20:28.992 --> 00:20:32.384
Out of the Order Data เยอะ ฉะนั้น InfluxDB ก็เลยอาจจะตอบโจทย์

00:20:33.152 --> 00:20:33.568
แต่ว่า

00:20:34.112 --> 00:20:37.856
เอ่อ การที่จะให้ได้ Throughput สูงขนาดนั้นน่ะครับ มันต้องใช้

00:20:38.688 --> 00:20:39.936
Protocol อะไรของมัน

00:20:40.608 --> 00:20:45.952
ซึ่งมันไม่ได้มี ผม ผมจำ ถ้าจำไม่ผิดคือมันไม่ได้มีโปรแกรมสำเร็จรูปให้ เราก็เลยต้องเขียนโค้ดเองด้วย GoLang

00:20:46.016 --> 00:20:47.328
ก็เลยลำบากหน่อย เพราะว่า

00:20:48.224 --> 00:20:53.024
ถ้าใช้ Protocol อะ ในการ Write ข้อมูลอะ มัน มันเร็วกว่าอะไรเงี้ยครับ ส่วน QuestDB เนี่ย

00:20:53.600 --> 00:20:58.240
ก็ขายเรื่อง Injection เหมือนกัน คือ เร็วมั้ย เร็วจริง แต่ว่าบั๊กเยอะมาก

00:20:58.560 --> 00:21:02.496
บั๊กเยอะมากครับ คือ ไม่รู้ผมเจอคนเดียวรึเปล่า แต่ว่าตอนนั้นใช้เวอร์ชัน

00:21:02.784 --> 00:21:04.768
ตอนนั้นนี่หนึ่งเวอร์ชันอะ คือบั๊กเยอะมาก แล้วก็

00:21:05.120 --> 00:21:08.960
ถึงขั้นต้องแบบ เข้าไป Contribute เลยอะ แบบไปช่วยแก้บั๊ก

00:21:09.056 --> 00:21:12.064
ได้เสื้อมาเต็มอะไรเงี้ย คือ ซึ่ง ซึ่งสุดท้ายก็

00:21:12.480 --> 00:21:16.160
เลิก เลิกใช้ดีกว่า เพราะว่า เออ บั๊ก บั๊กไม่ ไม่หมดสักทีอะไรเงี้ยครับ

00:21:17.024 --> 00:21:20.256
Quest QuestDB ก็ คือพอมันเป็น SQL อะ มันก็ใช้

00:21:20.608 --> 00:21:22.976
Change Data Capture ปกติได้หมด เหมือน เหมือน ได้เหมือนกัน ครับ

00:21:23.648 --> 00:21:28.928
โอเค งั้นเดี๋ยวขอขอบคุณพี่ฟามากนะคะ ขอเสียงปรบมือให้พี่ฟาหน่อยค่า

00:21:29.632 --> 00:21:32.064
สุด สุดท้ายครับ ขอ ขอแถมนิดนึงครับ

00:21:33.088 --> 00:21:35.936
อยาก อยากช่วยขายหนังสือครับ คือ

00:21:36.160 --> 00:21:42.784
หนังสือนี้มันเป็นหนังสือที่ผมมองว่าก็ทุกคนควรอ่านเนาะ อย่างเช่น Software Engineer เดี๋ยวเนี้ย ผมก็จะแนะนำให้น้องๆ ทุกคนในทีมอะ อ่าน

00:21:42.912 --> 00:21:45.728
อันนี้เหมือนกัน อ่านจบเล่มได้ยิ่งดี แต่ถ้าอ่านไม่จบก็

00:21:46.336 --> 00:21:50.336
ช่วงแรกสำคัญเนาะ ครึ่งหลังมันจะเป็นเรื่อง เรื่อง real time แหละ ก็อาจจะ

00:21:50.624 --> 00:21:53.280
เน้น fundamental ก่อน อะไรเงี้ยครับ แนะนำมากๆ

00:21:54.656 --> 00:22:00.096
อ่านทุกวัน วันละหน้าก็ได้ ขอบคุณครับ ขอบคุณพี่ฟ้ามากๆ นะคะ โอเค

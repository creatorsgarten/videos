WEBVTT - Auto-generated by https://github.com/dtinth/autosub

00:00:01.719 --> 00:00:04.517
สวัสดีทุกๆ ท่านที่เข้ามาฟังนะครับ

00:00:04.600 --> 00:00:07.560
ยินดีต้อนรับเข้าสู่ session Mastering Phi-3.5

00:00:08.400 --> 00:00:11.077
Experiment Innovative Approach on Small Language

00:00:11.160 --> 00:00:14.237
Model with JavaScript and Ollama นะครับ

00:00:14.320 --> 00:00:17.597
ซึ่งเราจะมาพูดคุยในเรื่องของการใช้ small language

00:00:17.680 --> 00:00:20.599
model หรือ model AI ที่มีขนาดเล็กนะครับ

00:00:20.760 --> 00:00:24.036
กับการใช้แอปพลิเคชัน Ollama และ develop ด้วย

00:00:24.119 --> 00:00:25.560
JavaScript นะครับ

00:00:25.680 --> 00:00:33.879
แนะนำตัวก่อนนะครับ ผมโบ๊ท ชรันธร นะครับ

00:00:34.360 --> 00:00:36.357
ตอนนี้ปัจจุบันเป็น Microsoft Learn Student

00:00:36.440 --> 00:00:37.316
Ambassador นะครับ

00:00:37.399 --> 00:00:41.357
แล้วก็อยู่ภายใต้ Microsoft Office Specialist ครับ

00:00:41.440 --> 00:00:46.280
แล้วก็ศึกษาเกี่ยวกับเรื่องของ open source small AI

00:00:46.600 --> 00:00:49.476
และในเรื่องของ natural language program นะครับ

00:00:49.559 --> 00:00:55.600
สำหรับใครที่อยากจะติดตามหรือว่าอยากจะตามข่าวพวก AI

00:00:56.039 --> 00:00:58.440
หรือว่าพวกนวัตกรรมใหม่ๆ ที่ผมลง

00:00:58.559 --> 00:01:02.037
ก็สามารถที่ติดตามใน Facebook หรือว่าใน Instagram

00:01:02.120 --> 00:01:04.640
แล้วก็ทาง LinkedIn ได้เลยนะครับ

00:01:08.000 --> 00:01:12.597
อยากจะ acknowledge กับทุกๆ คนแล้วก็ทุกๆ specialist

00:01:12.680 --> 00:01:17.799
ที่ให้ความรู้และ inspiration ในการทำเรื่องนี้นะครับ

00:01:19.439 --> 00:01:21.396
agenda เราอาจจะไปเร็วๆ นะครับ

00:01:21.479 --> 00:01:24.597
ก็จะมีในเรื่องของการ recap ในเรื่องของตัว open source

00:01:24.680 --> 00:01:26.957
และตัว language model นะครับ

00:01:27.040 --> 00:01:30.237
แล้วก็เราจะมาเล่ากันว่า Phi-3.5 เนี่ยมันคืออะไร

00:01:30.320 --> 00:01:35.036
แล้วก็เราก็จะมาบอกว่าถ้าเราอยากได้ runtime ที่มันเร็วๆ

00:01:35.119 --> 00:01:39.119
ก็มาลองใช้ Bun ที่เป็น framework ใหม่ของ JavaScript

00:01:39.479 --> 00:01:40.280
นะครับ

00:01:40.960 --> 00:01:43.356
จริงๆ session ตัวนี้นะครับ

00:01:43.439 --> 00:01:46.117
ก็มีการทำ workshop ไปเมื่อวานนี้เองนะครับ

00:01:46.200 --> 00:01:48.517
แล้วก็เดี๋ยวเราก็จะมา recap กันสั้นๆ

00:01:48.600 --> 00:01:50.600
ว่าเมื่อวานเราทำอะไรกันไปบ้างนะครับ

00:01:51.159 --> 00:01:54.557
ก่อนอื่นเลยเราจะบอกว่าในโลกของ language model

00:01:54.640 --> 00:01:57.599
เนี่ยมันก็จะมีในเรื่องของการใช้ model

00:01:58.399 --> 00:02:01.157
ที่ค่อนข้างที่เป็น premium และเป็น open source นะครับ

00:02:01.240 --> 00:02:03.157
บางอันก็จะฟรีบ้าง

00:02:03.240 --> 00:02:06.837
บางอันก็ไม่ได้โชว์ source code ให้เรา develop

00:02:06.920 --> 00:02:08.596
บางอันก็จะเป็น open source ที่มันฟรี

00:02:08.679 --> 00:02:11.876
แล้วก็ develop สามารถที่จะปรึกษาทาง forum

00:02:11.959 --> 00:02:14.197
หรือว่าทาง community ได้เช่นเดียวกันครับ

00:02:14.280 --> 00:02:17.960
แต่คราวนี้ครับ ทางที่เราใช้ในตัวของ proprietary

00:02:19.480 --> 00:02:20.436
หรือว่าตัว premium เนี่ย

00:02:20.519 --> 00:02:22.077
มาอาจจะเจอปัญหาบางอย่าง

00:02:22.160 --> 00:02:24.837
อย่างเช่นในเรื่องของ currently unavailable

00:02:24.920 --> 00:02:27.959
หรือว่า server มีปัญหา หรือว่าจะเป็นพวก bad gateway

00:02:28.400 --> 00:02:32.210
สมมุติว่าเราเห็นตัว major outage ที่เกิดขึ้นใน ChatGPT

00:02:32.293 --> 00:02:35.800
เนี่ยมันก็ไม่สามารถที่จะใช้งานของ ChatGPT ได้นะครับ

00:02:36.200 --> 00:02:40.156
ผมเลยเสนอว่าถ้าเราลองใช้ในตัวของ Hugging Face

00:02:40.239 --> 00:02:43.197
แล้วก็โหลด model เข้าสู่ server ของตัวเรา

00:02:43.280 --> 00:02:45.876
เนี่ยมันก็สามารถที่จะป้องกันในเรื่องของ data leak

00:02:45.959 --> 00:02:48.479
หรือว่าจะเป็นในเรื่องของระบบล่ม

00:02:48.640 --> 00:02:51.037
หรือว่าจะเป็นในเรื่องของ internet connection นะครับ

00:02:51.120 --> 00:02:54.757
แต่ว่าใดๆ ก็ตามแต่เนี่ยในเรื่องของ specification

00:02:54.840 --> 00:02:56.559
หรือว่าในเรื่องของ hardware เนี่ย

00:02:59.040 --> 00:03:01.960
ของเราอาจจะค่อนข้างที่ไม่มีเพียงพอครับ

00:03:02.560 --> 00:03:05.436
มันก็เลยนำมาสู่ในเรื่องของ small language model

00:03:05.519 --> 00:03:06.117
นะครับ

00:03:06.200 --> 00:03:07.837
ในตัว small language model เนี่ย

00:03:07.920 --> 00:03:10.037
มันก็สามารถที่จะใช้ใน offline ก็ได้

00:03:10.120 --> 00:03:15.917
หรือว่าจะใช้ในการใช้ข้อมูลภายในองค์กรได้เช่นกันครับ

00:03:16.000 --> 00:03:19.156
แล้วก็แน่นอนครับ มันป้องกันในเรื่องของ data privacy

00:03:19.239 --> 00:03:23.277
และในเรื่องของ lower resource requirement

00:03:23.360 --> 00:03:27.600
นั่นก็คือในเรื่องของการใช้ทรัพยากรที่น้อยนะครับ

00:03:27.720 --> 00:03:30.757
แล้วก็แน่นอนครับ ในการใช้ small language model เนี่ย

00:03:30.840 --> 00:03:35.677
มันสามารถที่จะลดเรื่องของการใช้พลังงานใน data center

00:03:35.760 --> 00:03:37.160
ได้เช่นเดียวกันครับ

00:03:37.840 --> 00:03:42.317
ใดๆ ก็ตามแต่ครับ ถ้าเรายังนึกภาพตัว SLM ไม่ออกเนี่ย

00:03:42.400 --> 00:03:44.839
ตัว LLM เนี่ยก็จะมีเป็นพวก GPT-4

00:03:45.200 --> 00:03:48.037
หรือว่าจะเป็นในเรื่องของ Gemini พวก Claude นะครับ

00:03:48.120 --> 00:03:50.637
แต่ว่าในตัวของ small language model เนี่ย

00:03:50.720 --> 00:03:52.040
มันก็จะมีในเรื่องของ Gemma

00:03:52.200 --> 00:03:56.399
แล้วก็ตัว Phi ที่ผมจะมานำเสนอในวันนี้นะครับ

00:03:57.079 --> 00:04:01.837
แต่คำถามคือตัวของ small language model

00:04:01.920 --> 00:04:04.117
มันต้องควรมีขนาดประมาณไหนนะครับ

00:04:04.200 --> 00:04:09.597
มันควรที่จะมีประมาณ 1-7 พันล้าน parameter นะครับ

00:04:09.680 --> 00:04:12.037
ที่จะสามารถที่จะโหลดเข้ามาในเครื่องได้

00:04:12.120 --> 00:04:14.520
แล้วก็ไม่กิน spec นะครับ

00:04:15.200 --> 00:04:18.077
ในตัวของการใช้ small language model เนี่ย

00:04:18.160 --> 00:04:22.116
มันก็จะถูกยุบมาอยู่ใน domain ที่ค่อนข้างที่ specific

00:04:22.199 --> 00:04:25.237
หรือว่าเจาะจงว่าให้ทำ task ไหนนะครับ

00:04:25.320 --> 00:04:28.676
แล้วก็แน่นอนครับ ในเรื่องของ cost to operate

00:04:28.759 --> 00:04:29.917
หรือว่าค่าใช้จ่ายเนี่ย

00:04:30.000 --> 00:04:33.120
ค่อนข้างที่จะต่ำจนถึงฟรีนะครับ

00:04:33.800 --> 00:04:36.956
ณ ตอนนี้ครับ ก็จะมีในเรื่องของการแข่งขัน

00:04:37.039 --> 00:04:40.997
ในเรื่องของประสิทธิภาพในเรื่องของ model ต่างๆ นะครับ

00:04:41.080 --> 00:04:43.400
ไม่ว่าจะเป็น Microsoft เองก็ test อยู่ที่

00:04:43.919 --> 00:04:46.037
3.8 พันล้าน parameter อยู่ที่ 2.2 GB

00:04:46.120 --> 00:04:47.836
หรือว่าจะเป็นในฝั่งของ Gemma เอง

00:04:47.919 --> 00:04:49.959
Gemma ก็จะอยู่ที่ 1.6 GB

00:04:50.160 --> 00:04:51.956
ซึ่งในการ train ข้อมูลเองเนี่ย

00:04:52.039 --> 00:04:54.200
มันก็ค่อนข้างที่จะแตกต่างกันไปนะครับ

00:04:54.360 --> 00:04:57.836
หรือแม้กระทั่งในประเทศไทยเราเองเนี่ย

00:04:57.919 --> 00:05:02.717
มันก็จะมีในเรื่องของตัวของไต้ฝุ่นแล้วก็ตัว KBDG นะครับ

00:05:02.800 --> 00:05:05.157
ที่ support ภาษาไทยนะครับ

00:05:05.240 --> 00:05:09.120
แต่ว่าเราจะเห็นได้เลยครับว่าถ้าเราใช้ตัวไต้ฝุ่น

00:05:10.080 --> 00:05:11.757
หรือว่าจะเป็นตัวของทะเลเองเนี่ย

00:05:11.840 --> 00:05:14.997
มันอาจจะ size อาจจะมีประมาณขนาดใหญ่

00:05:15.080 --> 00:05:18.597
ซึ่งมันอาจจะไม่สามารถที่จะใช้แบบในคอมพิวเตอร์ได้

00:05:18.680 --> 00:05:21.560
แบบ smooth นะครับ

00:05:23.280 --> 00:05:26.680
use case ของเราที่เราได้ใช้ก็จะมีในเรื่องของ

00:05:27.639 --> 00:05:31.836
การใช้ลองใช้ให้กับทาง influencer

00:05:31.919 --> 00:05:35.477
หรือว่าไม่ว่าจะเป็นทางดาราหรือว่าคนที่ specific

00:05:35.560 --> 00:05:38.400
ในเรื่องของ business และ marketing นั่นเองครับ

00:05:39.080 --> 00:05:40.080
แล้วก็แน่นอนครับ

00:05:41.199 --> 00:05:44.319
เราก็มี use case ต่างๆ นานาที่เราสามารถที่จะ

00:05:44.759 --> 00:05:47.920
เอา SLM เนี่ยมาสามารถ train ข้อมูลต่างๆ นานา

00:05:48.080 --> 00:05:49.637
ไม่ว่าจะเป็นในเรื่องของประวัติศาสตร์เอง

00:05:49.720 --> 00:05:52.039
หรือว่าจะเป็นในเรื่องของ retail หรือ finance

00:05:52.360 --> 00:05:55.160
ก็สามารถที่จะเอามา fine-tune ให้มันฉลาดมากขึ้น

00:05:55.319 --> 00:05:58.080
หรือว่าเราจะใช้ RAG ที่สามารถที่จะ

00:05:59.039 --> 00:06:03.600
เอามาเพื่อเพิ่มประสิทธิภาพและความฉลาดให้กับมันนั่นเองครับ

00:06:04.280 --> 00:06:10.000
คำถามคือแล้วเราจะยุบตัว LLM มาเป็น SLM ได้ยังไงครับ

00:06:10.680 --> 00:06:14.676
มันจะมีเทคนิคนึงครับที่ชื่อว่า quantization ครับ

00:06:14.759 --> 00:06:18.200
ก็คือจาก original model มาถูกยุบให้เหลือ

00:06:18.400 --> 00:06:19.597
quantized model ครับ

00:06:19.680 --> 00:06:22.236
แต่คำถามคือความต่างของ compressed model

00:06:22.319 --> 00:06:24.396
และตัวของ quantized model เนี่ยมันต่างกันยังไง

00:06:24.479 --> 00:06:26.516
ถ้าตัวของ compressed model เนี่ย

00:06:26.599 --> 00:06:27.960
มันก็อาจจะสูญเสียตัว node

00:06:28.199 --> 00:06:30.519
หรือว่าประสิทธิภาพบ้างนิดนึงครับ

00:06:30.759 --> 00:06:32.556
แต่จริงๆ มันก็ไม่ได้บ้างครับ

00:06:32.639 --> 00:06:35.757
เพราะว่าบางทีเนี่ยใจความของมัน

00:06:35.840 --> 00:06:39.116
หรือว่า summarize ของมันเนี่ยมันอาจจะหลุดไปเลยก็ได้ครับ

00:06:39.199 --> 00:06:42.077
แต่ว่า quantized model เนี่ยมันแค่ compress เฉยๆ

00:06:42.160 --> 00:06:44.396
มันอาจจะสูญเสียประสิทธิภาพนิดนึงครับ

00:06:44.479 --> 00:06:47.236
แต่ว่าเราสามารถที่จะเพิ่มศักยภาพ

00:06:47.319 --> 00:06:50.277
หรือการตอบของมันโดยการใช้ fine-tune

00:06:50.360 --> 00:06:52.880
หรือว่ายังใช้ RAG ได้เช่นเดียวกันนั่นเองครับ

00:06:53.560 --> 00:06:56.800
คราวนี้ครับ Microsoft เองก็สนับสนุนในการใช้

00:06:57.240 --> 00:06:59.639
small language model มากยิ่งขึ้นนั่นเองนะครับ

00:07:00.240 --> 00:07:02.956
layer ปกติเนี่ยเค้าจะดูในเรื่องของ model

00:07:03.039 --> 00:07:04.760
และตัวของ safety system

00:07:04.879 --> 00:07:07.516
แล้วก็แน่นอนครับในการ set system

00:07:07.599 --> 00:07:09.637
มันก็จะมีในเรื่องของ prompt นั่นเองนะครับ

00:07:09.720 --> 00:07:11.597
เราจะพูดในเรื่องของ model นะครับ

00:07:11.680 --> 00:07:14.917
ซึ่ง model ของเราเนี่ยมันชื่อว่า Phi นะครับ

00:07:15.000 --> 00:07:18.637
ตัว Phi เนี่ยมันเริ่มพัฒนาตั้งแต่ Phi-1 นะครับ

00:07:18.720 --> 00:07:22.717
ก็จะมี reasoning นิดๆ หน่อยๆ นะครับ

00:07:22.800 --> 00:07:25.637
แต่หลักๆ ก็จะมาในเรื่องของ coding มากขึ้น

00:07:25.720 --> 00:07:28.197
แล้วก็ตัว Phi-2 ก็จะถูก public release

00:07:28.280 --> 00:07:30.400
ในตัวของงาน Ignite ปี 2023

00:07:31.319 --> 00:07:32.999
แล้วก็ตัวล่าสุดก็คือตัว Phi-3

00:07:33.800 --> 00:07:36.676
Phi-3 ก็จะเป็นตัวที่เสถียรที่สุดนั่นเองครับ

00:07:36.759 --> 00:07:39.516
แต่ว่าล่าสุดในไม่กี่เดือนที่ผ่านมา

00:07:39.599 --> 00:07:41.159
ก็จะมีการ launch ตัว Phi-3.5

00:07:41.960 --> 00:07:45.956
ที่ถูก fine-tuning มาจาก Phi-3 นั่นเองครับ

00:07:46.039 --> 00:07:48.637
ซึ่งมันจะ support ในเรื่องของ multilingual

00:07:48.720 --> 00:07:51.759
และในเรื่องของ logical thinking นั่นเองครับผม

00:07:52.280 --> 00:07:55.676
ณ คราวนี้ครับ ตัวของ Phi family เองเนี่ยครับ

00:07:55.759 --> 00:07:57.877
มันจะเป็นตัว AI for science

00:07:57.960 --> 00:08:00.757
หรือว่ามันถูกคิดในเรื่องของวิทยาศาสตร์นั่นเองครับ

00:08:00.840 --> 00:08:06.560
มันก็จะมีในเรื่องของฟิสิกส์ เคมี และชีวภาพนะครับ

00:08:06.800 --> 00:08:12.516
แล้วก็แน่นอนครับมันถูก train ด้วย hyper computing นะครับ

00:08:12.599 --> 00:08:15.440
และในเรื่องของ scientific reasoning นะครับ

00:08:15.720 --> 00:08:20.076
คราวนี้ครับก็ถูกเอามา publish ในตัวของ Hugging Face

00:08:20.159 --> 00:08:20.556
นั่นเองครับ

00:08:20.639 --> 00:08:23.157
แล้วก็เราเองก็สามารถที่จะเอามาลองเล่น

00:08:23.240 --> 00:08:25.516
กับทาง Hugging Face นะครับ

00:08:25.599 --> 00:08:31.277
แล้วก็ตัวข้อมูลเองเนี่ยครับก็ถูก train ตั้งแต่นานแล้วครับ

00:08:31.360 --> 00:08:37.039
แล้วก็มาตัดจบเมื่อตุลาคมปี 2023 นั่นเองนะครับ

00:08:37.719 --> 00:08:40.277
มาพูดถึงในตัวของ performance นะครับ

00:08:40.360 --> 00:08:44.997
ตัว performance เองเนี่ยมันก็จะค่อนข้างที่จะโอเค

00:08:45.080 --> 00:08:45.797
ในระดับนึง

00:08:45.880 --> 00:08:47.517
แต่มันไม่ได้บอกว่ามันจะดีนะครับ

00:08:47.600 --> 00:08:49.077
แต่ว่ามันก็ถือว่าโอเค

00:08:49.160 --> 00:08:51.640
มันสามารถที่จะตอบคำถามเราได้ถูกต้อง

00:08:51.839 --> 00:08:53.477
เพราะว่าในการ train เนี่ยครับ

00:08:53.560 --> 00:08:58.397
มันค่อนข้างที่จะใช้ตัวข้อมูลจากหนังสือทั้งหมดเลยนะครับ

00:08:58.480 --> 00:09:01.800
แล้วก็ไม่มีข้อมูลที่มาจากอินเทอร์เน็ตครับ

00:09:02.480 --> 00:09:07.436
แล้วก็ตัว Phi-3.5 ก็ MOE มันก็จะเป็น mixture of expert

00:09:07.519 --> 00:09:10.357
ซึ่งมันจะช่วยในเรื่องของ multilingual

00:09:10.440 --> 00:09:13.839
ซึ่งมันสามารถที่จะตอบภาษาไทยได้ชัดเจนนั่นเองครับ

00:09:14.000 --> 00:09:16.720
ณ ตอนนี้ใน performance ของมันเองเนี่ยครับ

00:09:17.839 --> 00:09:19.876
ที่เราเอาไป run เนี่ย

00:09:19.959 --> 00:09:23.837
มันสามารถที่จะ run ทั้งตัวของ Azure AI, Hugging Face

00:09:23.920 --> 00:09:25.677
หรือว่าตัว web neural network

00:09:25.760 --> 00:09:29.160
ถ้าใครได้เข้าไปใน session ของอาจารย์สุรชาญ

00:09:29.839 --> 00:09:35.279
ของเมื่อวานนะครับ ก็มีให้ลองกันประมาณนี้นะครับ

00:09:35.959 --> 00:09:39.960
แล้วก็ตัว open model ก็ตอนนี้ก็ปล่อยทั้งตัว

00:09:40.200 --> 00:09:41.357
mini, small, medium

00:09:41.440 --> 00:09:44.439
แล้วก็ตัวเป็น multimodal ที่เป็น vision นั่นเองนะครับผม

00:09:44.920 --> 00:09:48.359
แต่คำถามคือแล้วตัว mixture of expert เนี่ย

00:09:48.480 --> 00:09:52.237
ทำไมมันถึงไม่ได้ถูกขนานนามว่าเป็น small language model

00:09:52.320 --> 00:09:56.840
เพราะว่ามันจำเป็นที่จะต้องใช้หลายๆ node

00:09:57.200 --> 00:09:58.957
หรือหลายๆ model เอามาประกอบกัน

00:09:59.040 --> 00:10:03.797
เพื่อสร้าง intelligence ให้กับการตอบของแต่ละภาษานั่นเองครับ

00:10:03.880 --> 00:10:06.920
มันเลยกลายเป็นว่าทั้งหมดของ model

00:10:07.440 --> 00:10:12.360
มันจะตกอยู่ที่ 42 billion parameter นั่นเองครับ

00:10:13.040 --> 00:10:15.756
ณ คราวนี้ใน principle ในการ train model เนี่ย

00:10:15.839 --> 00:10:17.917
มันก็จะมีประมาณนี้ของ Microsoft ครับ

00:10:18.000 --> 00:10:23.677
แต่ว่าผมเองผมอยากจะปรับให้มันสั้นเหลือแค่ 3 ตัวอักษร

00:10:23.760 --> 00:10:25.717
นั่นก็คือ PYD นะครับ

00:10:25.800 --> 00:10:28.000
ซึ่งมันคือตัวย่อของเพื่อนผมนั่นเอง

00:10:28.800 --> 00:10:33.079
ซึ่งเพื่อนผมบอกว่าในตัวของ PYD หรือตามชื่อเค้านั่นน่ะ

00:10:33.399 --> 00:10:35.597
เค้ามี P ก็คือ privacy

00:10:35.680 --> 00:10:37.117
Y ก็คือ your responsibility

00:10:37.200 --> 00:10:39.480
D ก็คือ diversity นั่นเองครับ

00:10:39.600 --> 00:10:43.000
ตัว P privacy ก็จะมีในเรื่องของการย่อขนาดข้อมูล

00:10:43.360 --> 00:10:45.196
ตัดข้อมูลที่ไม่จำเป็นออกไปนะครับ

00:10:45.279 --> 00:10:47.756
แล้วก็ทำให้เป็น anonymous นั่นเองครับ

00:10:47.839 --> 00:10:49.996
แต่ว่าถ้าเป็นตัว Y ก็คือ your responsibility

00:10:50.079 --> 00:10:52.640
เราก็ควรจัดการให้มีความโปร่งใสมากขึ้น

00:10:52.800 --> 00:10:55.556
แล้วก็ในเรื่องของการ take action ต่างๆ นะครับ

00:10:55.639 --> 00:10:58.840
แล้วก็ diversity เนี่ยก็จะมีในเรื่องของการแบ่งแยก

00:10:59.440 --> 00:11:05.800
ก็ไม่ควรจำกัดเพศอะไรประมาณนี้ครับ

00:11:06.720 --> 00:11:09.196
Responsible AI ก็จะประมาณนี้ครับ

00:11:09.279 --> 00:11:11.037
ในเรื่องของการทำ prompt engineering

00:11:11.120 --> 00:11:14.240
หรือว่าในการ set system เพื่อไม่ให้เกิดการ jailbreak

00:11:14.399 --> 00:11:15.839
อะไรประมาณนั้นนั่นเองครับ

00:11:16.200 --> 00:11:18.640
และผมอยากจะ introduce ให้กับทุกคน

00:11:19.480 --> 00:11:23.080
ก็คือตัวของ Ollama ที่ running AI on local device นะครับ

00:11:23.560 --> 00:11:28.077
ซึ่งว่ากันตามตรงนะครับว่าในตัว Ollama เองเนี่ย

00:11:28.160 --> 00:11:30.316
เราสามารถที่จะ develop หรือว่า install

00:11:30.399 --> 00:11:32.237
ภายในตัวของ terminal นั่นเองนะครับ

00:11:32.320 --> 00:11:35.599
ก็สามารถที่จะ run by using command ได้นะครับ

00:11:35.720 --> 00:11:42.957
หรือว่าเราเองก็สามารถที่จะ run ทั้งตัว local

00:11:43.040 --> 00:11:44.837
หรือว่า run ใน terminal ที่เป็น cloud

00:11:44.920 --> 00:11:46.397
ไม่ว่าจะเป็น Azure ก็ได้

00:11:46.480 --> 00:11:49.277
หรือว่าจะเป็นในเรื่องของ Google Cloud Run

00:11:49.360 --> 00:11:53.277
ก็สามารถที่จะใช้ run ใน shell ได้เช่นเดียวกันครับ

00:11:53.360 --> 00:11:55.480
ไม่ว่าคุณจะ run ในตัว Llama 3.2, Phi 3.5

00:11:56.639 --> 00:12:00.917
หรือเป็นพวก Gemma ก็สามารถที่จะ run ได้เช่นเดียวกันนะครับ

00:12:01.000 --> 00:12:03.876
แต่หลักๆ แล้วในตัวของ Ollama เองเนี่ย

00:12:03.959 --> 00:12:07.797
มันสามารถที่จะ run เฉพาะตัวของ text generation model

00:12:07.880 --> 00:12:10.640
ได้อย่างเดียวครับ

00:12:11.240 --> 00:12:12.360
โอเคครับ

00:12:13.040 --> 00:12:16.360
ก็ถ้าพูดตามหลักๆ ในเรื่องของ function

00:12:17.519 --> 00:12:19.556
หรือว่า process ของการทำงานเนี่ย

00:12:19.639 --> 00:12:21.959
เราจะเห็นได้เลยครับว่าตัว Ollama เนี่ย

00:12:22.399 --> 00:12:24.157
มันจะดึง model จาก registry

00:12:24.240 --> 00:12:27.120
หรือว่าตัว marketplace ของตัว Ollama นั่นเองครับ

00:12:27.399 --> 00:12:30.677
จากนั้นเราทำการ pull ให้ไปสู่ local machine

00:12:30.760 --> 00:12:32.360
หรือว่า server ของเรานะครับ

00:12:32.639 --> 00:12:35.760
ก็จะใช้คำสั่ง Ollama pull หรือ Ollama run

00:12:36.000 --> 00:12:37.237
เข้าไปข้างในนั่นเองครับ

00:12:37.320 --> 00:12:40.679
แล้วเราก็สามารถที่จะโยนตัวของ local host

00:12:41.079 --> 00:12:44.876
ที่สามารถที่จะเอามา develop ภายในภาษาของ JavaScript

00:12:44.959 --> 00:12:49.279
หรือว่าจะเป็น Python ได้นั่นเองนะครับ

00:12:49.959 --> 00:12:53.237
โอเคครับ เรามาสู่ในอีกเรื่องหนึ่งนะครับ

00:12:53.320 --> 00:12:56.436
ก็คือตัว runtime ที่ค่อนข้างที่จะมีประสิทธิภาพ

00:12:56.519 --> 00:13:00.116
และก็เร็วมากยิ่งขึ้น นั่นคือตัว Bun นั่นเองนะครับ

00:13:00.199 --> 00:13:02.876
ซึ่งมันจะเร็วแล้วก็ใช้ memory น้อย

00:13:02.959 --> 00:13:06.320
แล้วก็สามารถใช้แบบ cross platform ได้เช่นเดียวกันนะครับ

00:13:06.480 --> 00:13:08.517
คำถามว่า Bun มันคืออะไรครับ

00:13:08.600 --> 00:13:11.359
จริงๆ ไม่ใช่ Bun นี้นะครับ นั่นมันคือซาลาเปา

00:13:11.959 --> 00:13:16.080
ซึ่ง Bun เนี่ยมันก็เป็น runtime คล้ายๆ กับ Node.js

00:13:16.240 --> 00:13:17.837
แต่มันเร็วกว่านั่นเองนะครับ

00:13:17.920 --> 00:13:20.720
และแน่นอนครับ elegant APIs ก็คือมันก็

00:13:21.120 --> 00:13:24.077
ตัว API ค่อนข้างที่จะสั้นแล้วก็มีเรื่องของ

00:13:24.160 --> 00:13:27.320
cohesive DX นั่นเองนะครับ

00:13:28.000 --> 00:13:31.556
อันนี้ก็จะเป็น function รวมๆ ของตัวของ Bun นั่นเองครับ

00:13:31.639 --> 00:13:34.680
ก็จะมีในเรื่องของ drop-in Node.js compatibility นะครับ

00:13:34.880 --> 00:13:38.556
แล้วก็ในเรื่องของการรองรับตัว TypeScript

00:13:38.639 --> 00:13:41.717
ก็สามารถที่จะ run TypeScript ได้เช่นเดียวกันครับ

00:13:41.800 --> 00:13:44.677
แล้วก็แน่นอนครับ เป็น cross-platform shell script

00:13:44.760 --> 00:13:48.079
เราสามารถ run ผ่านตัวของ Google Shell

00:13:48.240 --> 00:13:51.480
หรือว่าจะเป็นในเรื่องของตัวของ Linux

00:13:51.720 --> 00:13:55.077
คุณก็สามารถที่จะ run ตัว Bun ได้เช่นเดียวกัน

00:13:55.160 --> 00:13:57.520
แล้วก็แน่นอนครับ มันสามารถ run ที่ Azure

00:13:57.639 --> 00:13:59.000
ได้เช่นเดียวกันนะครับ

00:13:59.959 --> 00:14:03.196
คราวนี้ครับ มาดูตัว speed performance ของมันนะครับ

00:14:03.279 --> 00:14:06.159
ว่าตัว server-side เนี่ย Bun เนี่ย

00:14:06.639 --> 00:14:10.037
ก็สามารถที่จะทำได้เร็วกว่า Deno แล้วก็ตัวของ Node.js

00:14:10.120 --> 00:14:10.676
นั่นเองครับ

00:14:10.759 --> 00:14:12.277
หรือว่าจะเป็น websocket เอง

00:14:12.360 --> 00:14:14.477
หรือว่าจะเป็น load a huge table เนี่ย

00:14:14.560 --> 00:14:17.237
เราจะเห็นได้เลยครับว่ามันค่อนข้างที่จะมีประสิทธิภาพ

00:14:17.320 --> 00:14:20.159
และ run ได้เร็วกว่านะครับ

00:14:20.839 --> 00:14:23.756
และตัว version ล่าสุดของ Bun นะครับ

00:14:23.839 --> 00:14:27.200
ก็อยู่ที่ Bun version 1.1.25 นะครับ

00:14:29.399 --> 00:14:32.797
โอเค แล้วก็ในเรื่องของการ installation เนี่ย

00:14:32.880 --> 00:14:34.436
ตัว installation ง่ายๆ เลยครับ

00:14:34.519 --> 00:14:36.316
เราสามารถที่จะ copy คำสั่งจากอินเทอร์เน็ต

00:14:36.399 --> 00:14:37.837
โดยการ curl ของมันนะครับ

00:14:37.920 --> 00:14:41.637
แล้วก็เอามา run ในตัว shell ของ Linux เองก็ได้ครับ

00:14:41.720 --> 00:14:44.797
หรือว่าจะเป็นแบบ install ผ่าน PowerShell

00:14:44.880 --> 00:14:47.240
ก็สามารถที่จะทำได้เช่นเดียวกันครับ

00:14:47.360 --> 00:14:51.320
หรือว่าเราก็สามารถที่จะ run ผ่านตัว npm

00:14:51.600 --> 00:14:55.040
ที่เป็น Node ได้เช่นเดียวกันนั่นเองครับ

00:14:55.720 --> 00:14:59.037
ณ คราวนี้ครับ เดี๋ยวผมจะมา demo กันสั้นๆ ว่า

00:14:59.120 --> 00:15:00.717
เอ๊ะ มัน run กันยังไง

00:15:00.800 --> 00:15:05.240
แล้ว process ที่มัน run เนี่ยมันเป็นยังไงบ้างนะครับ

00:15:05.920 --> 00:15:10.316
โอเคครับ ก็ก่อนอื่นเวลาเราจะตัว install

00:15:10.399 --> 00:15:11.720
ตัว project ของเราเนี่ย

00:15:12.279 --> 00:15:16.157
มันควรที่จะลงบาง framework หรือบาง package

00:15:16.240 --> 00:15:17.077
เพื่อให้มัน run ได้

00:15:17.160 --> 00:15:18.679
นั่นก็คือตัว Bun add Ollama

00:15:18.880 --> 00:15:22.196
หรือว่า add ทั้ง OpenAI ด้วยนะครับ

00:15:22.279 --> 00:15:24.920
บางอันเนี่ยเราสามารถที่จะดู syntax ได้ว่า

00:15:25.199 --> 00:15:27.277
ตัว text generation ปกติเนี่ย

00:15:27.360 --> 00:15:30.316
เราจะ import Ollama อย่างเดียวก็ได้เช่นเดียวกันครับ

00:15:30.399 --> 00:15:33.040
แล้วก็เราสามารถที่จะ set ตัว system ว่า

00:15:33.880 --> 00:15:34.839
เลือก model ว่าเป็น Phi 3.5

00:15:36.519 --> 00:15:39.999
จากนั้นเราก็ set message แล้วก็ set role content ว่า

00:15:40.360 --> 00:15:43.996
สมมุติว่า set ว่า "Why is the sky blue?"

00:15:44.079 --> 00:15:49.556
ก็เราก็ถามว่าทำไมท้องฟ้าสีฟ้าอะไรประมาณนั้นนะครับ

00:15:49.639 --> 00:15:50.960
แล้วเราก็ set ตัว 𝚌𝚘𝚗𝚜𝚘𝚕𝚎.𝚕𝚘𝚐

00:15:51.399 --> 00:15:56.519
เพื่อ print ตัว answer ออกมานั่นเองครับ

00:15:57.199 --> 00:16:01.397
หรือว่าเราจะปรับเปลี่ยนเป็นในเรื่องของ

00:16:01.480 --> 00:16:03.436
streaming response นะครับ

00:16:03.519 --> 00:16:05.676
ซึ่งเมื่อกี้เนี่ย เรา print ออกมา

00:16:05.759 --> 00:16:08.556
เราไม่รู้เลยครับว่าตัว model มัน run อยู่หรือเปล่า

00:16:08.639 --> 00:16:12.316
แต่ว่าถ้าเราปรับมาเป็นแบบ streaming response เนี่ย

00:16:12.399 --> 00:16:14.040
เราสามารถที่จะเห็นได้เลยครับว่า

00:16:15.040 --> 00:16:18.316
ตัว model มันกำลังพิมพ์ให้เราได้เห็นอยู่เช่นเดียวกัน

00:16:18.399 --> 00:16:19.120
นั่นเองครับ

00:16:19.560 --> 00:16:24.480
ซึ่งตัว setting เนี่ยมันก็คล้ายๆ ประมาณนี้นั่นเองนะครับ

00:16:25.160 --> 00:16:28.520
ซึ่งคำถามว่ามันสามารถที่จะ run Ollama

00:16:29.480 --> 00:16:34.240
ใน TypeScript ได้ไหม มัน run ได้นะครับ

00:16:34.920 --> 00:16:37.240
ซึ่งเดี๋ยวผมจะ demo ให้ดูว่า

00:16:38.079 --> 00:16:40.440
หลักการทำงานเนี่ยมันเป็นเช่นใดนะครับ

00:16:48.430 --> 00:17:03.519
โอเคครับ สักครู่นะครับ

00:17:04.439 --> 00:17:06.680
ได้มั้ย จับหน่อย

00:17:07.839 --> 00:17:09.480
โอเคครับ

00:17:10.839 --> 00:17:13.317
ตอนแรกผมจะเข้าตัว Visual Studio Code ครับ

00:17:13.400 --> 00:17:14.880
แต่ว่าจริงๆ แล้วครับ

00:17:15.319 --> 00:17:17.079
เดี๋ยวผมจะทิ้งตัว resource ไว้ว่า

00:17:18.039 --> 00:17:21.960
ตัวโปรเจคผมเขียนอะไรไว้บ้างใน GitHub นะครับ

00:17:22.280 --> 00:17:24.559
หลักๆ แล้วครับเราก็จะเห็นได้ว่า

00:17:25.400 --> 00:17:29.640
ตัวที่เป็นตัว text generation อันนี้ผมขอเคลียร์ทิ้งไว้นะครับ

00:17:29.760 --> 00:17:31.160
ตัว text generate เนี่ย

00:17:31.600 --> 00:17:33.960
ผมก็เซ็ตติ้งไว้ประมาณนี้นะครับ

00:17:34.919 --> 00:17:37.080
โอเค อันนี้เห็นอยู่ใช่มั้ยครับ

00:17:37.520 --> 00:17:38.360
โอเคครับ

00:17:39.000 --> 00:17:41.357
เริ่มแรกก็จะ import ตัว Ollama เข้ามาครับ

00:17:41.440 --> 00:17:45.596
แล้วก็เซ็ตเลยครับว่า model ที่เราเลือกใช้คือตัวไหนนะครับ

00:17:45.679 --> 00:17:47.879
แล้วก็ตั้ง prompt ไว้นะครับ

00:17:48.200 --> 00:17:50.960
สมมุติในกรณีนี้ผมจะเปลี่ยนจาก Gemma-2

00:17:51.440 --> 00:17:52.480
เปลี่ยนตรงนี้แหละครับ

00:17:52.960 --> 00:17:55.520
เปลี่ยนเป็น phi-3.5 ครับ

00:17:56.200 --> 00:17:58.277
แต่ในๆ อื่นเราสามารถที่จะเช็กว่า

00:17:58.360 --> 00:18:00.637
เรา install ตัวของ Ollama แล้วหรือยัง

00:18:00.720 --> 00:18:07.200
เราสามารถพิมพ์ command ว่า Ollama ครับ

00:18:07.880 --> 00:18:10.397
ซึ่ง Ollama พอเราพิมพ์ Ollama ปุ๊บ

00:18:10.480 --> 00:18:14.119
มันก็จะโชว์ command ทุกๆ command ที่เราสามารถใช้นั่นเองนะครับ

00:18:14.240 --> 00:18:16.960
แล้วก็ถ้าสมมุติว่าเราอยากจะ list ว่า

00:18:17.240 --> 00:18:19.157
ตัวโมเดลมันมีตัวไหนบ้างนะครับ

00:18:19.240 --> 00:18:23.397
เราก็สามารถพิมพ์ว่า Ollama ls ครับ

00:18:23.480 --> 00:18:27.037
มันก็จะบอกเลยครับว่า เรามีโมเดลตัวไหนบ้างครับ

00:18:27.120 --> 00:18:28.440
อย่างเช่นตอนนี้ผมมี phi-3.5

00:18:29.480 --> 00:18:32.359
มี Gemma-2 ที่เป็น 2 พันล้านพารามิเตอร์

00:18:32.600 --> 00:18:34.119
แล้วก็ตัว Nomic นะครับ

00:18:34.240 --> 00:18:36.920
ที่ช่วยในเรื่องของ embed text นะครับ

00:18:37.240 --> 00:18:41.716
ซึ่งตอนนี้ผมเขียนตัว 𝚐𝚎𝚗𝚎𝚛𝚊𝚝𝚎𝟷.𝚓𝚜 เสร็จแล้วนะครับ

00:18:41.799 --> 00:18:45.836
ผมก็ทำการ run ตัว bun ใช้คำสั่ง bun เลยครับ

00:18:45.919 --> 00:18:48.920
ที่เป็น runtime แล้วก็ตามด้วยชื่อไฟล์ครับ

00:18:52.840 --> 00:18:54.320
𝚐𝚎𝚗𝚎𝚛𝚊𝚝𝚎𝟷.𝚓𝚜 นะครับ

00:18:54.720 --> 00:18:56.997
ก็มันก็จะรอช่วงนึงครับ

00:18:57.080 --> 00:18:59.920
ให้มันปริ้นท์คำตอบออกมานั่นเองครับ

00:19:00.600 --> 00:19:02.917
ซึ่งพอเราใส่ output อย่างเดียวเนี่ย

00:19:03.000 --> 00:19:04.279
เดี๋ยวเรามาลุ้นกันว่า

00:19:04.520 --> 00:19:07.239
คำตอบที่ได้ออกมานี่จะเป็นยังไงนะครับ

00:19:07.360 --> 00:19:11.816
ก็คำตอบที่ได้ออกมามันจะเป็นไฟล์คล้ายๆ structure ของ JSON

00:19:11.899 --> 00:19:12.397
นั่นเองครับ

00:19:12.480 --> 00:19:13.877
ซึ่งมันจะบอกเลยครับว่า

00:19:13.960 --> 00:19:16.476
model ที่เราใช้เนี่ย ใช้ตัวอะไรนะครับ

00:19:16.559 --> 00:19:17.960
แล้ว create เมื่อไหร่

00:19:18.159 --> 00:19:21.877
แล้วก็มันก็จะบอกครับ response อยู่ที่ response

00:19:21.960 --> 00:19:23.596
มันคืออะไรประมาณนี้นะครับ

00:19:23.679 --> 00:19:25.879
แล้วแน่นอนครับมันก็จะมี evaluation count

00:19:26.200 --> 00:19:27.840
แล้วก็ตัว prompt count นั่นเองครับ

00:19:29.200 --> 00:19:37.520
ถ้าผมเปลี่ยนเป็น 𝚘𝚞𝚝𝚙𝚞𝚝.𝚛𝚎𝚜𝚙𝚘𝚗𝚜𝚎 นะครับ

00:19:38.320 --> 00:20:09.359
แล้วเรามาลอง run อีกรอบนึงครับ

00:20:10.559 --> 00:20:11.117
sorry ครับ

00:20:11.200 --> 00:20:20.680
โอเคครับ รอสักครู่ครับ

00:20:21.360 --> 00:20:25.357
ซึ่งมันก็จะใช้ตัว resource ภายในเครื่อง

00:20:25.440 --> 00:20:27.076
แล้วก็เอามาประมวลผลนั่นเองครับ

00:20:27.159 --> 00:20:32.960
มันก็จะได้เป็น 2 + 5 = 7 ก็ถูกนะครับ

00:20:33.640 --> 00:20:37.239
สมมุติว่าผม code ตัว rag นะครับ

00:20:37.400 --> 00:20:39.877
ตัว rag เองใน demo วันนี้นะครับ

00:20:39.960 --> 00:20:44.076
ก็จะเป็นตัว rag ภายใต้ไฟล์ของ CSV นะครับ

00:20:44.159 --> 00:20:46.476
ซึ่งผมก็จะ import ตัว 𝚏𝚜 ออกมา

00:20:46.559 --> 00:20:48.317
แล้วก็ตัวของ openai ครับ

00:20:48.400 --> 00:20:51.397
แล้วก็อาจจะมีการ set ในเรื่องของ question ว่า

00:20:51.480 --> 00:20:55.637
Prius รุ่นไหนเร็วที่สุดอะไรประมาณนั้นนะครับ

00:20:55.720 --> 00:20:56.997
แล้วก็แน่นอนครับ

00:20:57.080 --> 00:21:01.836
ผมต้องดึงข้อมูลจากตัวไฟล์ CSV ที่เป็นตัว hybrid CSV

00:21:01.919 --> 00:21:04.956
ก็ตัว structure ไฟล์มันก็จะประมาณนี้ครับ

00:21:05.039 --> 00:21:08.080
แล้วก็คั่นด้วย comma อะไรประมาณนั้นนะครับ

00:21:08.320 --> 00:21:11.797
แล้วก็ตัวของ result ออกมาเนี่ย

00:21:11.880 --> 00:21:13.196
ผมอยากจะออกมาเป็น markdown

00:21:13.279 --> 00:21:17.320
แล้วก็ผมก็จะอิงตัวของ openai นะครับ

00:21:18.640 --> 00:21:21.080
เอามา run แต่ว่าคำถามคือ

00:21:21.480 --> 00:21:25.037
มีคนอาจจะสงสัยว่าถ้าเรารันของ openai ด้วยเนี่ย

00:21:25.120 --> 00:21:27.157
เราจะต้องใส่ API key ด้วยไหมครับ

00:21:27.240 --> 00:21:30.637
คำตอบคือเราไม่จำเป็นที่จะต้องใส่ตัว API key นะครับ

00:21:30.720 --> 00:21:33.076
แล้วก็ใส่ตัว base URL แล้วก็ API key

00:21:33.159 --> 00:21:35.677
ก็เขียนว่า no needed by Ollama นะครับ

00:21:35.760 --> 00:21:37.557
แล้วก็ตัว structure ที่สร้างเนี่ย

00:21:37.640 --> 00:21:40.040
มันก็จะประมาณนี้นะครับผม

00:21:40.880 --> 00:21:42.196
พอเรามารันนะครับ

00:21:42.279 --> 00:21:47.040
เราก็สามารถใส่คำสั่งได้เลยครับว่า bun sample

00:21:47.400 --> 00:21:50.879
ผมใส่ในไฟล์ sample ครับ

00:21:51.559 --> 00:21:57.239
แล้วก็ขีดด้วย 07 𝚛𝚊𝚐.𝚝𝚜 ครับ

00:21:57.440 --> 00:22:00.076
มันก็ไฟล์ spec map นะครับ

00:22:00.159 --> 00:22:03.716
แล้วก็มาตอบครับว่า fastest Prius คือเท่าไหร่

00:22:03.799 --> 00:22:06.640
แล้วมันก็จะตอบว่า accelerate time อยู่ที่ 8.82

00:22:07.480 --> 00:22:12.399
ซึ่งเป็นรุ่นของ Prius PHV นะครับ from 2012

00:22:13.520 --> 00:22:15.560
อะไรประมาณนั้นนะครับ

00:22:16.240 --> 00:22:20.277
ถ้าสมมุติว่าเราอยากดูตัว streaming ใน browser นะครับ

00:22:20.360 --> 00:22:26.196
ก็เราก็ set ประมาณนี้ครับ

00:22:26.279 --> 00:22:29.157
แล้วก็เราลองเล่นกับมันนะครับ

00:22:29.240 --> 00:22:37.840
อย่างเช่น bun sample ครับ

00:22:38.520 --> 00:22:47.357
แล้วก็เป็น 04 𝚜𝚝𝚛𝚎𝚊𝚖.𝚝𝚜 นะครับ

00:22:47.440 --> 00:22:50.237
แล้วมันก็ค่อยๆ พิมพ์อะไรประมาณนี้นะครับ

00:22:50.320 --> 00:22:53.956
เราใช้คำสั่งว่า say hello in pirate state

00:22:54.039 --> 00:22:55.596
ก็ ahoy อะไรประมาณนั้นครับ

00:22:55.679 --> 00:22:59.317
แล้วก็จริงๆ เราเองก็สามารถที่จะ develop

00:22:59.400 --> 00:23:02.320
ให้มันเป็นตัวโปรแกรมที่สามารถที่ summarize

00:23:02.440 --> 00:23:04.557
ตัวไฟล์ txt ได้นั่นเองครับ

00:23:04.640 --> 00:23:08.960
ก็สมมุติว่าเป็น run bun sample เป็น 06 ครับ

00:23:10.200 --> 00:23:18.920
06-𝚜𝚞𝚖𝚖𝚊𝚛𝚒𝚣𝚎𝚛.𝚝𝚜 นะครับ

00:23:21.480 --> 00:23:24.320
แต่ว่าอันนี้มันอาจจะใช้เวลานานนะครับ

00:23:25.559 --> 00:23:28.637
เดี๋ยวเราค่อยมาดูกันว่ามันจะเป็นยังไงนะครับ

00:23:28.720 --> 00:23:29.757
เดี๋ยวผมค้างไว้ตรงนี้

00:23:29.840 --> 00:23:39.599
เดี๋ยวเรามาคุยในเรื่องของ key takeaway กันก่อนนะครับ

00:23:40.440 --> 00:23:41.760
ขอบคุณนะครับ

00:23:42.640 --> 00:23:45.640
โอเคครับ หลักๆ แล้วเนี่ย

00:23:46.640 --> 00:23:50.476
มีคนก็จะตั้งคำถามว่าเวลาเรา develop เองเนี่ย

00:23:50.559 --> 00:23:53.517
เราจะเลือก python ดีหรือว่าจะเป็น javascript ดีนะครับ

00:23:53.600 --> 00:23:54.797
ซึ่งหลักๆ เองเนี่ย

00:23:54.880 --> 00:23:58.836
คือในเรื่องของ framework ในการ run AI ใน python

00:23:58.919 --> 00:24:01.640
เนี่ยค่อนข้างที่จะมี library มากกว่าครับ

00:24:01.840 --> 00:24:03.840
ส่วน javascript เนี่ยมันอาจจะแบบว่า

00:24:04.320 --> 00:24:05.960
มี framework ที่จะเอาไว้ run

00:24:06.080 --> 00:24:09.917
แบบว่าใน framework ที่เป็น web development

00:24:10.000 --> 00:24:10.997
ซะส่วนใหญ่นะครับ

00:24:11.080 --> 00:24:14.517
แต่ว่าถ้าเริ่มในเรื่องของ fundamental จริงๆ เนี่ย

00:24:14.600 --> 00:24:16.840
อยากจะให้เริ่มในฝั่งของ python นะครับ

00:24:16.960 --> 00:24:18.157
แต่ว่าจริงๆ แล้วเนี่ย

00:24:18.240 --> 00:24:20.317
ใน use case ของการใช้ javascript

00:24:20.400 --> 00:24:26.999
มันก็จะมีมากส่วนหนึ่งนั่นเองนะครับ

00:24:27.679 --> 00:24:30.080
แล้วก็ผมอยากจะทิ้งท้ายว่า

00:24:31.000 --> 00:24:32.960
ณ trend ตอนนี้ครับว่า

00:24:33.600 --> 00:24:37.797
การ train language model หรือ large language model เนี่ย

00:24:37.880 --> 00:24:40.037
มันยังมีความสำคัญของมันอยู่หรือเปล่า

00:24:40.120 --> 00:24:41.799
ซึ่งจริงๆ แล้วผมมองว่า

00:24:42.039 --> 00:24:43.716
มันอาจจะมีความสำคัญบ้างนะครับ

00:24:43.799 --> 00:24:47.319
แต่ว่ามันอาจจะไม่สำคัญในอนาคต

00:24:47.480 --> 00:24:50.637
ซึ่งคำถามคือเขาจะไปดูในเรื่องไหนมากที่สุด

00:24:50.720 --> 00:24:53.479
เขาอาจจะไปดูในเรื่องของการตอบคำถาม

00:24:53.640 --> 00:24:57.960
หรือว่าวิธีการประมวลผลว่าทำไมมันถึงตอบอย่างนั้น

00:24:58.360 --> 00:25:01.157
ทำไมมันถึงทำงานอย่างนั้น อะไรประมาณนี้นะครับ

00:25:01.240 --> 00:25:02.797
แล้วก็แน่นอนครับ

00:25:02.880 --> 00:25:05.517
เราเองเราค่อนข้างที่จะไม่ค่อย trust ตัว answers

00:25:05.600 --> 00:25:08.037
ในตัวของ language model

00:25:08.120 --> 00:25:10.476
ซึ่งมันก็จะมีในเรื่องของ trustworthiness

00:25:10.559 --> 00:25:11.677
แล้วก็ responsible AI

00:25:11.760 --> 00:25:14.640
ที่จะเอามา develop มากยิ่งขึ้นนั่นเองนะครับ

00:25:14.840 --> 00:25:16.840
แน่นอนครับมันก็จะมีในเรื่องของ trend

00:25:17.000 --> 00:25:19.716
ในการใช้ small language model มากยิ่งขึ้นในอนาคตครับ

00:25:19.799 --> 00:25:21.759
แล้วก็ในเรื่องของ miniaturization

00:25:22.120 --> 00:25:25.677
ก็เราก็จะเพิ่มในเรื่องของ language fluency

00:25:25.760 --> 00:25:28.440
แล้วก็ knowledge base ให้มันฉลาดมากยิ่งขึ้น

00:25:28.559 --> 00:25:30.760
ใน device ที่มีขนาดเล็กลง

00:25:30.919 --> 00:25:34.357
หรือว่าจะเป็นพวกแล็ปท็อปที่พวกเราใช้

00:25:34.440 --> 00:25:36.920
หรือว่าจะบนสมาร์ตโฟนนั่นเองครับ

00:25:37.159 --> 00:25:40.600
ซึ่งในตัวของ research ของ Phi-3 ที่ออกมาเนี่ย

00:25:40.919 --> 00:25:45.316
เขาก็ได้ทดลองในการใช้ small language model บนมือถือ

00:25:45.399 --> 00:25:49.160
เนี่ยมันก็ค่อนข้างที่จะลื่นอยู่พอสมควรครับ

00:25:49.840 --> 00:25:53.677
และแน่นอนครับก็จะเป็นในเรื่องของ federated learning

00:25:53.760 --> 00:25:55.719
ก็จะ train จากตัวใหญ่

00:25:55.919 --> 00:25:59.997
แล้วก็มากระจายมาสู่ตัวของเครื่อง device

00:26:00.080 --> 00:26:04.520
ที่มีขนาดเล็กหลายๆ เครื่องได้นั่นเองนะครับ

00:26:05.200 --> 00:26:06.880
และคำถามสุดท้ายก็คือ

00:26:08.000 --> 00:26:11.037
หลายคนอาจจะสงสัยว่าทำไมคนไทย

00:26:11.120 --> 00:26:13.480
หรือว่าหลายๆ คนที่อยู่แถวนี้

00:26:13.679 --> 00:26:15.716
เราอาจจะยังไม่รู้จัก small language model

00:26:15.799 --> 00:26:17.279
ก็จะมีเหตุผลว่า

00:26:17.880 --> 00:26:20.357
ตัวประสิทธิภาพของมันมันอาจจะ hallucinate

00:26:20.440 --> 00:26:22.196
หรือว่าอาจจะเกิดอาการหลอนนะครับ

00:26:22.279 --> 00:26:25.479
หรือว่า performance อาจจะยังไม่ดีพอนะครับ

00:26:25.679 --> 00:26:27.960
หรือว่าตัว resources ของ device

00:26:28.399 --> 00:26:32.157
ซึ่งศักยภาพของเครื่องมันอาจจะรันไม่ได้แบบเต็มที่นะครับ

00:26:32.240 --> 00:26:34.840
แล้วก็แน่นอนครับเรื่อง language barrier เนี่ย

00:26:35.000 --> 00:26:37.797
บางทีเนี่ยมันอาจจะตอบภาษาไทยไม่รู้เรื่อง

00:26:37.880 --> 00:26:40.917
หรืออาจจะตอบใน context ที่ค่อนข้างที่จะผิด

00:26:41.000 --> 00:26:44.437
หรือว่าอันสุดท้ายเนี่ยคือเรื่องของ low engagement

00:26:44.520 --> 00:26:47.560
ก็อาจจะไม่ได้เล่าสู่กันฟังอย่างทั่วถึง

00:26:47.880 --> 00:26:48.997
ซึ่งวันนี้เองเนี่ย

00:26:49.080 --> 00:26:52.757
ก็ผมเป็นตัวแทนของ Microsoft Learn Student Ambassadors เนี่ย

00:26:52.840 --> 00:26:58.200
ก็อยากจะมาแนะนำในเรื่องของการใช้ small language model

00:26:58.520 --> 00:27:02.476
อยากให้ทุกคนได้ลองใช้แล้วก็ลองสนุกไปกับมันนั่นเองครับ

00:27:02.559 --> 00:27:07.799
ก็อยากจะฝากเรื่องราวเล็กๆ น้อยๆ จาก

00:27:08.440 --> 00:27:10.600
ที่เรารู้อยู่แล้วเนี่ย

00:27:11.200 --> 00:27:13.637
เราก็มาเรียนรู้กันเพิ่มขึ้นนั่นเองนะครับ

00:27:13.720 --> 00:27:17.557
ก็สามารถที่จะ follow ผมใน Facebook

00:27:17.640 --> 00:27:21.237
แล้วก็สามารถที่จะ follow ใน Instagram ได้เช่นเดียวกันนะครับ

00:27:21.320 --> 00:27:26.080
ก็หลักๆ แล้วเนี่ยผมก็จะโยนตัวพวกข้อมูลต่างๆ

00:27:26.200 --> 00:27:30.877
พวกสิ่งที่ผม train หรือว่าในเรื่องของ tech ใหม่ๆ

00:27:30.960 --> 00:27:34.037
เนี่ยผมก็จะโพสต์ไม่ว่าจะเป็นช่องทางของ Facebook

00:27:34.120 --> 00:27:36.757
แล้วก็ทาง Instagram ซะส่วนใหญ่นั่นเองครับ

00:27:36.840 --> 00:27:38.277
แต่ว่าเวลาเขียนบล็อกเองเนี่ย

00:27:38.360 --> 00:27:42.119
ผมก็จะเขียนผ่าน Medium นะครับ

00:27:42.799 --> 00:27:45.557
แล้วก็เรากลับไปดูที่ results นะครับ

00:27:45.640 --> 00:27:50.560
ก็พอเรารันตัว Bun ตัว sample ของ summarizer

00:27:50.799 --> 00:27:53.956
มันก็จะดึงตัว text ที่ผมมีอยู่ในไฟล์ของ data

00:27:54.039 --> 00:27:55.797
ที่เป็น AI Wikipedia นะครับ

00:27:55.880 --> 00:27:58.557
แล้วมันก็สรุปเลยครับว่ามันมีอะไรบ้าง

00:27:58.640 --> 00:28:04.997
AI มีอะไรบ้างแล้วก็มันก็ไล่ๆ ตัวพวก section

00:28:05.080 --> 00:28:08.157
ว่ามีอะไรบ้างพวก human rights AI อะไรประมาณนี้ครับ

00:28:08.240 --> 00:28:10.040
มันก็ list ออกมาเลยครับ

00:28:10.679 --> 00:28:13.399
ซึ่งจริงๆ แล้วเนี่ย ผมเองผมก็ set ว่า

00:28:14.440 --> 00:28:16.840
original text ก็จะมีอยู่ที่ 74,000

00:28:17.600 --> 00:28:20.076
แล้วก็ summarize text ก็เห็นได้ชัดเลยครับ

00:28:20.159 --> 00:28:22.120
ว่ามันลดไปประมาณนึง

00:28:22.279 --> 00:28:25.879
ก็เหลือสัก 4,400 characters อะไรประมาณนั้นนั่นเองนะครับ

00:28:26.279 --> 00:28:28.200
ก็ session ผมมีประมาณนี้

00:28:28.399 --> 00:28:30.956
ก็ขอบคุณทุกๆ คนที่มาและรับชมรับฟังครับผม

00:28:31.039 --> 00:28:32.680
ขอบคุณครับ

00:28:33.360 --> 00:28:35.119
ขอบคุณน้องโบ๊ทมากเลยครับ

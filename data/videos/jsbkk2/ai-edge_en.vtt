WEBVTT - Auto-generated by https://github.com/dtinth/autosub

00:00:00.180 --> 00:00:04.520
ขอบคุณครับ สวัสดีครับ Thank you.

00:00:05.440 --> 00:00:07.959
Can you all hear me okay in the back?

00:00:08.559 --> 00:00:11.640
That's great. I see some thumbs up there.

00:00:12.440 --> 00:00:16.557
I have a little demo that actually is not super

00:00:16.640 --> 00:00:19.356
related to my talk, but I was just working on it

00:00:19.439 --> 00:00:21.160
yesterday and I thought it was pretty cool.

00:00:21.400 --> 00:00:23.880
So I'd love to try it out with you.

00:00:24.760 --> 00:00:26.797
And I'm going to film you while we do it.

00:00:26.880 --> 00:00:29.480
So we're doing a little exercise.

00:00:30.960 --> 00:00:34.476
So basically, what this does is it just captures

00:00:34.559 --> 00:00:35.640
the video.

00:00:35.760 --> 00:00:38.000
So I'm just going to capture you.

00:00:39.040 --> 00:00:42.757
And then I'm going to use Google Gemini to basically

00:00:42.840 --> 00:00:45.840
give me a description of what's happening in the video.

00:00:46.559 --> 00:00:49.760
And then I'm just using ElevenLabs to give me

00:00:50.760 --> 00:00:53.877
text-to-speech to just say what's happening.

00:00:53.960 --> 00:01:00.037
So what I'd love for you to do is if you just clap

00:01:00.120 --> 00:01:02.599
or put your arms up in the air or maybe stand up,

00:01:03.280 --> 00:01:06.716
just do some unpredictable stuff and let's see what

00:01:06.799 --> 00:01:10.917
Google Gemini thinks is happening.

00:01:11.000 --> 00:01:17.080
Okay, so let's go in three, two, one.

00:01:17.840 --> 00:01:23.479
Okay, applause, some waving, stand up, jump.

00:01:24.159 --> 00:01:26.797
Okay, let's see what's happening there.

00:01:26.880 --> 00:01:29.880
And hopefully, I pray to the demo gods.

00:01:37.159 --> 00:01:39.599
Okay, I don't think we have audio.

00:01:40.119 --> 00:01:43.040
Do we have audio? Sorry, I probably should have told.

00:01:47.079 --> 00:01:48.157
It's muted, microphone.

00:01:48.240 --> 00:01:49.479
It's muted? Okay.

00:01:50.040 --> 00:01:53.076
Yeah, I didn't tell them that I want audio.

00:01:53.159 --> 00:01:54.716
So I can just read it out.

00:01:54.799 --> 00:01:57.716
The camera pans up to show an auditorium full of

00:01:57.799 --> 00:01:59.160
people sitting in chairs.

00:01:59.439 --> 00:02:01.519
Many of them are wearing face masks.

00:02:01.920 --> 00:02:04.359
Is that true? That doesn't seem true.

00:02:05.280 --> 00:02:06.956
The people are waving and cheering.

00:02:07.039 --> 00:02:12.199
Cheering? That's very good.

00:02:12.879 --> 00:02:13.119
Let me close the microphone.

00:02:15.959 --> 00:02:16.279
Okay.

00:02:16.959 --> 00:02:19.240
I put my audio through the HDMI.

00:02:19.519 --> 00:02:20.640
Can you use that? No?

00:02:23.080 --> 00:02:25.516
You got the aux.

00:02:25.599 --> 00:02:26.919
I think it's okay.

00:02:27.599 --> 00:02:28.440
Yeah, it's fine.

00:02:28.640 --> 00:02:30.439
We'll just use my voice.

00:02:31.440 --> 00:02:35.837
So yeah, I think it kind of got the gist of that,

00:02:35.920 --> 00:02:37.560
right? Like waving and cheering.

00:02:37.959 --> 00:02:38.477
That's great.

00:02:38.560 --> 00:02:42.156
So anyway, that was just a little excursion before

00:02:42.239 --> 00:02:42.920
we dive in.

00:02:44.040 --> 00:02:46.960
So that demo was actually using APIs.

00:02:57.800 --> 00:03:04.199
So that was using the Gemini API and the ElevenLabs API.

00:03:04.319 --> 00:03:07.320
But what we want to talk today is kind of local AI

00:03:07.440 --> 00:03:12.280
and actually using LLMs and inference in the browser

00:03:12.640 --> 00:03:14.478
and in edge runtimes.

00:03:14.959 --> 00:03:16.679
So this is me.

00:03:17.440 --> 00:03:20.917
My name is Thor or if you're speaking Mandarin,

00:03:21.000 --> 00:03:21.640
Leishen.

00:03:22.159 --> 00:03:23.560
I gave that name myself.

00:03:24.000 --> 00:03:26.397
Since no one was laughing, I guess there's no

00:03:26.480 --> 00:03:27.880
Mandarin speakers in the room.

00:03:28.439 --> 00:03:29.279
No? Okay.

00:03:30.319 --> 00:03:32.436
My Thai unfortunately is not so great.

00:03:32.519 --> 00:03:36.160
But yeah, if you want to see kind of fun little demos,

00:03:36.480 --> 00:03:38.600
you can follow me on Twitter.

00:03:39.159 --> 00:03:40.440
You can just scan that here.

00:03:40.920 --> 00:03:44.837
And this QR code is just a link to my slides because

00:03:44.920 --> 00:03:48.040
there's a bunch of demos and resources in the slides.

00:03:48.920 --> 00:03:51.637
So you can just keep that for the future and check

00:03:51.720 --> 00:03:53.959
out the demos later on.

00:03:56.920 --> 00:03:57.240
Cool.

00:03:58.280 --> 00:04:09.677
So before we dive into running large language models

00:04:09.760 --> 00:04:15.079
in the browser, let's look at running them locally

00:04:15.200 --> 00:04:16.359
on your machine.

00:04:17.000 --> 00:04:20.800
Who here has used things like Ollama before?

00:04:21.320 --> 00:04:21.720
Okay.

00:04:22.160 --> 00:04:25.240
I think that's probably the most popular one here.

00:04:26.120 --> 00:04:31.077
So Ollama, another project that Mozilla is working on

00:04:31.160 --> 00:04:33.160
is called Llamafile.

00:04:33.919 --> 00:04:36.359
Have people heard of Llamafile here?

00:04:37.039 --> 00:04:37.279
No?

00:04:37.600 --> 00:04:38.000
Yeah.

00:04:38.520 --> 00:04:38.920
Okay.

00:04:39.919 --> 00:04:44.240
And then one that's actually built in Singapore

00:04:45.360 --> 00:04:46.960
is called Cortex.

00:04:47.479 --> 00:04:51.596
So cortex.so, people might have heard of jan.ai,

00:04:51.680 --> 00:04:58.199
so JAN, which is kind of a local ChatGPT client,

00:04:58.960 --> 00:05:04.400
that is powered by Cortex as the underlying API engine.

00:05:06.000 --> 00:05:10.319
And what's really cool with that, for example,

00:05:11.919 --> 00:05:15.000
all of these are powered by llama.cpp.

00:05:16.639 --> 00:05:20.079
So basically llama.cpp is kind of the underlying

00:05:20.840 --> 00:05:25.519
implementation of the inference models underneath.

00:05:26.919 --> 00:05:30.997
Open source project, really neat, and that is what

00:05:31.080 --> 00:05:35.317
is powering a lot of the local model stuff.

00:05:35.400 --> 00:05:37.520
So CPP is just C++.

00:05:38.680 --> 00:05:42.480
It's basically an inference engine written in C++

00:05:43.440 --> 00:05:46.960
that can run on a variety of hardware,

00:05:47.319 --> 00:05:50.797
including your CPU if you're using that locally

00:05:50.880 --> 00:05:52.760
on your machine and don't have a GPU,

00:05:53.560 --> 00:05:57.040
it's basically running the workloads on your GPU.

00:05:57.800 --> 00:05:58.200
Okay.

00:05:58.800 --> 00:06:02.559
Now, let me quickly show you Llamafile,

00:06:02.680 --> 00:06:05.520
which is a really neat project as well.

00:06:06.319 --> 00:06:08.800
So basically what they are doing is they are bundling

00:06:08.960 --> 00:06:13.000
llama.cpp, the engine, together with the model

00:06:13.160 --> 00:06:14.560
in a single file.

00:06:14.960 --> 00:06:18.239
So what you can do then is you can download

00:06:18.599 --> 00:06:22.197
the llama file, and you can run it locally

00:06:22.280 --> 00:06:24.039
on your machine.

00:06:24.479 --> 00:06:26.360
So we can do that here.

00:06:26.479 --> 00:06:29.800
So basically, I just use tiny llama here.

00:06:30.520 --> 00:06:35.836
You're running that, optimized for CPU usage

00:06:35.919 --> 00:06:36.680
in this case.

00:06:37.680 --> 00:06:41.640
And then you actually get like a localhost client.

00:06:41.960 --> 00:06:44.440
It doesn't look as nice, but you can say

00:06:44.639 --> 00:06:47.839
"Who are you?"

00:06:48.720 --> 00:06:50.960
And then you can chat with it and it basically says,

00:06:51.319 --> 00:06:53.239
"I'm llama, nice to meet you too."

00:06:54.000 --> 00:06:58.320
And then we can say maybe, "Tell me about Bangkok."

00:07:01.280 --> 00:07:04.956
"The city has so much to offer, one of the most

00:07:05.039 --> 00:07:07.597
vibrant and exciting cities in Southeast Asia."

00:07:07.680 --> 00:07:08.159
There we go.

00:07:08.360 --> 00:07:11.796
I think Llamafile knows that I'm in Bangkok

00:07:11.879 --> 00:07:14.479
and it's trying to please.

00:07:17.720 --> 00:07:21.639
Yes, so these are some of the projects that are

00:07:22.520 --> 00:07:31.679
exciting for running large language models locally.

00:07:31.800 --> 00:07:35.879
So if you have a beefy machine, you can probably go up

00:07:36.280 --> 00:07:38.920
to some of the larger models as well.

00:07:39.280 --> 00:07:43.796
Tiny llama I think just has a fairly small token

00:07:43.879 --> 00:07:48.840
window that's included, but it works quite well

00:07:49.080 --> 00:07:50.320
on the machine.

00:07:50.680 --> 00:07:56.037
Now, before we dive into AI in the browser

00:07:56.120 --> 00:07:59.560
and actually running your inference directly

00:08:00.800 --> 00:08:04.400
in the browser, let's do a little demo here.

00:08:05.000 --> 00:08:08.920
So what this does is, actually similar to what

00:08:09.240 --> 00:08:12.879
Piti was talking about just before,

00:08:13.720 --> 00:08:16.279
vector search within MongoDB.

00:08:16.720 --> 00:08:21.679
So what this is doing is basically a semantic search,

00:08:22.360 --> 00:08:27.116
or vector similarity search, but fully locally

00:08:27.199 --> 00:08:29.080
in the browser.

00:08:29.639 --> 00:08:38.039
So the idea behind semantic search is that instead of

00:08:38.599 --> 00:08:40.836
text search where you're looking for words

00:08:40.919 --> 00:08:42.600
and similarities in the words,

00:08:43.240 --> 00:08:46.717
you're looking for semantic similarity, right?

00:08:46.800 --> 00:08:50.159
So you're looking, okay, what is fun?

00:08:51.200 --> 00:08:54.600
And based on human knowledge and context here,

00:08:55.480 --> 00:08:58.159
the model thinks, yeah, driving a car can be fun,

00:08:58.600 --> 00:09:00.240
playing with a dog can be fun,

00:09:01.000 --> 00:09:03.960
sleeping can be great fun as well.

00:09:08.279 --> 00:09:10.680
And the great thing is so basically the first time

00:09:11.079 --> 00:09:14.357
when I hit the button, it was loading for a bit,

00:09:14.440 --> 00:09:16.999
but it was actually loading down the model

00:09:17.399 --> 00:09:18.560
from Hugging Face.

00:09:18.959 --> 00:09:21.357
So Hugging Face, kind of like the GitHub

00:09:21.440 --> 00:09:23.240
for large language models.

00:09:23.680 --> 00:09:26.077
So it was pulling down that model locally

00:09:26.160 --> 00:09:27.320
into the browser.

00:09:27.680 --> 00:09:30.277
And so now what I can do is actually when I turn off

00:09:30.360 --> 00:09:34.277
my Wi-Fi, so I now have the model loaded locally

00:09:34.360 --> 00:09:39.280
into the browser, I can, yeah.

00:09:39.720 --> 00:09:40.240
All good?

00:09:44.399 --> 00:09:51.489
[announcement about tax invoice in Thai]

00:09:52.399 --> 00:09:52.879
Sorry.

00:09:53.160 --> 00:09:53.560
Next.

00:09:53.880 --> 00:09:55.880
I have no idea what he said.

00:09:56.560 --> 00:10:02.400
But no one is leaving, so I hope it wasn't too bad.

00:10:04.079 --> 00:10:05.560
I hope I didn't say anything bad.

00:10:05.720 --> 00:10:06.479
All good?

00:10:07.079 --> 00:10:09.360
Do we need to translate for the English speakers or?

00:10:09.839 --> 00:10:10.079
No?

00:10:10.640 --> 00:10:11.280
No, no, no.

00:10:11.959 --> 00:10:12.359
Okay.

00:10:12.640 --> 00:10:15.440
So this is information that's only relevant to Thai people.

00:10:16.160 --> 00:10:16.799
Very good.

00:10:17.200 --> 00:10:18.320
So, okay.

00:10:18.519 --> 00:10:21.637
We had fun, but maybe what we can do as well is we can

00:10:21.720 --> 00:10:23.840
search for furniture.

00:10:24.120 --> 00:10:24.597
Right?

00:10:24.680 --> 00:10:30.160
If you had a normal search where you're doing text search,

00:10:30.560 --> 00:10:32.077
you wouldn't find any of this, right?

00:10:32.160 --> 00:10:34.597
It would look for the word sort of furniture

00:10:34.680 --> 00:10:36.800
or misspellings of furniture.

00:10:37.000 --> 00:10:39.400
But what we want here is semantically,

00:10:39.880 --> 00:10:44.797
without having to categorize the information,

00:10:44.880 --> 00:10:48.560
we actually want to encode this information,

00:10:48.839 --> 00:10:53.080
all the human knowledge, the semantics, the context

00:10:53.680 --> 00:10:54.316
into this here.

00:10:54.399 --> 00:10:56.720
And so this is where the embeddings come in.

00:10:57.240 --> 00:11:00.237
So the large language model basically turns

00:11:00.320 --> 00:11:03.640
the entire context of this information

00:11:03.839 --> 00:11:05.680
into just an array of numbers.

00:11:06.040 --> 00:11:08.556
And then what we can do is we can search for things

00:11:08.639 --> 00:11:09.719
that are similar.

00:11:10.240 --> 00:11:12.556
And so what we do is we also create an embedding

00:11:12.639 --> 00:11:13.759
for the furniture,

00:11:13.959 --> 00:11:17.837
and then we just perform a similarity search.

00:11:17.920 --> 00:11:20.797
We just look, okay, what is similar to this?

00:11:20.880 --> 00:11:24.316
And we see, okay, desk, bed, chair is furniture.

00:11:24.399 --> 00:11:28.279
Now what we can do as well here is food for example.

00:11:28.560 --> 00:11:33.480
So tomato, banana, a hot dog.

00:11:34.040 --> 00:11:35.240
Okay, this is a hot dog.

00:11:35.360 --> 00:11:35.920
Yes, no.

00:11:37.279 --> 00:11:39.399
So those are food items, right, that we can eat.

00:11:40.440 --> 00:11:41.999
Or fruit.

00:11:42.519 --> 00:11:47.477
So we know a tomato is a fruit, right?

00:11:47.560 --> 00:11:50.196
But we wouldn't put it in a fruit salad ideally.

00:11:50.279 --> 00:11:52.040
But yeah, banana, apple, tomato.

00:11:52.480 --> 00:11:57.519
But now if we do electronics.

00:11:57.639 --> 00:11:57.959
Yes.

00:11:58.200 --> 00:11:59.277
Someone said Apple.

00:11:59.360 --> 00:12:00.677
Correct, right?

00:12:00.760 --> 00:12:04.959
As humans, we know this thing is an Apple electronic device.

00:12:05.240 --> 00:12:07.277
If I were to put that into a text search,

00:12:07.360 --> 00:12:08.600
you wouldn't find that.

00:12:09.240 --> 00:12:10.196
But so this is really cool.

00:12:10.279 --> 00:12:11.440
My Wi-Fi is still off.

00:12:12.079 --> 00:12:16.040
So this is running completely locally in my browser.

00:12:17.639 --> 00:12:22.359
Including actually a full Postgres database.

00:12:22.639 --> 00:12:24.996
So this is a really cool open source project.

00:12:25.079 --> 00:12:26.999
It's called PGlite,

00:12:27.720 --> 00:12:32.839
which actually basically runs Postgres in your browser

00:12:33.199 --> 00:12:35.756
via Wasm, which is really cool.

00:12:35.839 --> 00:12:38.679
So if you have some time, check out that demo.

00:12:39.560 --> 00:12:40.640
That's really neat.

00:12:40.839 --> 00:12:45.719
But now let's maybe dig into how that works under the hood.

00:12:47.320 --> 00:12:51.436
So how this works in the browser is actually

00:12:51.519 --> 00:12:54.960
it's using this ONNX runtime.

00:12:55.279 --> 00:12:57.999
So Open Neural Network Exchange.

00:12:58.399 --> 00:13:01.320
And I think actually originally it's an open source project

00:13:01.680 --> 00:13:03.999
by a team at Microsoft.

00:13:04.880 --> 00:13:07.520
Now more and more companies are contributing to it as well.

00:13:08.000 --> 00:13:14.080
But the idea is to provide this ONNX format

00:13:14.720 --> 00:13:18.880
as well as a runtime where we can train our models

00:13:19.199 --> 00:13:23.200
and output them in a format that we can then run

00:13:24.000 --> 00:13:28.600
on this ONNX runtime, TensorRT, Apache MXNet.

00:13:29.120 --> 00:13:31.599
So it's kind of a compatibility layer

00:13:32.720 --> 00:13:35.639
for different large language models

00:13:35.880 --> 00:13:41.039
to allow us to run these models on various hardware,

00:13:41.279 --> 00:13:43.120
various different devices.

00:13:43.839 --> 00:13:46.199
And so what this actually means is,

00:13:46.480 --> 00:13:48.960
when we're using the ONNX runtime,

00:13:49.680 --> 00:13:51.600
we can deploy that in the cloud,

00:13:53.519 --> 00:13:55.797
edge devices, mobile applications actually.

00:13:55.880 --> 00:13:59.797
So there's an ONNX runtime that can run locally

00:13:59.880 --> 00:14:01.200
on your phone,

00:14:02.120 --> 00:14:04.440
but then also browser.

00:14:04.800 --> 00:14:06.116
And so for the web browser,

00:14:06.199 --> 00:14:10.720
we have an ONNX runtime that runs in Wasm.

00:14:11.480 --> 00:14:14.996
So basically using Wasm with the ONNX runtime,

00:14:15.079 --> 00:14:17.676
we can run these language models

00:14:17.759 --> 00:14:23.080
that are in the ONNX format directly in the browser,

00:14:23.199 --> 00:14:24.359
which is really neat.

00:14:24.720 --> 00:14:27.159
So if you look for example on Hugging Face,

00:14:27.320 --> 00:14:29.360
you can just put in ONNX,

00:14:30.440 --> 00:14:32.040
and you can find all the models

00:14:32.360 --> 00:14:34.560
that are basically in this ONNX format,

00:14:35.160 --> 00:14:37.919
which you can then use in the browser.

00:14:38.160 --> 00:14:43.640
And how do you actually use them in practice in the browser?

00:14:44.240 --> 00:14:47.277
And that is a great open source project actually

00:14:47.360 --> 00:14:51.879
by Hugging Face which is called Transformers JS.

00:14:52.480 --> 00:14:53.917
So again, if you go into the slides,

00:14:54.000 --> 00:14:56.996
you can find all this documentation there,

00:14:57.079 --> 00:14:58.719
so everything is linked out.

00:14:59.880 --> 00:15:05.879
But so Transformers JS is the JavaScript interface

00:15:06.360 --> 00:15:09.759
to this ONNX runtime that is running in Wasm

00:15:10.199 --> 00:15:11.399
in your browser.

00:15:12.040 --> 00:15:14.720
So here you can see Transformers JS uses

00:15:15.000 --> 00:15:17.676
the ONNX runtime in the browser,

00:15:17.759 --> 00:15:21.680
and the best part about it is that you can easily convert

00:15:21.880 --> 00:15:25.077
your pre-trained PyTorch, TensorFlow models.

00:15:25.160 --> 00:15:27.960
So if you're actually training models yourself,

00:15:28.079 --> 00:15:31.480
you can fairly easily output them in format

00:15:31.920 --> 00:15:37.079
to then use it with Transformers JS in the browser.

00:15:38.240 --> 00:15:41.597
And there's a tool as well called Optimum, apparently,

00:15:41.680 --> 00:15:42.439
which you can use.

00:15:43.399 --> 00:15:45.116
Yeah, you can click through to the documentation

00:15:45.199 --> 00:15:48.359
to learn a little bit more about this.

00:15:49.240 --> 00:15:53.599
Now this first demo was fun as well,

00:15:53.920 --> 00:15:56.479
but I also have another demo.

00:15:57.160 --> 00:15:59.000
You can scan that here as well,

00:15:59.519 --> 00:16:01.199
or you can click through in the slides.

00:16:01.759 --> 00:16:03.600
I call it Babelfish AI.

00:16:04.639 --> 00:16:04.879
Oh.

00:16:05.160 --> 00:16:09.879
And I guess I forgot to link the slides there.

00:16:10.480 --> 00:16:11.280
That's not good.

00:16:11.440 --> 00:16:11.797
Okay.

00:16:11.880 --> 00:16:13.639
So we'll type that in.

00:16:14.399 --> 00:16:15.119
Babelfish.

00:16:17.959 --> 00:16:21.239
So it's on the Supabase community there.

00:16:22.680 --> 00:16:23.639
Let's open this up.

00:16:26.440 --> 00:16:28.159
Do you know OpenAI Whisper?

00:16:28.560 --> 00:16:31.756
It's kind of this model that allows you to transcribe

00:16:31.839 --> 00:16:34.679
text in various different languages.

00:16:35.480 --> 00:16:40.436
The demo we're doing here is we're loading a smaller

00:16:40.519 --> 00:16:44.599
version of the open source OpenAI model,

00:16:45.199 --> 00:16:51.239
and we're using that to transcribe my audio

00:16:51.839 --> 00:16:54.240
directly in the browser.

00:16:54.600 --> 00:16:57.597
So this is running locally on my machine in the

00:16:57.680 --> 00:17:00.920
browser, transcribing whatever I'm saying.

00:17:01.440 --> 00:17:04.317
And then, I thought, okay, that's fun, that's pretty

00:17:04.400 --> 00:17:07.000
cool, but then maybe we can do another thing.

00:17:07.679 --> 00:17:13.400
We're just using in this case here, Supabase Realtime

00:17:15.000 --> 00:17:18.200
to basically stream.

00:17:28.120 --> 00:17:31.480
Let me share here.

00:17:32.440 --> 00:17:37.157
So if you scan this it won't because the model is

00:17:37.240 --> 00:17:37.720
pretty big.

00:17:37.880 --> 00:17:41.157
The translation model is actually an open source model

00:17:41.240 --> 00:17:42.079
by Meta,

00:17:42.679 --> 00:17:46.159
which allows text-to-text translation of 200

00:17:46.799 --> 00:17:49.520
languages into 200 other languages,

00:17:49.720 --> 00:17:51.119
which means it's pretty big.

00:17:52.039 --> 00:17:54.800
So it probably will crash your phone,

00:17:55.240 --> 00:17:57.677
sorry if you scanned this already.

00:17:57.760 --> 00:17:59.956
But if you scan it with your phone and then open it

00:18:00.039 --> 00:18:00.960
on your laptop,

00:18:01.080 --> 00:18:03.919
you can follow along live if you want to.

00:18:06.520 --> 00:18:15.517
So here we're transcribing what I'm saying in English,

00:18:15.600 --> 00:18:18.040
but now, maybe you in the audience,

00:18:18.400 --> 00:18:21.596
you don't speak English, and I'm sorry if that's

00:18:21.679 --> 00:18:22.280
the case.

00:18:22.480 --> 00:18:24.240
But then what we can do is we can just say,

00:18:24.440 --> 00:18:27.600
okay, let's just translate that into Thai.

00:18:28.280 --> 00:18:29.639
Now obviously, I don't read Thai,

00:18:29.799 --> 00:18:33.960
so I don't know if this is accurate or not.

00:18:35.400 --> 00:18:38.836
However, what we can do is anyone read Thai here?

00:18:38.919 --> 00:18:42.120
Does that look reasonable?

00:18:42.960 --> 00:18:43.677
Someone says, "Yeah."

00:18:43.760 --> 00:18:45.117
I don't know if he actually speaks Thai,

00:18:45.200 --> 00:18:47.359
but I'm going to trust him.

00:18:50.919 --> 00:18:55.517
Let me go back into- there's Danish as well,

00:18:55.600 --> 00:18:56.240
there's Dutch.

00:18:56.640 --> 00:18:59.760
I know here, John is Dutch, for example.

00:19:00.200 --> 00:19:08.520
So we can translate that into Dutch for him.

00:19:09.280 --> 00:19:11.359
Does that look good, John?

00:19:15.919 --> 00:19:18.039
Okay, maybe we'll go the other round.

00:19:18.919 --> 00:19:20.960
I know, you know, English.

00:19:22.760 --> 00:19:24.824
So now the cooler thing as well is

00:19:26.864 --> 00:19:29.856
zum Beispiel, wir haben hier Tobias, der spricht Deutsch.

00:19:31.349 --> 00:19:35.186
Aber ihr alle sprecht kein Deutsch,

00:19:35.649 --> 00:19:38.012
also können wir das dann auf Englisch übersetzen.

00:19:47.720 --> 00:19:52.760
Well, so it actually works quite well for sort of

00:19:53.600 --> 00:19:56.560
the Latin languages.

00:19:57.039 --> 00:19:59.037
But now what we could do as well is

00:20:01.538 --> 00:20:02.329
大家好

00:20:04.047 --> 00:20:11.557
我是住在新加坡的德国人

00:20:11.640 --> 00:20:12.200
German.

00:20:12.720 --> 00:20:14.439
Well, that's not what I said.

00:20:21.720 --> 00:20:24.880
In Mandarin, we don't have any punctuation.

00:20:26.840 --> 00:20:30.956
It's a bit wonky, but you know what I'm trying to

00:20:31.039 --> 00:20:31.557
convey here.

00:20:31.640 --> 00:20:46.159
Or maybe French.

00:20:34.008 --> 00:20:47.037
Bonjour, un mot français, c'est très mauvais.

00:20:47.120 --> 00:20:48.596
And the French word is very bad.

00:20:48.679 --> 00:20:50.240
Know what I said?

00:20:50.440 --> 00:20:51.560
My French is very bad.

00:20:54.400 --> 00:20:59.080
But really what I'm trying to convey here is that

00:21:00.360 --> 00:21:02.237
we're running this locally in the browser.

00:21:02.320 --> 00:21:02.836
Where's English?

00:21:02.919 --> 00:21:03.279
Come on.

00:21:04.159 --> 00:21:04.719
English.

00:21:06.080 --> 00:21:08.637
My MacBook is pretty beefy, so it actually works

00:21:08.720 --> 00:21:09.319
quite well.

00:21:09.840 --> 00:21:12.277
The OpenAI Whisper, especially when we're using

00:21:12.360 --> 00:21:13.000
WebGPU.

00:21:14.000 --> 00:21:18.600
Transformers.js can tap into WebGPU as well to

00:21:18.919 --> 00:21:19.920
accelerate that.

00:21:20.240 --> 00:21:24.359
So really what I'm trying to convey to you is that

00:21:25.039 --> 00:21:25.997
this is very exciting.

00:21:26.080 --> 00:21:29.117
There's a lot of stuff happening in the open source

00:21:29.200 --> 00:21:32.880
space, as well as the local AI space.

00:21:33.520 --> 00:21:34.760
And I think that's really cool.

00:21:36.320 --> 00:21:43.120
So I hope this can inspire you, maybe add some AI to

00:21:43.279 --> 00:21:46.716
your client-side applications and tap into that.

00:21:46.799 --> 00:21:49.677
Because in the future, you're going to be running

00:21:49.760 --> 00:21:52.637
large language models locally on your phone, on your

00:21:52.720 --> 00:21:56.437
fridge, on your toaster, probably everywhere.

00:21:56.520 --> 00:21:59.159
So I think that's really exciting there.

00:21:59.480 --> 00:22:02.117
I don't have much time to get into the Supabase Edge

00:22:02.200 --> 00:22:02.760
runtime.

00:22:03.120 --> 00:22:05.716
It's also using the ONNX runtime, just with another

00:22:05.799 --> 00:22:09.076
cool open source project called Ort, which is

00:22:09.159 --> 00:22:11.040
machine learning inference for Rust.

00:22:12.640 --> 00:22:16.117
Because Wasm was too slow in that edge runtime, so

00:22:16.200 --> 00:22:18.277
we're just using the Rust layer there.

00:22:18.360 --> 00:22:21.119
And with that, ขอบคุณครับ.

00:22:21.400 --> 00:22:22.280
Thanks so much.

WEBVTT - Auto-generated by https://github.com/dtinth/autosub

00:00:01.160 --> 00:00:02.639
สวัสดีค่ะ แพนด้าค่ะ

00:00:02.760 --> 00:00:04.676
เป็นเจ้าของเพจ Darkgrammar นะคะ

00:00:04.759 --> 00:00:07.356
เป็นเพจให้ความรู้พวก software development

00:00:07.439 --> 00:00:08.917
แล้วก็มีแชร์ tool ต่างๆ

00:00:09.000 --> 00:00:11.600
ถ้าใครสนใจก็ลองไปติดตามดูได้ค่ะ

00:00:11.960 --> 00:00:14.239
แต่ว่าวันนี้เราจะมาคุยกันในหัวข้อ

00:00:14.519 --> 00:00:17.757
How to trick your users into filling in your boring AI

00:00:17.840 --> 00:00:18.877
ethics survey

00:00:18.960 --> 00:00:22.036
หรือว่าการหลอกให้คนช่วยทำแบบสอบถามหน้าเบื่อๆ

00:00:22.119 --> 00:00:23.920
ของเราให้หน่อยค่ะ

00:00:24.680 --> 00:00:26.759
ซึ่งการหลอกหรือการ trick ก็คือ

00:00:27.039 --> 00:00:30.116
การทำให้คนทำรู้สึกหรือคิดอะไรสักอย่าง

00:00:30.199 --> 00:00:33.557
ที่เขาไม่ได้ตั้งใจจะทำรู้สึกหรือคิดตั้งแต่แรก

00:00:33.640 --> 00:00:34.479
ด้วยตัวเองค่ะ

00:00:34.879 --> 00:00:37.677
ซึ่งในเคสนี้การหลอกมันก็ไร้สาระหน่อย

00:00:37.760 --> 00:00:39.597
ก็คือหลอกให้คนทำแบบสอบถามให้เราหน่อย

00:00:39.680 --> 00:00:41.840
เพื่อเราจะได้มี data เยอะๆ ไปทำ research

00:00:42.160 --> 00:00:43.040
ไปส่งอาจารย์ค่ะ

00:00:43.280 --> 00:00:46.877
แต่ใน presentation นี้มันก็มี double meaning นิดนึง

00:00:46.960 --> 00:00:50.197
เพราะว่า research ตอนนี้เกี่ยวกับ media manipulation

00:00:50.280 --> 00:00:52.760
โดยการใช้ AI แล้วก็ social media ค่ะ

00:00:52.920 --> 00:00:55.760
ซึ่งการ trick ในเคสนี้ก็จะเป็นการ trick

00:00:56.399 --> 00:00:58.476
ให้เขารู้สึกหรือคิดในอะไรบางอย่าง

00:00:58.559 --> 00:01:01.799
ที่เราต้องการให้เขารู้สึกหรือคิดค่ะ

00:01:02.559 --> 00:01:05.679
ในปี 2019 ได้มี paper ตัวนึงออกมา

00:01:06.200 --> 00:01:09.880
เกี่ยวกับการทำ social media manipulation ในเมืองจีน

00:01:10.000 --> 00:01:12.200
ที่ออกมาจาก Political Warfare Unit ของจีน

00:01:12.799 --> 00:01:15.880
ซึ่งเขา describe ว่าเขาอยากจะสร้าง AI system ตัวนึง

00:01:16.280 --> 00:01:18.557
ที่สามารถ sway public opinion ได้

00:01:18.640 --> 00:01:21.676
AI system ตัวนี้จะสร้าง account ปลอมเยอะมาก

00:01:21.759 --> 00:01:23.200
เป็นพันๆ account เป็นหมื่นๆ account

00:01:23.960 --> 00:01:26.917
โดยการใช้ AI generate profile generate content

00:01:27.000 --> 00:01:29.200
generate bio generate comment ต่างๆ

00:01:29.520 --> 00:01:32.437
แล้วประมาณ 90% ของ role ของ AI พวกนี้

00:01:32.520 --> 00:01:34.799
มันจะเอาไว้สร้าง content lifestyle ปกติ

00:01:35.119 --> 00:01:37.957
ทำให้ profile พวกนี้ดูเหมือนเป็น profile ของคนจริงๆ

00:01:38.040 --> 00:01:41.676
ไม่เหมือน IO ที่เราจะจับได้ว่าเป็น Facebook page ปลอม

00:01:41.759 --> 00:01:42.157
ที่มันเพิ่งเปิด

00:01:42.240 --> 00:01:44.676
เพราะงั้นเขาจะพยายามดันให้มี history นานๆ

00:01:44.759 --> 00:01:45.880
ให้มันดูเป็นคนจริง

00:01:46.560 --> 00:01:48.877
แต่พอเขาต้องการ push agenda อะไรบางอย่าง

00:01:48.960 --> 00:01:51.797
เวลาเขาต้องการ sway public opinion อะไรบางอย่าง

00:01:51.880 --> 00:01:54.197
เขาก็จะเอา account พวกนี้แหละไป comment

00:01:54.280 --> 00:01:55.597
ไป flood hashtag ต่างๆ

00:01:55.680 --> 00:01:58.956
ให้คนรู้สึกว่ามีคน believe หรือมีคนเชื่อใน message

00:01:59.039 --> 00:02:00.800
ตัวนี้มากกว่าที่มันมี

00:02:05.719 --> 00:02:08.596
bottom line ก็คือ social media manipulation

00:02:08.679 --> 00:02:09.837
เนี่ยมันมีอยู่แล้วแหละ

00:02:09.920 --> 00:02:10.757
มันมีมาตั้งนานแล้ว

00:02:10.840 --> 00:02:12.197
มันมีมาเป็น 5 ปี 10 ปีแล้ว

00:02:12.280 --> 00:02:13.840
แต่ก่อนหน้านี้มันก็จะใช้ bot

00:02:14.040 --> 00:02:17.236
แล้วใช้กำลังจำนวนมากในการที่จะทำ account พวกนี้

00:02:17.319 --> 00:02:18.077
manually ขึ้นมา

00:02:18.160 --> 00:02:20.317
ต้องมีคนพยายามตอบ พยายามมีคนคิด content

00:02:20.400 --> 00:02:22.236
แต่หลังจากมี generative AI ขึ้นมา

00:02:22.319 --> 00:02:25.160
content พวกนี้ก็จะเพิ่มขึ้นใน magnitude ที่เยอะมากๆ

00:02:25.319 --> 00:02:28.637
เพราะเราไม่จำเป็นต้องมีคนเยอะๆ มาสร้าง bot พวกนี้แล้ว

00:02:28.720 --> 00:02:30.277
เราสามารถให้ AI มัน generate ได้

00:02:30.360 --> 00:02:33.397
เราสามารถให้มันสร้าง content ที่เสมือนจริงได้

00:02:33.480 --> 00:02:35.921
โดยใช้เวลาและ resource น้อยมากๆ

00:02:36.480 --> 00:02:38.917
ทำให้คิดถึง experiment ตัวนึงขึ้นมา

00:02:39.000 --> 00:02:41.717
ที่จะลอง test ดูว่าในวันที่ social media

00:02:41.800 --> 00:02:43.076
มันเต็มไปด้วย AI content

00:02:43.159 --> 00:02:45.880
แล้วเราจะสามารถ distinguish มันได้ดีขนาดไหน

00:02:46.159 --> 00:02:48.317
ก็เลยสร้างตัว Turing Test ก็มา

00:02:48.400 --> 00:02:50.156
คำว่า Turing Test บวกคำว่า tweet

00:02:50.239 --> 00:02:52.796
เอา Turing Test มาอยู่ใน context ของ social media

00:02:52.879 --> 00:02:56.840
experiment ตัวนี้ก็จะ show tweet หนึ่งตัวให้ user ดู

00:02:56.959 --> 00:02:59.117
แล้วก็ให้ user เลือกว่าเขาคิดว่าเป็น content

00:02:59.200 --> 00:03:01.880
ที่มาจาก political figure public figure ที่เขารู้จัก

00:03:02.200 --> 00:03:04.796
หรือว่าเป็น tweet ที่ AI เป็นคน generate ขึ้นมา

00:03:04.879 --> 00:03:06.917
พยายามเรียนแบบ public figure พวกนี้

00:03:07.000 --> 00:03:10.156
ด้วยความที่ experiment ตัวนี้ทำตอนที่อยู่ที่อเมริกา

00:03:10.239 --> 00:03:12.317
ก็เลยเลือก Elon Musk Trump กับ Taylor Swift

00:03:12.400 --> 00:03:15.000
เพราะว่าเป็น public figure ที่คนน่าจะรู้จักและคุ้นเคย

00:03:15.360 --> 00:03:17.876
แล้วก็น่าจะพอรู้ว่า Trump เขา tweet โทนประมาณไหน

00:03:17.959 --> 00:03:19.557
Elon Musk เขา tweet เกี่ยวกับเรื่องอะไรบ้าง

00:03:19.640 --> 00:03:21.640
เขาจะได้มี context ในการเดาได้

00:03:22.159 --> 00:03:25.119
ปัญหาแรกที่เจอเลยก็คือ ChatGPT

00:03:25.680 --> 00:03:28.917
มันไม่ยอม generate tweet ที่เรียนแบบ Donald Trump

00:03:29.000 --> 00:03:30.640
เพราะว่ามันมี safeguard อะไรบางอย่าง

00:03:30.920 --> 00:03:33.197
ที่พยายามไม่ go against พวก moral guideline

00:03:33.280 --> 00:03:34.957
เพราะช่วงแรกๆ มันก็จะมีดราม่าเหมือนกัน

00:03:35.040 --> 00:03:37.637
ที่มันตอบคำถามที่ไม่ค่อยถูกนัก

00:03:37.720 --> 00:03:39.720
หรือคำถามที่ไม่ค่อยถูกใจใครหลายๆ คนนัก

00:03:40.040 --> 00:03:42.119
แต่ว่าถึงมันจะมี safeguard ระดับนึง

00:03:42.360 --> 00:03:43.917
มันก็ bypass ค่อนข้างง่าย

00:03:44.000 --> 00:03:46.879
ถ้าเรารีเฟรสคำถามมันก็ยอม generate ให้เราอยู่ดี

00:03:52.079 --> 00:03:53.677
พอเราได้ data ออกมา

00:03:53.760 --> 00:03:56.000
เราก็จะมี data ส่วนหนึ่งที่ generate จาก AI

00:03:56.120 --> 00:03:58.000
แล้วก็ data อีกส่วนหนึ่งที่ไปเอามาจาก Kaggle

00:03:58.280 --> 00:03:59.677
ก็เป็น tweeting ของ Donald Trump

00:03:59.760 --> 00:04:01.318
tweeting ของ public figure ต่างๆ

00:04:01.920 --> 00:04:03.197
ตอนแรกเราก็รู้สึกว่า อ๋อ

00:04:03.280 --> 00:04:05.477
อันนี้เราก็แค่ยัดลงไปใน Google Form ก็ได้

00:04:05.560 --> 00:04:08.400
แล้วก็ให้คนมาถามเป็นคำถาม Google Form 30 ข้อ

00:04:08.640 --> 00:04:09.356
10 อันของ Trump

00:04:09.439 --> 00:04:10.439
10 อันของ Taylor Swift

00:04:10.680 --> 00:04:11.716
10 อันของ Elon Musk

00:04:11.799 --> 00:04:14.957
แต่พอทำตัว Google Form ตัวนี้เสร็จแล้วส่งเข้าไปในกลุ่มเนี่ย

00:04:15.040 --> 00:04:16.519
มีคนตอบกลับมาแค่ 2 คน

00:04:16.959 --> 00:04:19.597
ซึ่งมันไม่ค่อยพอเท่าไหร่ในการทำ data

00:04:19.680 --> 00:04:24.200
เราก็เลยพยายามหาวิธีที่ทำยังไงดีให้คนมานั่งตอบคำถาม 30 ข้อของเรา

00:04:24.639 --> 00:04:26.157
แทนที่จะเป็น Google Form

00:04:26.240 --> 00:04:28.840
เราก็เลยลองเขียน web app ง่ายๆขึ้นมา

00:04:29.720 --> 00:04:31.757
ที่เป็น web app ที่เขียนโดยใช้ React

00:04:31.840 --> 00:04:34.080
พยายาม mimic UI ของ Twitter

00:04:34.360 --> 00:04:37.357
ให้เขารู้สึกเหมือนเขาเล่นเกมมากกว่าที่เขาจะทำข้อสอบ

00:04:38.160 --> 00:04:40.956
ตอนเขาตอบคำตอบที่ถูก card ก็จะกลายเป็นสีเขียว

00:04:41.039 --> 00:04:42.719
พอเขาตอบผิดมันจะกลายเป็นสีแดง

00:04:43.000 --> 00:04:45.836
ซึ่งตัว positive reinforcement มันก็ทำให้เขาอยากเล่นต่อ

00:04:45.919 --> 00:04:49.277
หรือว่าตอนเขาตอบผิดเขาก็จะสามารถพยายาม recognize pattern

00:04:49.360 --> 00:04:50.437
อะไรต่างๆของ AI ได้

00:04:50.520 --> 00:04:52.960
เขาก็จะพยายาม improve คำตอบของตัวเองในคราวต่อไป

00:04:53.680 --> 00:04:55.956
แล้วอีกอย่างหนึ่งที่ดีมากๆของการเขียน app เองก็คือ

00:04:56.039 --> 00:04:58.597
เราสามารถ store data ได้ทุกคลิกเลย

00:04:58.680 --> 00:05:02.480
สมมุติว่าเขาเล่นแค่ของ Elon Musk ไป ตอบแค่ 10 ข้อ

00:05:02.800 --> 00:05:04.676
เราก็สามารถเอา data ตรงนั้นมาทำต่อแล้ว

00:05:04.759 --> 00:05:07.560
เราไม่จำเป็นต้องรอเขาตอบทั้ง 30 ข้อแล้วรอเขา submit

00:05:08.520 --> 00:05:13.676
ปรากฏว่าการที่เลือกทำแบบนี้ทำให้มีคนตอบแบบสอบถามกลับมาเยอะมาก

00:05:13.759 --> 00:05:16.116
แทนที่จะมีคำตอบมาแค่ 2 อัน

00:05:16.199 --> 00:05:18.240
มีคำตอบเป็นพันภายในวันแรก

00:05:18.440 --> 00:05:21.600
ซึ่งคำตอบเป็นพันไม่ใช่เพราะมีคนตอบ 1,000 คน

00:05:22.440 --> 00:05:25.360
คำตอบมันมีเป็นพันเพราะว่ามีคนเล่นมากกว่า 1 ครั้ง

00:05:25.479 --> 00:05:30.757
มี response มากๆเพราะเขาอยากจะ improve score ของเขา

00:05:30.840 --> 00:05:32.199
เขาอยากลอง beat AI ดู

00:05:32.680 --> 00:05:34.437
ซึ่งเป็นอะไรที่เราไม่ได้ expect ตั้งแต่แรก

00:05:34.520 --> 00:05:36.037
แต่พอดีเราเก็บเลข session ไว้

00:05:36.120 --> 00:05:38.240
เราก็เลยสามารถเอา data ตรงนี้มา track ได้

00:05:38.639 --> 00:05:40.717
นี่ก็เลยน่าจะเป็น example ดีๆอย่างหนึ่ง

00:05:40.800 --> 00:05:42.477
ที่เราใช้ JavaScript เราใช้ React

00:05:42.560 --> 00:05:44.717
เราใช้การเขียน front-end interface ง่ายๆ

00:05:44.800 --> 00:05:46.357
ที่คนส่วนใหญ่ก็น่าจะเขียนเป็นอยู่แล้ว

00:05:46.440 --> 00:05:49.676
มา adapt ในงานที่ไม่ได้ยากมาก

00:05:49.759 --> 00:05:52.920
แต่เราได้ result เราได้ feedback กลับมาเยอะมากๆ

00:05:55.199 --> 00:05:57.160
พอเราเอากราฟตัวนี้กลับมา plot

00:05:57.680 --> 00:05:59.836
เราก็จะเห็นว่าถ้าใน attempt แรก

00:05:59.919 --> 00:06:02.960
เขาแทบจะแยกไม่ออกระหว่าง AI กับ user จริงเลย

00:06:03.560 --> 00:06:04.840
accuracy ตอนแรกอยู่ที่ 53.8%

00:06:06.080 --> 00:06:08.040
ก็คือเท่าๆกับ random chance เลย

00:06:08.800 --> 00:06:11.800
แต่ว่าพอเขาลองไปครั้งที่ 2 ครั้งที่ 3 ครั้งที่ 4 ครั้งที่ 5

00:06:12.080 --> 00:06:14.836
เราก็จะเห็นว่าเขาสามารถ identify traits ของ tweet

00:06:14.919 --> 00:06:16.000
ที่เป็น AI ได้ดีขึ้น

00:06:16.199 --> 00:06:19.520
ภายในครั้งที่ 5 หลายๆ response ก็จะมี accuracy สูงถึง 80-90%

00:06:21.280 --> 00:06:22.676
ซึ่งมันก็ดู positive อยู่

00:06:22.759 --> 00:06:26.637
มันดูเหมือนเรามีหวังอยู่ที่เราจะสามารถ beat AI ได้

00:06:26.720 --> 00:06:28.040
เราสามารถฉลาดขึ้นได้

00:06:28.440 --> 00:06:32.357
แต่ว่าอันนี้อาจจะเป็น statistic ที่ดีเกินจริงไปนิดนึง

00:06:32.440 --> 00:06:35.197
เพราะว่าใน context นี้เราใช้ AI model แค่ตัวเดียว

00:06:35.280 --> 00:06:36.197
เราใช้ prompt เดียวกัน

00:06:36.280 --> 00:06:38.437
มันก็ไม่แปลกที่เขาจะ recognize pattern ได้

00:06:38.520 --> 00:06:39.840
แต่ว่าใน environment จริง

00:06:40.319 --> 00:06:41.836
มันมี AI model มากกว่า 1 ตัว

00:06:41.919 --> 00:06:43.478
มัน mix กับ tweet ที่เยอะมากๆ

00:06:43.599 --> 00:06:44.836
มันก็จะอาจจะไม่ได้ตอบง่าย

00:06:44.919 --> 00:06:46.800
improvement มันอาจจะไม่ได้เยอะขนาดนี้

00:06:47.319 --> 00:06:48.956
แต่มันก็จะมาที่คำถามว่า

00:06:49.039 --> 00:06:51.477
แล้วมันเป็นปัญหาจริงๆขนาดนั้นเลยเหรอ

00:06:51.560 --> 00:06:54.676
ถ้า AI มันเต็ม Twitter ไปหมด

00:06:54.759 --> 00:06:56.961
ถ้ามันมี AI generated tweet เยอะมากๆ

00:06:57.240 --> 00:06:58.520
มันผิดขนาดนั้นเลยเหรอ

00:06:58.840 --> 00:07:01.796
ก็เลยอยากให้มาลองดู real life case study แล้วกัน

00:07:01.879 --> 00:07:05.520
ว่าตอนนี้ AI content มันมีการใช้ไปในทางที่ไม่ดี

00:07:05.960 --> 00:07:11.519
หรือการใช้ในงานที่ค่อนข้างอันตรายในเคสแบบไหนบ้าง

00:07:13.280 --> 00:07:14.917
อันนี้เป็นเคสที่ค่อนข้างล่าสุด

00:07:15.000 --> 00:07:16.960
ก็คือ Microsoft มี report ออกมาว่า

00:07:17.080 --> 00:07:19.357
เมืองจีนได้ใช้พวก AI generated image

00:07:19.440 --> 00:07:22.637
พยายามสร้าง political divide ในอเมริกา

00:07:22.720 --> 00:07:25.920
ช่วงก่อน election ในปี 2024 ที่กำลังจะถึง

00:07:26.319 --> 00:07:29.637
ก็จะสร้าง content ที่ค่อนข้างทำให้คนโกรธ

00:07:29.720 --> 00:07:32.197
ทำให้คน divide เพื่อพยายามจะ sway election

00:07:32.280 --> 00:07:33.240
พยายาม sway vote

00:07:33.560 --> 00:07:35.997
ซึ่ง rumor ประมาณนี้ทุกคนก็น่าจะเคยได้ยินกัน

00:07:36.080 --> 00:07:39.279
ใน election ในปี 2016 เหมือนกัน

00:07:40.039 --> 00:07:42.160
ล่าสุดช่วงสงครามรัสเซียยูเครน

00:07:42.479 --> 00:07:46.799
ก็มีคนทำ deepfake ของ president ของยูเครน Zelenskyy ออกมา

00:07:47.000 --> 00:07:49.197
ที่บอกว่าให้ฝั่งยูเครนวางอาวุธลง

00:07:49.280 --> 00:07:51.836
แล้วเขาก็เอาตัว deepfake ตัวนี้ tag เว็บไซต์ข่าว

00:07:51.919 --> 00:07:53.799
แล้วก็ไปอยู่ใน fanpage ของเว็บไซต์ข่าว

00:07:54.039 --> 00:07:57.836
แล้วก็เอา tag ตัวนี้ไปอยู่ใน live broadcast เหมือนกัน

00:07:57.919 --> 00:08:00.277
ซึ่ง message ประมาณนี้ก็เป็น message ที่อันตราย

00:08:00.360 --> 00:08:03.879
โดยเฉพาะในช่วงสงครามที่ information มันค่อนข้าง access ได้ยาก

00:08:04.199 --> 00:08:06.717
ถ้าคนเห็น deepfake ตัวนี้เร็วๆเขาก็อาจจะเชื่อไปแล้ว

00:08:06.800 --> 00:08:09.400
แล้วมันก็อาจจะมี effect ที่ catastrophic มากๆ

00:08:09.639 --> 00:08:11.997
แต่ก็โชคดีที่ message นี้มันไม่ได้ไปไกลมาก

00:08:12.080 --> 00:08:14.556
เพราะว่านี่เป็น deepfake ที่ทำขึ้นมาค่อนข้างถูก

00:08:14.639 --> 00:08:18.197
ทุกคนมองแบบซักพักนึงก็น่าจะพอรู้ว่าหัวมันแบบลอยๆ

00:08:18.280 --> 00:08:20.440
แล้วก็เขาก็มาตอบโต้ค่อนข้างเร็วใน Twitter ว่า

00:08:20.720 --> 00:08:23.320
นี้ไม่ใช่ deepfake จริง

00:08:24.000 --> 00:08:28.917
แล้วก็นี่เป็นอีกเคสนึงที่อาจจะไม่ได้เกี่ยวกับ political matter ขนาดนั้น

00:08:29.000 --> 00:08:30.997
ไม่ได้เกี่ยวกับ state อะไรที่ใหญ่มาก

00:08:31.080 --> 00:08:32.919
แต่ว่านี่เป็นเคส court case

00:08:33.159 --> 00:08:35.480
ที่มีคุณแม่คนนึงที่อยากได้ custody ของลูก

00:08:35.680 --> 00:08:38.117
เขาก็เลย deepfake เสียงของสามีตัวเอง

00:08:38.200 --> 00:08:39.636
ที่กำลัง threaten ตัวเอง

00:08:39.719 --> 00:08:42.639
เพื่อที่จะ convince ให้ court ให้ custody ตัวเอง

00:08:43.120 --> 00:08:48.756
ซึ่งเคสแบบนี้ มันไม่ได้เป็น one-off case ที่น่า concerning

00:08:48.839 --> 00:08:52.517
มัน concerning เพราะว่าในวันที่ AI มันสามารถ mimic คน

00:08:52.600 --> 00:08:54.320
จนเราไม่สามารถ distinguish ได้

00:08:55.480 --> 00:08:58.717
video evidence audio evidence มันจะยังใช้ใน court ได้รึเปล่า

00:08:58.800 --> 00:09:01.759
ถ้ามันเป็นวันที่เราตัดต่อรูปได้เหมือนมาก 100%

00:09:02.399 --> 00:09:05.117
ทุกคนก็จะสามารถบอกได้ว่ารูปนี้มันไม่มีความหมาย

00:09:05.200 --> 00:09:06.876
เพราะทุกคนสามารถตัดต่อรูปได้

00:09:06.959 --> 00:09:10.037
แล้วอย่างงี้เราจะใช้อะไรในการ prove ความ guilty

00:09:10.120 --> 00:09:14.280
หรือความ innocence

00:09:14.640 --> 00:09:17.040
essence ของ talk นี้ก็เลยว่า

00:09:17.160 --> 00:09:20.520
ครั้งต่อไปที่มีคนส่ง survey

00:09:21.040 --> 00:09:23.640
หรือมีคนส่ง content อะไรบน social media

00:09:23.959 --> 00:09:27.280
ที่ทำให้คุณรู้สึกโกรธ รู้สึก riled up

00:09:27.860 --> 00:09:29.240
รู้สึกว่าคุณอยากจะทำอะไร

00:09:29.440 --> 00:09:32.920
รู้สึกอะไร หรือคิดอะไรที่คุณไม่ได้ทำรู้สึก

00:09:33.079 --> 00:09:35.040
หรือคิดตั้งแต่แรก

00:09:35.800 --> 00:09:39.917
ก็ลองกลับมาคิดดูว่าคุณกำลังถูกหลอกอยู่รึเปล่า

00:09:40.000 --> 00:09:42.717
เพราะว่าใน landscape ของ social media

00:09:42.800 --> 00:09:44.800
หลายๆคนน่าจะเคยได้ยิน quote ว่า

00:09:45.440 --> 00:09:47.637
if you are not paying for the product, you're the product

00:09:47.720 --> 00:09:49.756
ซึ่ง product ใน social media

00:09:49.839 --> 00:09:53.677
ก็คือสิ่งที่ทุกคนทำ สิ่งที่ทุกคนคิด สิ่งที่ทุกคนเป็น

00:09:53.760 --> 00:09:57.280
behavior เล็กๆ perception ที่มันเปลี่ยนไปอย่างที่เราไม่รู้ตัว

00:09:57.399 --> 00:09:59.397
และนี่คือสิ่งที่เขากำลังขายให้พวกเรา

00:09:59.480 --> 00:10:00.920
ขายให้ advertiser ต่างๆ

00:10:01.360 --> 00:10:03.879
ก็ขอให้ทุกคนใช้ความระมัดระวัง

00:10:04.320 --> 00:10:06.837
แล้วก็ beware ของ content ใน social media

00:10:06.920 --> 00:10:09.677
ทั้งที่เป็น content ที่คนสร้าง

00:10:09.760 --> 00:10:11.680
แล้วก็ content ที่จะ generate โดย AI

00:10:11.800 --> 00:10:12.917
ที่จะเพิ่มขึ้นเรื่อยๆค่ะ

00:10:13.000 --> 00:10:13.680
ขอบคุณค่ะ

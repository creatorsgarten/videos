import { anthropic } from '@ai-sdk/anthropic'
import { google } from '@ai-sdk/google'
import { generateObject } from 'ai'
import 'dotenv/config'
import fs from 'fs'
import pMap from 'p-map'
import pRetry from 'p-retry'
import subtitle from 'subtitle'
import yargs from 'yargs'
import { z } from 'zod'
import { serializeVtt } from '../serializeVtt'

const argv = await yargs(process.argv.slice(2))
  .strict()
  .help()
  .option('subtitlePath', {
    describe: 'Path to the subtitle file (.vtt)',
    type: 'string',
    demandOption: true,
  })
  .option('model', {
    describe: 'Model to use for improving subtitles (gemini, claude)',
    type: 'string',
    choices: ['gemini', 'claude'],
    default: 'gemini',
  })
  .option('skip', {
    describe: 'Skip first N segments',
    type: 'number',
    default: 0,
  })
  .option('limit', {
    describe: 'Limit processing to N segments (to save on tokens)',
    type: 'number',
    default: undefined,
  })
  .option('batchSize', {
    describe: 'Number of segments to process in each batch',
    type: 'number',
    default: 100,
  })
  .option('concurrency', {
    describe: 'Number of batches to process concurrently',
    type: 'number',
    default: 10,
  })
  .option('retries', {
    describe: 'Number of retries for failed batch processing',
    type: 'number',
    default: 3,
  })
  .parse()

const subtitlePath = argv.subtitlePath
if (!subtitlePath) {
  throw new Error('Subtitle path is required')
}

if (!fs.existsSync(subtitlePath)) {
  throw new Error(`Subtitle file not found: ${subtitlePath}`)
}

// Parse VTT file to JSON
function vttToJsonObject(vtt: string) {
  const parsed = subtitle.parseSync(vtt)
  const cues = parsed.filter((x) => x.type === 'cue').map((x) => x.data)
  const cueToIds = new Map<subtitle.Cue, string[]>()
  let nextId = 1

  const segments = []
  for (const cue of cues) {
    const start = cue.start / 1000
    const text = cue.text
    if (!text.trim()) {
      continue
    }
    const lines = text.split('\n').map((x) => x.trim())
    const ids: string[] = []
    for (const line of lines) {
      const id = `${nextId++}`
      segments.push({ id, text: line })
      ids.push(id)
    }
    cueToIds.set(cue, ids)
  }

  return { segments, cueToIds, parsed }
}

const model =
  argv.model === 'gemini'
    ? google('gemini-2.0-flash-exp')
    : anthropic('claude-3-7-sonnet-20250219')

// Define Zod schema for subtitle segments
const SegmentSchema = z.object({
  id: z.string(),
  text: z.string(),
})

const SegmentsSchema = z.object({
  segments: z.array(SegmentSchema),
})

async function improveSubtitleBatch(segments: z.infer<typeof SegmentSchema>[]) {
  return pRetry(
    async () => {
      console.log(`Processing batch with ${segments.length} segments...`)

      const improvedSegments = await generateObject({
        model: model,
        schema: SegmentsSchema,
        prompt: `I need your help to improve subtitle text quality. 

The original transcript was generated by an automatic speech recognition system, which has some issues:
1. Words are often misspelled
2. Numbers are written as words instead of digits (e.g. "twenty three" should be "23")
3. Technical terms may be incorrectly transcribed
4. Punctuation may be missing or incorrect
5. Sentence fragments may be unclear

Please correct these issues while maintaining the original meaning. Keep proper names and technical terms intact.

Examples of improvements (key is the original text, value is the improved text):
${JSON.stringify({
  โอรามะ: 'Ollama',
  หนึ่งร้อยยี่สิบสาม: '123',
  โอเพ่นเอไอ: 'OpenAI',
  'gpt four o': 'GPT-4o',
  'แอล แอล เอ็ม': 'LLM',
  สองจุดห้า: '2.5',
  เพอร์ฟอร์แมนซ์: 'Perfomance',
})}

Words that are transliterated from English should be corrected to the original English word and NOT translated to Thai. For example, "ฟรีดอม" should be "freedom" and NOT "เสรีภาพ".
However, some loan words that are very commonly used in Thai language can be left as is, such as "โมเดล" or "ลิงก์", unless it is part of a proper name.

Sometimes, the speaker will say "um" or "uh" as a filler word. These should be removed from the text.

Important rules:
- Return exactly ${segments.length} segments in the same order
- Each segment must have the same 'id' as the original
- Do not merge or split segments
- Do not omit any segments
- Preserve technical terminology but fix misspellings

Original subtitle segments:
${JSON.stringify({ segments }, null, 2)}

Please return the improved segments with the same structure. Each segment should have the same 'id' but with improved 'text' content.`,
        temperature: 0.2,
        maxTokens: 8000,
      })
      console.log(
        `Batch completed with token usage: ${JSON.stringify(
          improvedSegments.usage,
        )}`,
      )

      return improvedSegments.object.segments
    },
    {
      retries: argv.retries,
      onFailedAttempt: (error) => {
        console.error(
          `Batch processing attempt failed (${error.attemptNumber}/${
            argv.retries + 1
          }): ${error.message}`,
        )
      },
    },
  )
}

// Main execution
const vttContent = fs.readFileSync(subtitlePath, 'utf-8')
const { segments: allSegments, cueToIds, parsed } = vttToJsonObject(vttContent)
console.log(`Found ${allSegments.length} subtitle segments`)

// Apply skip and limit if specified
const skippedSegments = argv.skip ? allSegments.slice(argv.skip) : allSegments
const segmentsToProcess = argv.limit
  ? skippedSegments.slice(0, argv.limit)
  : skippedSegments

if (argv.skip) {
  console.log(`Skipped first ${argv.skip} segments`)
}
if (argv.limit) {
  console.log(`Limited to ${argv.limit} segments`)
}

// Split segments into batches of specified size
const batchSize = argv.batchSize
const batches: z.infer<typeof SegmentSchema>[][] = []

for (let i = 0; i < segmentsToProcess.length; i += batchSize) {
  batches.push(segmentsToProcess.slice(i, i + batchSize))
}

console.log(
  `Processing ${segmentsToProcess.length} segments in ${batches.length} batches (batch size: ${batchSize})`,
)

// Process batches in parallel with the specified concurrency
const improvedSegmentsBatches = await pMap(
  batches,
  async (batch) => {
    return await improveSubtitleBatch(batch)
  },
  {
    concurrency: argv.concurrency,
  },
)

// Flatten the batch results
const improvedSegments = {
  segments: improvedSegmentsBatches.flat(),
}

console.log(`Improved ${improvedSegments.segments.length} segments`)

// Reconstruct VTT file with improved segments
const idToOriginalSegments = new Map(
  allSegments.map((segment) => [segment.id, segment]),
)
const idToImprovedSegment = new Map(
  improvedSegments.segments.map((segment) => [segment.id, segment]),
)
const result = serializeVtt(
  parsed.flatMap((node): subtitle.Cue[] => {
    if (node.type !== 'cue') return []
    const cue = node.data
    const ids = cueToIds.get(cue)
    if (!ids) return [cue]
    return [
      {
        ...cue,
        text: ids
          .map(
            (id) =>
              idToImprovedSegment.get(id)?.text ??
              idToOriginalSegments.get(id)?.text,
          )
          .join('\n'),
      },
    ]
  }),
)
fs.writeFileSync(subtitlePath, result)
console.log(`Updated subtitle file: ${subtitlePath}`)
